[INFO] 2024-06-13 21:59:36,637 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-13 21:59:36,637 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: qwen_7b
[INFO] 2024-06-13 21:59:36,638 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': False, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': False, 'recompute_slice_activation': False}
[INFO] 2024-06-13 21:59:36,638 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 1, 'vocab_emb_dp': False, 'gradient_aggregation_group': 4}
[INFO] 2024-06-13 21:59:36,639 [mindformers/trainer/base_trainer.py:196] _check_global_batch_size_for_auto_parallel: The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 8 = 1 * 8 * 1 * 1
[ERROR] 2024-06-13 21:59:36,641 [mindformers/tools/cloud_adapter/cloud_monitor.py:43] wrapper: Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/cloud_adapter/cloud_monitor.py", line 34, in wrapper
    result = run_func(*args, **kwargs)
  File "/home/ma-user/work/mindformers/research/qwen/run_qwen.py", line 132, in main
    result = task.predict(input_data=prompt,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/_checkparam.py", line 1313, in wrapper
    return func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/trainer.py", line 675, in predict
    output_result = self.trainer.predict(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/causal_language_modeling/causal_language_modeling.py", line 315, in predict
    return self.predict_process(config=config,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/base_trainer.py", line 860, in predict_process
    check_rules(config, mode='predict', network=network, task=self.task)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 217, in check_rules
    _check_mode(config, mode, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 55, in _check_mode
    _rule_bs_divisible_by_dp(config, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 97, in _rule_bs_divisible_by_dp
    raise ValueError(f"batch_size should be divisible by dp. "
ValueError: batch_size should be divisible by dp. But batch_size % dp = 1 % 8 = 1

[INFO] 2024-06-13 22:01:37,238 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-13 22:01:37,240 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: qwen_7b
[INFO] 2024-06-13 22:01:37,240 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': False, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': False, 'recompute_slice_activation': False}
[INFO] 2024-06-13 22:01:37,240 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 8, 'vocab_emb_dp': False, 'gradient_aggregation_group': 4}
[INFO] 2024-06-13 22:01:37,241 [mindformers/trainer/base_trainer.py:196] _check_global_batch_size_for_auto_parallel: The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 8 = 1 * 8 * 1 * 1
[ERROR] 2024-06-13 22:01:37,243 [mindformers/tools/cloud_adapter/cloud_monitor.py:43] wrapper: Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/cloud_adapter/cloud_monitor.py", line 34, in wrapper
    result = run_func(*args, **kwargs)
  File "/home/ma-user/work/mindformers/research/qwen/run_qwen.py", line 132, in main
    result = task.predict(input_data=prompt,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/_checkparam.py", line 1313, in wrapper
    return func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/trainer.py", line 675, in predict
    output_result = self.trainer.predict(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/causal_language_modeling/causal_language_modeling.py", line 315, in predict
    return self.predict_process(config=config,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/base_trainer.py", line 860, in predict_process
    check_rules(config, mode='predict', network=network, task=self.task)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 217, in check_rules
    _check_mode(config, mode, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 55, in _check_mode
    _rule_bs_divisible_by_dp(config, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 97, in _rule_bs_divisible_by_dp
    raise ValueError(f"batch_size should be divisible by dp. "
ValueError: batch_size should be divisible by dp. But batch_size % dp = 1 % 8 = 1

[INFO] 2024-06-13 22:03:07,291 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-13 22:03:07,292 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: qwen_7b
[INFO] 2024-06-13 22:03:07,292 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': False, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': False, 'recompute_slice_activation': False}
[INFO] 2024-06-13 22:03:07,292 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 1, 'vocab_emb_dp': False, 'gradient_aggregation_group': 4}
[INFO] 2024-06-13 22:03:07,293 [mindformers/trainer/base_trainer.py:196] _check_global_batch_size_for_auto_parallel: The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 64 = 8 * 8 * 1 * 1
[ERROR] 2024-06-13 22:03:07,295 [mindformers/tools/cloud_adapter/cloud_monitor.py:43] wrapper: Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/cloud_adapter/cloud_monitor.py", line 34, in wrapper
    result = run_func(*args, **kwargs)
  File "/home/ma-user/work/mindformers/research/qwen/run_qwen.py", line 132, in main
    result = task.predict(input_data=prompt,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/_checkparam.py", line 1313, in wrapper
    return func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/trainer.py", line 675, in predict
    output_result = self.trainer.predict(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/causal_language_modeling/causal_language_modeling.py", line 315, in predict
    return self.predict_process(config=config,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/base_trainer.py", line 860, in predict_process
    check_rules(config, mode='predict', network=network, task=self.task)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 217, in check_rules
    _check_mode(config, mode, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 55, in _check_mode
    _rule_bs_divisible_by_dp(config, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 97, in _rule_bs_divisible_by_dp
    raise ValueError(f"batch_size should be divisible by dp. "
ValueError: batch_size should be divisible by dp. But batch_size % dp = 1 % 8 = 1

[INFO] 2024-06-13 22:06:31,558 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-13 22:06:31,560 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: qwen_7b
[INFO] 2024-06-13 22:06:31,560 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': False, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': False, 'recompute_slice_activation': False}
[INFO] 2024-06-13 22:06:31,560 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 2, 'vocab_emb_dp': False, 'gradient_aggregation_group': 4}
[INFO] 2024-06-13 22:06:31,561 [mindformers/trainer/base_trainer.py:196] _check_global_batch_size_for_auto_parallel: The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 64 = 8 * 8 * 1 * 1
[ERROR] 2024-06-13 22:06:31,564 [mindformers/tools/cloud_adapter/cloud_monitor.py:43] wrapper: Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/cloud_adapter/cloud_monitor.py", line 34, in wrapper
    result = run_func(*args, **kwargs)
  File "/home/ma-user/work/mindformers/research/qwen/run_qwen.py", line 132, in main
    result = task.predict(input_data=prompt,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/_checkparam.py", line 1313, in wrapper
    return func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/trainer.py", line 675, in predict
    output_result = self.trainer.predict(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/causal_language_modeling/causal_language_modeling.py", line 315, in predict
    return self.predict_process(config=config,
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/base_trainer.py", line 860, in predict_process
    check_rules(config, mode='predict', network=network, task=self.task)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 217, in check_rules
    _check_mode(config, mode, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 55, in _check_mode
    _rule_bs_divisible_by_dp(config, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 97, in _rule_bs_divisible_by_dp
    raise ValueError(f"batch_size should be divisible by dp. "
ValueError: batch_size should be divisible by dp. But batch_size % dp = 1 % 8 = 1

[INFO] 2024-06-14 10:47:15,005 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-14 10:47:15,006 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: baichuan2_7b_lora
[WARNING] 2024-06-14 10:47:15,006 [mindformers/trainer/base_trainer.py:131] __init__: Input model name is not in the supported list or unspecified.
[WARNING] 2024-06-14 10:47:15,007 [mindformers/trainer/base_trainer.py:132] __init__: See the list of supported task and model name: OrderedDict([('general', OrderedDict([('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/general/run_general_task.yaml')])), ('masked_image_modeling', OrderedDict([('mae_vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml')])), ('image_classification', OrderedDict([('vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('mindspore/vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('mindspore/swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml')])), ('fill_mask', OrderedDict([('bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_base_uncased.yaml'), ('bert_tiny_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml')])), ('contrastive_language_image_pretrain', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('blip2_stage1_vit_g', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml'), ('blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml'), ('blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml')])), ('image_to_text_retrieval', OrderedDict([('blip2_stage1_evaluator', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml')])), ('zero_shot_image_classification', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml'), ('blip2_stage1_classification', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml')])), ('image_to_text_generation', OrderedDict([('itt_blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml'), ('itt_blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml')])), ('translation', OrderedDict([('t5_small', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml'), ('t5_tiny', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_tiny_on_wmt16.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml')])), ('text_classification', OrderedDict([('txtcls_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml'), ('txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('mindspore/txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('gpt2_txtcls', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_txtcls.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml')])), ('token_classification', OrderedDict([('tokcls_bert_base_chinese', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml'), ('tokcls_bert_base_chinese_cluener', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml')])), ('question_answering', OrderedDict([('qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('qa_bert_base_uncased_squad', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('mindspore/qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml')])), ('text_generation', OrderedDict([('gpt2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml'), ('gpt2_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_lora.yaml'), ('gpt2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_13b.yaml'), ('gpt2_52b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_52b.yaml'), ('gpt2_xl', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl.yaml'), ('gpt2_xl_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl_lora.yaml'), ('llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b.yaml'), ('llama_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_13b.yaml'), ('llama_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_65b.yaml'), ('llama2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_7b.yaml'), ('llama2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_13b.yaml'), ('llama2_70b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_70b.yaml'), ('codellama_34b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codellama/run_codellama_34b_910b.yaml'), ('llama_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b_lora.yaml'), ('pangualpha_2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_2_6b.yaml'), ('pangualpha_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_13b.yaml'), ('glm_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_finetune.yaml'), ('glm_6b_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_infer.yaml'), ('glm_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora.yaml'), ('glm_6b_lora_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora_infer.yaml'), ('glm2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b.yaml'), ('glm2_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_lora.yaml'), ('glm2_6b_ptuning2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_ptuning2.yaml'), ('glm3_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm3/run_glm3_6b.yaml'), ('codegeex2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codegeex2/run_codegeex2_6b.yaml'), ('bloom_560m', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_560m.yaml'), ('bloom_7.1b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_7.1b.yaml'), ('bloom_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_65b.yaml'), ('bloom_176b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_176b.yaml'), ('baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan/run_baichuan_7b.yaml'), ('baichuan2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_7b.yaml'), ('baichuan2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_13b.yaml'), ('ziya_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/ziya/run_ziya_13b.yaml'), ('skywork_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/skywork/run_skywork_13b.yaml'), ('internlm_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b.yaml'), ('internlm_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b_lora.yaml'), ('qwen_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b.yaml'), ('qwen_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b_lora.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml')])), ('segment_anything', OrderedDict([('sam_vit_b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-b.yaml'), ('sam_vit_l', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-l.yaml'), ('sam_vit_h', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml')]))])
[WARNING] 2024-06-14 10:47:15,008 [mindformers/trainer/base_trainer.py:133] __init__: The default model config: /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml will now be used for the text_generation task 
[INFO] 2024-06-14 10:47:15,008 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
[INFO] 2024-06-14 10:47:15,008 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 4, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
[INFO] 2024-06-14 10:47:15,009 [mindformers/trainer/base_trainer.py:196] _check_global_batch_size_for_auto_parallel: The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 8 = 2 * 4 * 1 * 1
[INFO] 2024-06-14 10:47:15,009 [mindformers/trainer/base_trainer.py:624] training_process: .........Build Dataset For Train..........
[INFO] 2024-06-14 10:47:15,009 [mindformers/trainer/base_trainer.py:353] create_train_dataset: .........Build Dataset From Config..........
[INFO] 2024-06-14 10:47:15,010 [mindformers/dataset/causal_language_model_dataset.py:166] __new__: Now Create Causal Language Model Dataset.
[INFO] 2024-06-14 10:47:15,016 [mindformers/trainer/utils.py:155] check_runner_config: Will be Training epochs:1, sink_size:2
[INFO] 2024-06-14 10:47:15,016 [mindformers/trainer/utils.py:157] check_runner_config: Create training dataset finish, dataset size:1250
[ERROR] 2024-06-14 10:47:15,019 [mindformers/tools/cloud_adapter/cloud_monitor.py:43] wrapper: Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/cloud_adapter/cloud_monitor.py", line 34, in wrapper
    result = run_func(*args, **kwargs)
  File "/home/ma-user/work/mindformers/research/baichuan2/run_baichuan2.py", line 169, in main
    trainer.finetune(finetune_checkpoint=ckpt, auto_trans_ckpt=config.auto_trans_ckpt, resume_training=resume)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/_checkparam.py", line 1313, in wrapper
    return func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/trainer.py", line 520, in finetune
    self.trainer.train(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/causal_language_modeling/causal_language_modeling.py", line 97, in train
    self.training_process(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/base_trainer.py", line 654, in training_process
    check_rules(config, mode='train', network=network, dataset=dataset)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 219, in check_rules
    _check_parallel(config)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 134, in _check_parallel
    raise ValueError(f"The parallel config data_parallel * model_parallel * pipeline_stage should "
ValueError: The parallel config data_parallel * model_parallel * pipeline_stage should be equal to device_num, but get dp*mp*pp = 4*1*1 = 4 != device_num(8)

[INFO] 2024-06-14 10:48:17,355 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-14 10:48:17,356 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: baichuan2_7b_lora
[WARNING] 2024-06-14 10:48:17,356 [mindformers/trainer/base_trainer.py:131] __init__: Input model name is not in the supported list or unspecified.
[WARNING] 2024-06-14 10:48:17,356 [mindformers/trainer/base_trainer.py:132] __init__: See the list of supported task and model name: OrderedDict([('general', OrderedDict([('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/general/run_general_task.yaml')])), ('masked_image_modeling', OrderedDict([('mae_vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml')])), ('image_classification', OrderedDict([('vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('mindspore/vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('mindspore/swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml')])), ('fill_mask', OrderedDict([('bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_base_uncased.yaml'), ('bert_tiny_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml')])), ('contrastive_language_image_pretrain', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('blip2_stage1_vit_g', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml'), ('blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml'), ('blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml')])), ('image_to_text_retrieval', OrderedDict([('blip2_stage1_evaluator', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml')])), ('zero_shot_image_classification', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml'), ('blip2_stage1_classification', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml')])), ('image_to_text_generation', OrderedDict([('itt_blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml'), ('itt_blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml')])), ('translation', OrderedDict([('t5_small', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml'), ('t5_tiny', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_tiny_on_wmt16.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml')])), ('text_classification', OrderedDict([('txtcls_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml'), ('txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('mindspore/txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('gpt2_txtcls', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_txtcls.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml')])), ('token_classification', OrderedDict([('tokcls_bert_base_chinese', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml'), ('tokcls_bert_base_chinese_cluener', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml')])), ('question_answering', OrderedDict([('qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('qa_bert_base_uncased_squad', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('mindspore/qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml')])), ('text_generation', OrderedDict([('gpt2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml'), ('gpt2_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_lora.yaml'), ('gpt2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_13b.yaml'), ('gpt2_52b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_52b.yaml'), ('gpt2_xl', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl.yaml'), ('gpt2_xl_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl_lora.yaml'), ('llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b.yaml'), ('llama_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_13b.yaml'), ('llama_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_65b.yaml'), ('llama2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_7b.yaml'), ('llama2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_13b.yaml'), ('llama2_70b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_70b.yaml'), ('codellama_34b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codellama/run_codellama_34b_910b.yaml'), ('llama_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b_lora.yaml'), ('pangualpha_2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_2_6b.yaml'), ('pangualpha_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_13b.yaml'), ('glm_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_finetune.yaml'), ('glm_6b_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_infer.yaml'), ('glm_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora.yaml'), ('glm_6b_lora_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora_infer.yaml'), ('glm2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b.yaml'), ('glm2_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_lora.yaml'), ('glm2_6b_ptuning2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_ptuning2.yaml'), ('glm3_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm3/run_glm3_6b.yaml'), ('codegeex2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codegeex2/run_codegeex2_6b.yaml'), ('bloom_560m', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_560m.yaml'), ('bloom_7.1b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_7.1b.yaml'), ('bloom_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_65b.yaml'), ('bloom_176b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_176b.yaml'), ('baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan/run_baichuan_7b.yaml'), ('baichuan2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_7b.yaml'), ('baichuan2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_13b.yaml'), ('ziya_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/ziya/run_ziya_13b.yaml'), ('skywork_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/skywork/run_skywork_13b.yaml'), ('internlm_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b.yaml'), ('internlm_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b_lora.yaml'), ('qwen_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b.yaml'), ('qwen_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b_lora.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml')])), ('segment_anything', OrderedDict([('sam_vit_b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-b.yaml'), ('sam_vit_l', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-l.yaml'), ('sam_vit_h', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml')]))])
[WARNING] 2024-06-14 10:48:17,357 [mindformers/trainer/base_trainer.py:133] __init__: The default model config: /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml will now be used for the text_generation task 
[INFO] 2024-06-14 10:48:17,357 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
[INFO] 2024-06-14 10:48:17,358 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 4, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
[INFO] 2024-06-14 10:48:17,358 [mindformers/trainer/base_trainer.py:196] _check_global_batch_size_for_auto_parallel: The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 8 = 2 * 4 * 1 * 1
[INFO] 2024-06-14 10:48:17,359 [mindformers/trainer/base_trainer.py:624] training_process: .........Build Dataset For Train..........
[INFO] 2024-06-14 10:48:17,359 [mindformers/trainer/base_trainer.py:353] create_train_dataset: .........Build Dataset From Config..........
[INFO] 2024-06-14 10:48:17,359 [mindformers/dataset/causal_language_model_dataset.py:166] __new__: Now Create Causal Language Model Dataset.
[INFO] 2024-06-14 10:48:17,365 [mindformers/trainer/utils.py:155] check_runner_config: Will be Training epochs:1, sink_size:2
[INFO] 2024-06-14 10:48:17,365 [mindformers/trainer/utils.py:157] check_runner_config: Create training dataset finish, dataset size:1250
[ERROR] 2024-06-14 10:48:17,368 [mindformers/tools/cloud_adapter/cloud_monitor.py:43] wrapper: Traceback (most recent call last):
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/cloud_adapter/cloud_monitor.py", line 34, in wrapper
    result = run_func(*args, **kwargs)
  File "/home/ma-user/work/mindformers/research/baichuan2/run_baichuan2.py", line 169, in main
    trainer.finetune(finetune_checkpoint=ckpt, auto_trans_ckpt=config.auto_trans_ckpt, resume_training=resume)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/_checkparam.py", line 1313, in wrapper
    return func(*args, **kwargs)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/trainer.py", line 520, in finetune
    self.trainer.train(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/causal_language_modeling/causal_language_modeling.py", line 97, in train
    self.training_process(
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/trainer/base_trainer.py", line 654, in training_process
    check_rules(config, mode='train', network=network, dataset=dataset)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 219, in check_rules
    _check_parallel(config)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindformers/tools/check_rules.py", line 134, in _check_parallel
    raise ValueError(f"The parallel config data_parallel * model_parallel * pipeline_stage should "
ValueError: The parallel config data_parallel * model_parallel * pipeline_stage should be equal to device_num, but get dp*mp*pp = 4*1*1 = 4 != device_num(8)

[INFO] 2024-06-14 10:49:38,123 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-14 10:49:38,123 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: baichuan2_7b_lora
[WARNING] 2024-06-14 10:49:38,124 [mindformers/trainer/base_trainer.py:131] __init__: Input model name is not in the supported list or unspecified.
[WARNING] 2024-06-14 10:49:38,124 [mindformers/trainer/base_trainer.py:132] __init__: See the list of supported task and model name: OrderedDict([('general', OrderedDict([('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/general/run_general_task.yaml')])), ('masked_image_modeling', OrderedDict([('mae_vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml')])), ('image_classification', OrderedDict([('vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('mindspore/vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('mindspore/swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml')])), ('fill_mask', OrderedDict([('bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_base_uncased.yaml'), ('bert_tiny_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml')])), ('contrastive_language_image_pretrain', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('blip2_stage1_vit_g', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml'), ('blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml'), ('blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml')])), ('image_to_text_retrieval', OrderedDict([('blip2_stage1_evaluator', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml')])), ('zero_shot_image_classification', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml'), ('blip2_stage1_classification', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml')])), ('image_to_text_generation', OrderedDict([('itt_blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml'), ('itt_blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml')])), ('translation', OrderedDict([('t5_small', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml'), ('t5_tiny', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_tiny_on_wmt16.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml')])), ('text_classification', OrderedDict([('txtcls_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml'), ('txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('mindspore/txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('gpt2_txtcls', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_txtcls.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml')])), ('token_classification', OrderedDict([('tokcls_bert_base_chinese', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml'), ('tokcls_bert_base_chinese_cluener', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml')])), ('question_answering', OrderedDict([('qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('qa_bert_base_uncased_squad', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('mindspore/qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml')])), ('text_generation', OrderedDict([('gpt2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml'), ('gpt2_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_lora.yaml'), ('gpt2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_13b.yaml'), ('gpt2_52b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_52b.yaml'), ('gpt2_xl', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl.yaml'), ('gpt2_xl_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl_lora.yaml'), ('llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b.yaml'), ('llama_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_13b.yaml'), ('llama_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_65b.yaml'), ('llama2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_7b.yaml'), ('llama2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_13b.yaml'), ('llama2_70b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_70b.yaml'), ('codellama_34b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codellama/run_codellama_34b_910b.yaml'), ('llama_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b_lora.yaml'), ('pangualpha_2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_2_6b.yaml'), ('pangualpha_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_13b.yaml'), ('glm_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_finetune.yaml'), ('glm_6b_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_infer.yaml'), ('glm_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora.yaml'), ('glm_6b_lora_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora_infer.yaml'), ('glm2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b.yaml'), ('glm2_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_lora.yaml'), ('glm2_6b_ptuning2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_ptuning2.yaml'), ('glm3_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm3/run_glm3_6b.yaml'), ('codegeex2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codegeex2/run_codegeex2_6b.yaml'), ('bloom_560m', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_560m.yaml'), ('bloom_7.1b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_7.1b.yaml'), ('bloom_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_65b.yaml'), ('bloom_176b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_176b.yaml'), ('baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan/run_baichuan_7b.yaml'), ('baichuan2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_7b.yaml'), ('baichuan2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_13b.yaml'), ('ziya_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/ziya/run_ziya_13b.yaml'), ('skywork_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/skywork/run_skywork_13b.yaml'), ('internlm_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b.yaml'), ('internlm_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b_lora.yaml'), ('qwen_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b.yaml'), ('qwen_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b_lora.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml')])), ('segment_anything', OrderedDict([('sam_vit_b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-b.yaml'), ('sam_vit_l', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-l.yaml'), ('sam_vit_h', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml')]))])
[WARNING] 2024-06-14 10:49:38,125 [mindformers/trainer/base_trainer.py:133] __init__: The default model config: /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml will now be used for the text_generation task 
[INFO] 2024-06-14 10:49:38,125 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
[INFO] 2024-06-14 10:49:38,126 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
[INFO] 2024-06-14 10:49:38,126 [mindformers/trainer/base_trainer.py:196] _check_global_batch_size_for_auto_parallel: The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 16 = 2 * 8 * 1 * 1
[INFO] 2024-06-14 10:49:38,126 [mindformers/trainer/base_trainer.py:624] training_process: .........Build Dataset For Train..........
[INFO] 2024-06-14 10:49:38,126 [mindformers/trainer/base_trainer.py:353] create_train_dataset: .........Build Dataset From Config..........
[INFO] 2024-06-14 10:49:38,127 [mindformers/dataset/causal_language_model_dataset.py:166] __new__: Now Create Causal Language Model Dataset.
[INFO] 2024-06-14 10:49:38,133 [mindformers/trainer/utils.py:155] check_runner_config: Will be Training epochs:1, sink_size:2
[INFO] 2024-06-14 10:49:38,133 [mindformers/trainer/utils.py:157] check_runner_config: Create training dataset finish, dataset size:625
[INFO] 2024-06-14 10:49:38,134 [mindformers/trainer/base_trainer.py:657] training_process: .........Build Net For Train..........
[INFO] 2024-06-14 10:49:38,134 [mindformers/trainer/base_trainer.py:388] create_network: .........Build Network From Config..........
[WARNING] 2024-06-14 10:49:38,134 [mindformers/models/llama/llama_config.py:185] __init__: Argument `compute_in_2d` is deprecated.
[INFO] 2024-06-14 10:49:38,135 [mindformers/version_control.py:60] decorator: The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
[INFO] 2024-06-14 10:49:38,135 [mindformers/version_control.py:64] decorator: 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
[INFO] 2024-06-14 10:49:38,135 [mindformers/version_control.py:73] decorator: The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
[WARNING] 2024-06-14 10:49:38,174 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,195 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,215 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,236 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,258 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,279 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,299 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,319 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,340 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 10:49:38,360 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[INFO] 2024-06-14 10:49:38,830 [mindformers/models/base_model.py:117] load_checkpoint: model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[INFO] 2024-06-14 10:49:39,137 [mindformers/models/base_model.py:117] load_checkpoint: model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[INFO] 2024-06-14 10:49:39,160 [mindformers/trainer/base_trainer.py:539] count_parameters: Network Parameters: 0 M.
[INFO] 2024-06-14 10:49:39,160 [mindformers/trainer/base_trainer.py:682] training_process: .........Build Optimizer For Train..........
[INFO] 2024-06-14 10:49:39,161 [mindformers/trainer/base_trainer.py:435] create_optimizer_scheduler: .........Build Optimizer From Config..........
[INFO] 2024-06-14 10:49:39,161 [mindformers/trainer/base_trainer.py:468] create_lr_scheduler: .........Build LR Schedule From Config..........
[WARNING] 2024-06-14 10:49:39,166 [mindformers/trainer/optimizer_grouped_parameters.py:74] get_optimizer_grouped_parameters: dynamic_lr_schedule will be reset and invalid when layer_scale is False.
[INFO] 2024-06-14 10:49:39,171 [mindformers/trainer/optimizer_grouped_parameters.py:113] get_optimizer_grouped_parameters: Param groups = {
  "decay": {
    "weight_decay": 0.0001,
    "params": [
      "model.layers.0.attention.wq.mindpet_delta_lora_a",
      "model.layers.0.attention.wq.mindpet_delta_lora_b",
      "model.layers.0.attention.wk.mindpet_delta_lora_a",
      "model.layers.0.attention.wk.mindpet_delta_lora_b",
      "model.layers.0.attention.wv.mindpet_delta_lora_a",
      "model.layers.0.attention.wv.mindpet_delta_lora_b",
      "model.layers.1.attention.wq.mindpet_delta_lora_a",
      "model.layers.1.attention.wq.mindpet_delta_lora_b",
      "model.layers.1.attention.wk.mindpet_delta_lora_a",
      "model.layers.1.attention.wk.mindpet_delta_lora_b",
      "model.layers.1.attention.wv.mindpet_delta_lora_a",
      "model.layers.1.attention.wv.mindpet_delta_lora_b",
      "model.layers.2.attention.wq.mindpet_delta_lora_a",
      "model.layers.2.attention.wq.mindpet_delta_lora_b",
      "model.layers.2.attention.wk.mindpet_delta_lora_a",
      "model.layers.2.attention.wk.mindpet_delta_lora_b",
      "model.layers.2.attention.wv.mindpet_delta_lora_a",
      "model.layers.2.attention.wv.mindpet_delta_lora_b",
      "model.layers.3.attention.wq.mindpet_delta_lora_a",
      "model.layers.3.attention.wq.mindpet_delta_lora_b",
      "model.layers.3.attention.wk.mindpet_delta_lora_a",
      "model.layers.3.attention.wk.mindpet_delta_lora_b",
      "model.layers.3.attention.wv.mindpet_delta_lora_a",
      "model.layers.3.attention.wv.mindpet_delta_lora_b",
      "model.layers.4.attention.wq.mindpet_delta_lora_a",
      "model.layers.4.attention.wq.mindpet_delta_lora_b",
      "model.layers.4.attention.wk.mindpet_delta_lora_a",
      "model.layers.4.attention.wk.mindpet_delta_lora_b",
      "model.layers.4.attention.wv.mindpet_delta_lora_a",
      "model.layers.4.attention.wv.mindpet_delta_lora_b",
      "model.layers.5.attention.wq.mindpet_delta_lora_a",
      "model.layers.5.attention.wq.mindpet_delta_lora_b",
      "model.layers.5.attention.wk.mindpet_delta_lora_a",
      "model.layers.5.attention.wk.mindpet_delta_lora_b",
      "model.layers.5.attention.wv.mindpet_delta_lora_a",
      "model.layers.5.attention.wv.mindpet_delta_lora_b",
      "model.layers.6.attention.wq.mindpet_delta_lora_a",
      "model.layers.6.attention.wq.mindpet_delta_lora_b",
      "model.layers.6.attention.wk.mindpet_delta_lora_a",
      "model.layers.6.attention.wk.mindpet_delta_lora_b",
      "model.layers.6.attention.wv.mindpet_delta_lora_a",
      "model.layers.6.attention.wv.mindpet_delta_lora_b",
      "model.layers.7.attention.wq.mindpet_delta_lora_a",
      "model.layers.7.attention.wq.mindpet_delta_lora_b",
      "model.layers.7.attention.wk.mindpet_delta_lora_a",
      "model.layers.7.attention.wk.mindpet_delta_lora_b",
      "model.layers.7.attention.wv.mindpet_delta_lora_a",
      "model.layers.7.attention.wv.mindpet_delta_lora_b",
      "model.layers.8.attention.wq.mindpet_delta_lora_a",
      "model.layers.8.attention.wq.mindpet_delta_lora_b",
      "model.layers.8.attention.wk.mindpet_delta_lora_a",
      "model.layers.8.attention.wk.mindpet_delta_lora_b",
      "model.layers.8.attention.wv.mindpet_delta_lora_a",
      "model.layers.8.attention.wv.mindpet_delta_lora_b",
      "model.layers.9.attention.wq.mindpet_delta_lora_a",
      "model.layers.9.attention.wq.mindpet_delta_lora_b",
      "model.layers.9.attention.wk.mindpet_delta_lora_a",
      "model.layers.9.attention.wk.mindpet_delta_lora_b",
      "model.layers.9.attention.wv.mindpet_delta_lora_a",
      "model.layers.9.attention.wv.mindpet_delta_lora_b",
      "model.layers.10.attention.wq.mindpet_delta_lora_a",
      "model.layers.10.attention.wq.mindpet_delta_lora_b",
      "model.layers.10.attention.wk.mindpet_delta_lora_a",
      "model.layers.10.attention.wk.mindpet_delta_lora_b",
      "model.layers.10.attention.wv.mindpet_delta_lora_a",
      "model.layers.10.attention.wv.mindpet_delta_lora_b",
      "model.layers.11.attention.wq.mindpet_delta_lora_a",
      "model.layers.11.attention.wq.mindpet_delta_lora_b",
      "model.layers.11.attention.wk.mindpet_delta_lora_a",
      "model.layers.11.attention.wk.mindpet_delta_lora_b",
      "model.layers.11.attention.wv.mindpet_delta_lora_a",
      "model.layers.11.attention.wv.mindpet_delta_lora_b",
      "model.layers.12.attention.wq.mindpet_delta_lora_a",
      "model.layers.12.attention.wq.mindpet_delta_lora_b",
      "model.layers.12.attention.wk.mindpet_delta_lora_a",
      "model.layers.12.attention.wk.mindpet_delta_lora_b",
      "model.layers.12.attention.wv.mindpet_delta_lora_a",
      "model.layers.12.attention.wv.mindpet_delta_lora_b",
      "model.layers.13.attention.wq.mindpet_delta_lora_a",
      "model.layers.13.attention.wq.mindpet_delta_lora_b",
      "model.layers.13.attention.wk.mindpet_delta_lora_a",
      "model.layers.13.attention.wk.mindpet_delta_lora_b",
      "model.layers.13.attention.wv.mindpet_delta_lora_a",
      "model.layers.13.attention.wv.mindpet_delta_lora_b",
      "model.layers.14.attention.wq.mindpet_delta_lora_a",
      "model.layers.14.attention.wq.mindpet_delta_lora_b",
      "model.layers.14.attention.wk.mindpet_delta_lora_a",
      "model.layers.14.attention.wk.mindpet_delta_lora_b",
      "model.layers.14.attention.wv.mindpet_delta_lora_a",
      "model.layers.14.attention.wv.mindpet_delta_lora_b",
      "model.layers.15.attention.wq.mindpet_delta_lora_a",
      "model.layers.15.attention.wq.mindpet_delta_lora_b",
      "model.layers.15.attention.wk.mindpet_delta_lora_a",
      "model.layers.15.attention.wk.mindpet_delta_lora_b",
      "model.layers.15.attention.wv.mindpet_delta_lora_a",
      "model.layers.15.attention.wv.mindpet_delta_lora_b",
      "model.layers.16.attention.wq.mindpet_delta_lora_a",
      "model.layers.16.attention.wq.mindpet_delta_lora_b",
      "model.layers.16.attention.wk.mindpet_delta_lora_a",
      "model.layers.16.attention.wk.mindpet_delta_lora_b",
      "model.layers.16.attention.wv.mindpet_delta_lora_a",
      "model.layers.16.attention.wv.mindpet_delta_lora_b",
      "model.layers.17.attention.wq.mindpet_delta_lora_a",
      "model.layers.17.attention.wq.mindpet_delta_lora_b",
      "model.layers.17.attention.wk.mindpet_delta_lora_a",
      "model.layers.17.attention.wk.mindpet_delta_lora_b",
      "model.layers.17.attention.wv.mindpet_delta_lora_a",
      "model.layers.17.attention.wv.mindpet_delta_lora_b",
      "model.layers.18.attention.wq.mindpet_delta_lora_a",
      "model.layers.18.attention.wq.mindpet_delta_lora_b",
      "model.layers.18.attention.wk.mindpet_delta_lora_a",
      "model.layers.18.attention.wk.mindpet_delta_lora_b",
      "model.layers.18.attention.wv.mindpet_delta_lora_a",
      "model.layers.18.attention.wv.mindpet_delta_lora_b",
      "model.layers.19.attention.wq.mindpet_delta_lora_a",
      "model.layers.19.attention.wq.mindpet_delta_lora_b",
      "model.layers.19.attention.wk.mindpet_delta_lora_a",
      "model.layers.19.attention.wk.mindpet_delta_lora_b",
      "model.layers.19.attention.wv.mindpet_delta_lora_a",
      "model.layers.19.attention.wv.mindpet_delta_lora_b",
      "model.layers.20.attention.wq.mindpet_delta_lora_a",
      "model.layers.20.attention.wq.mindpet_delta_lora_b",
      "model.layers.20.attention.wk.mindpet_delta_lora_a",
      "model.layers.20.attention.wk.mindpet_delta_lora_b",
      "model.layers.20.attention.wv.mindpet_delta_lora_a",
      "model.layers.20.attention.wv.mindpet_delta_lora_b",
      "model.layers.21.attention.wq.mindpet_delta_lora_a",
      "model.layers.21.attention.wq.mindpet_delta_lora_b",
      "model.layers.21.attention.wk.mindpet_delta_lora_a",
      "model.layers.21.attention.wk.mindpet_delta_lora_b",
      "model.layers.21.attention.wv.mindpet_delta_lora_a",
      "model.layers.21.attention.wv.mindpet_delta_lora_b",
      "model.layers.22.attention.wq.mindpet_delta_lora_a",
      "model.layers.22.attention.wq.mindpet_delta_lora_b",
      "model.layers.22.attention.wk.mindpet_delta_lora_a",
      "model.layers.22.attention.wk.mindpet_delta_lora_b",
      "model.layers.22.attention.wv.mindpet_delta_lora_a",
      "model.layers.22.attention.wv.mindpet_delta_lora_b",
      "model.layers.23.attention.wq.mindpet_delta_lora_a",
      "model.layers.23.attention.wq.mindpet_delta_lora_b",
      "model.layers.23.attention.wk.mindpet_delta_lora_a",
      "model.layers.23.attention.wk.mindpet_delta_lora_b",
      "model.layers.23.attention.wv.mindpet_delta_lora_a",
      "model.layers.23.attention.wv.mindpet_delta_lora_b",
      "model.layers.24.attention.wq.mindpet_delta_lora_a",
      "model.layers.24.attention.wq.mindpet_delta_lora_b",
      "model.layers.24.attention.wk.mindpet_delta_lora_a",
      "model.layers.24.attention.wk.mindpet_delta_lora_b",
      "model.layers.24.attention.wv.mindpet_delta_lora_a",
      "model.layers.24.attention.wv.mindpet_delta_lora_b",
      "model.layers.25.attention.wq.mindpet_delta_lora_a",
      "model.layers.25.attention.wq.mindpet_delta_lora_b",
      "model.layers.25.attention.wk.mindpet_delta_lora_a",
      "model.layers.25.attention.wk.mindpet_delta_lora_b",
      "model.layers.25.attention.wv.mindpet_delta_lora_a",
      "model.layers.25.attention.wv.mindpet_delta_lora_b",
      "model.layers.26.attention.wq.mindpet_delta_lora_a",
      "model.layers.26.attention.wq.mindpet_delta_lora_b",
      "model.layers.26.attention.wk.mindpet_delta_lora_a",
      "model.layers.26.attention.wk.mindpet_delta_lora_b",
      "model.layers.26.attention.wv.mindpet_delta_lora_a",
      "model.layers.26.attention.wv.mindpet_delta_lora_b",
      "model.layers.27.attention.wq.mindpet_delta_lora_a",
      "model.layers.27.attention.wq.mindpet_delta_lora_b",
      "model.layers.27.attention.wk.mindpet_delta_lora_a",
      "model.layers.27.attention.wk.mindpet_delta_lora_b",
      "model.layers.27.attention.wv.mindpet_delta_lora_a",
      "model.layers.27.attention.wv.mindpet_delta_lora_b",
      "model.layers.28.attention.wq.mindpet_delta_lora_a",
      "model.layers.28.attention.wq.mindpet_delta_lora_b",
      "model.layers.28.attention.wk.mindpet_delta_lora_a",
      "model.layers.28.attention.wk.mindpet_delta_lora_b",
      "model.layers.28.attention.wv.mindpet_delta_lora_a",
      "model.layers.28.attention.wv.mindpet_delta_lora_b",
      "model.layers.29.attention.wq.mindpet_delta_lora_a",
      "model.layers.29.attention.wq.mindpet_delta_lora_b",
      "model.layers.29.attention.wk.mindpet_delta_lora_a",
      "model.layers.29.attention.wk.mindpet_delta_lora_b",
      "model.layers.29.attention.wv.mindpet_delta_lora_a",
      "model.layers.29.attention.wv.mindpet_delta_lora_b",
      "model.layers.30.attention.wq.mindpet_delta_lora_a",
      "model.layers.30.attention.wq.mindpet_delta_lora_b",
      "model.layers.30.attention.wk.mindpet_delta_lora_a",
      "model.layers.30.attention.wk.mindpet_delta_lora_b",
      "model.layers.30.attention.wv.mindpet_delta_lora_a",
      "model.layers.30.attention.wv.mindpet_delta_lora_b",
      "model.layers.31.attention.wq.mindpet_delta_lora_a",
      "model.layers.31.attention.wq.mindpet_delta_lora_b",
      "model.layers.31.attention.wk.mindpet_delta_lora_a",
      "model.layers.31.attention.wk.mindpet_delta_lora_b",
      "model.layers.31.attention.wv.mindpet_delta_lora_a",
      "model.layers.31.attention.wv.mindpet_delta_lora_b"
    ]
  }
}
[INFO] 2024-06-14 10:49:39,300 [mindformers/trainer/base_trainer.py:688] training_process: .........Build Running Wrapper From Config For Train..........
[INFO] 2024-06-14 10:49:39,301 [mindformers/trainer/base_trainer.py:505] create_model_wrapper: .........Build Model Wrapper for Train From Config..........
[INFO] 2024-06-14 10:49:39,310 [mindformers/trainer/base_trainer.py:695] training_process: .........Build Callbacks For Train..........
[INFO] 2024-06-14 10:49:39,312 [mindformers/core/callback/callback.py:531] __init__: Integrated_save is changed to False when using auto_parallel.
[INFO] 2024-06-14 10:49:39,313 [mindformers/trainer/base_trainer.py:730] training_process: .........Starting Init Train Model..........
[INFO] 2024-06-14 10:49:39,313 [mindformers/trainer/utils.py:342] transform_and_load_checkpoint: .........Building model.........
[INFO] 2024-06-14 10:53:44,608 [mindformers/trainer/utils.py:355] transform_and_load_checkpoint: /home/ma-user/work/mindformers/research/output is_share_disk: False
[INFO] 2024-06-14 10:53:44,613 [mindformers/trainer/utils.py:356] transform_and_load_checkpoint: world_size: 8
[INFO] 2024-06-14 10:53:44,623 [mindformers/trainer/utils.py:537] get_src_and_dst_strategy: .........Collecting strategy.........
[INFO] 2024-06-14 10:53:44,632 [mindformers/trainer/utils.py:544] get_src_and_dst_strategy: pipeline_stage = 1, strategy using ./output/strategy/ckpt_strategy_rank_0_rank_0.ckpt
[INFO] 2024-06-14 10:53:44,636 [mindformers/trainer/utils.py:400] check_ckpt_for_transform: Make soft link of checkpoint file from /home/ma-user/work/mindformers/research/baichuan2/models to ./output/softlink_ckpt/models
[INFO] 2024-06-14 10:53:44,647 [mindformers/trainer/utils.py:584] transform_ckpt: .........Transforming ckpt.........
[INFO] 2024-06-14 10:53:44,650 [mindformers/trainer/utils.py:585] transform_ckpt: Src ckpt strategy: None
[INFO] 2024-06-14 10:53:44,655 [mindformers/trainer/utils.py:586] transform_ckpt: Src ckpt: ./output/softlink_ckpt/models
[INFO] 2024-06-14 10:53:44,658 [mindformers/trainer/utils.py:587] transform_ckpt: Dst ckpt strategy: ./output/strategy/ckpt_strategy_rank_0_rank_0.ckpt
[INFO] 2024-06-14 10:53:44,660 [mindformers/trainer/utils.py:588] transform_ckpt: Dst ckpt: ./output/transformed_checkpoint/models
[INFO] 2024-06-14 10:57:32,584 [mindformers/trainer/utils.py:595] transform_ckpt: .........Transform succeed!.........
[INFO] 2024-06-14 10:57:32,587 [mindformers/trainer/utils.py:727] show_progress: Transforming checkpoint: ||100%
[INFO] 2024-06-14 10:57:32,588 [mindformers/trainer/utils.py:733] load_ckpt: .............Start load checkpoint from checkpoint..................
[INFO] 2024-06-14 10:57:32,588 [mindformers/trainer/utils.py:251] load_distributed_checkpoint: When distributed loads are sliced weights,load_checkpoint should be a checkpoint directory containing the directory of rank_{0-*},The directory structure is as follows: **checkpoint_root_dir/rank_{0-*}/**.ckpt
[INFO] 2024-06-14 10:57:40,206 [mindformers/trainer/utils.py:264] load_distributed_checkpoint: Distribute load is success.
[INFO] 2024-06-14 10:57:40,207 [mindformers/trainer/utils.py:740] load_ckpt: loaded checkpoint: ./output/transformed_checkpoint/models
[INFO] 2024-06-14 10:57:43,925 [mindformers/trainer/utils.py:767] load_ckpt: Network parameters are not loaded: (['model.layers.0.attention.wq.mindpet_delta_lora_a', 'model.layers.0.attention.wq.mindpet_delta_lora_b', 'model.layers.0.attention.wk.mindpet_delta_lora_a', 'model.layers.0.attention.wk.mindpet_delta_lora_b', 'model.layers.0.attention.wv.mindpet_delta_lora_a', 'model.layers.0.attention.wv.mindpet_delta_lora_b', 'model.layers.1.attention.wq.mindpet_delta_lora_a', 'model.layers.1.attention.wq.mindpet_delta_lora_b', 'model.layers.1.attention.wk.mindpet_delta_lora_a', 'model.layers.1.attention.wk.mindpet_delta_lora_b', 'model.layers.1.attention.wv.mindpet_delta_lora_a', 'model.layers.1.attention.wv.mindpet_delta_lora_b', 'model.layers.2.attention.wq.mindpet_delta_lora_a', 'model.layers.2.attention.wq.mindpet_delta_lora_b', 'model.layers.2.attention.wk.mindpet_delta_lora_a', 'model.layers.2.attention.wk.mindpet_delta_lora_b', 'model.layers.2.attention.wv.mindpet_delta_lora_a', 'model.layers.2.attention.wv.mindpet_delta_lora_b', 'model.layers.3.attention.wq.mindpet_delta_lora_a', 'model.layers.3.attention.wq.mindpet_delta_lora_b', 'model.layers.3.attention.wk.mindpet_delta_lora_a', 'model.layers.3.attention.wk.mindpet_delta_lora_b', 'model.layers.3.attention.wv.mindpet_delta_lora_a', 'model.layers.3.attention.wv.mindpet_delta_lora_b', 'model.layers.4.attention.wq.mindpet_delta_lora_a', 'model.layers.4.attention.wq.mindpet_delta_lora_b', 'model.layers.4.attention.wk.mindpet_delta_lora_a', 'model.layers.4.attention.wk.mindpet_delta_lora_b', 'model.layers.4.attention.wv.mindpet_delta_lora_a', 'model.layers.4.attention.wv.mindpet_delta_lora_b', 'model.layers.5.attention.wq.mindpet_delta_lora_a', 'model.layers.5.attention.wq.mindpet_delta_lora_b', 'model.layers.5.attention.wk.mindpet_delta_lora_a', 'model.layers.5.attention.wk.mindpet_delta_lora_b', 'model.layers.5.attention.wv.mindpet_delta_lora_a', 'model.layers.5.attention.wv.mindpet_delta_lora_b', 'model.layers.6.attention.wq.mindpet_delta_lora_a', 'model.layers.6.attention.wq.mindpet_delta_lora_b', 'model.layers.6.attention.wk.mindpet_delta_lora_a', 'model.layers.6.attention.wk.mindpet_delta_lora_b', 'model.layers.6.attention.wv.mindpet_delta_lora_a', 'model.layers.6.attention.wv.mindpet_delta_lora_b', 'model.layers.7.attention.wq.mindpet_delta_lora_a', 'model.layers.7.attention.wq.mindpet_delta_lora_b', 'model.layers.7.attention.wk.mindpet_delta_lora_a', 'model.layers.7.attention.wk.mindpet_delta_lora_b', 'model.layers.7.attention.wv.mindpet_delta_lora_a', 'model.layers.7.attention.wv.mindpet_delta_lora_b', 'model.layers.8.attention.wq.mindpet_delta_lora_a', 'model.layers.8.attention.wq.mindpet_delta_lora_b', 'model.layers.8.attention.wk.mindpet_delta_lora_a', 'model.layers.8.attention.wk.mindpet_delta_lora_b', 'model.layers.8.attention.wv.mindpet_delta_lora_a', 'model.layers.8.attention.wv.mindpet_delta_lora_b', 'model.layers.9.attention.wq.mindpet_delta_lora_a', 'model.layers.9.attention.wq.mindpet_delta_lora_b', 'model.layers.9.attention.wk.mindpet_delta_lora_a', 'model.layers.9.attention.wk.mindpet_delta_lora_b', 'model.layers.9.attention.wv.mindpet_delta_lora_a', 'model.layers.9.attention.wv.mindpet_delta_lora_b', 'model.layers.10.attention.wq.mindpet_delta_lora_a', 'model.layers.10.attention.wq.mindpet_delta_lora_b', 'model.layers.10.attention.wk.mindpet_delta_lora_a', 'model.layers.10.attention.wk.mindpet_delta_lora_b', 'model.layers.10.attention.wv.mindpet_delta_lora_a', 'model.layers.10.attention.wv.mindpet_delta_lora_b', 'model.layers.11.attention.wq.mindpet_delta_lora_a', 'model.layers.11.attention.wq.mindpet_delta_lora_b', 'model.layers.11.attention.wk.mindpet_delta_lora_a', 'model.layers.11.attention.wk.mindpet_delta_lora_b', 'model.layers.11.attention.wv.mindpet_delta_lora_a', 'model.layers.11.attention.wv.mindpet_delta_lora_b', 'model.layers.12.attention.wq.mindpet_delta_lora_a', 'model.layers.12.attention.wq.mindpet_delta_lora_b', 'model.layers.12.attention.wk.mindpet_delta_lora_a', 'model.layers.12.attention.wk.mindpet_delta_lora_b', 'model.layers.12.attention.wv.mindpet_delta_lora_a', 'model.layers.12.attention.wv.mindpet_delta_lora_b', 'model.layers.13.attention.wq.mindpet_delta_lora_a', 'model.layers.13.attention.wq.mindpet_delta_lora_b', 'model.layers.13.attention.wk.mindpet_delta_lora_a', 'model.layers.13.attention.wk.mindpet_delta_lora_b', 'model.layers.13.attention.wv.mindpet_delta_lora_a', 'model.layers.13.attention.wv.mindpet_delta_lora_b', 'model.layers.14.attention.wq.mindpet_delta_lora_a', 'model.layers.14.attention.wq.mindpet_delta_lora_b', 'model.layers.14.attention.wk.mindpet_delta_lora_a', 'model.layers.14.attention.wk.mindpet_delta_lora_b', 'model.layers.14.attention.wv.mindpet_delta_lora_a', 'model.layers.14.attention.wv.mindpet_delta_lora_b', 'model.layers.15.attention.wq.mindpet_delta_lora_a', 'model.layers.15.attention.wq.mindpet_delta_lora_b', 'model.layers.15.attention.wk.mindpet_delta_lora_a', 'model.layers.15.attention.wk.mindpet_delta_lora_b', 'model.layers.15.attention.wv.mindpet_delta_lora_a', 'model.layers.15.attention.wv.mindpet_delta_lora_b', 'model.layers.16.attention.wq.mindpet_delta_lora_a', 'model.layers.16.attention.wq.mindpet_delta_lora_b', 'model.layers.16.attention.wk.mindpet_delta_lora_a', 'model.layers.16.attention.wk.mindpet_delta_lora_b', 'model.layers.16.attention.wv.mindpet_delta_lora_a', 'model.layers.16.attention.wv.mindpet_delta_lora_b', 'model.layers.17.attention.wq.mindpet_delta_lora_a', 'model.layers.17.attention.wq.mindpet_delta_lora_b', 'model.layers.17.attention.wk.mindpet_delta_lora_a', 'model.layers.17.attention.wk.mindpet_delta_lora_b', 'model.layers.17.attention.wv.mindpet_delta_lora_a', 'model.layers.17.attention.wv.mindpet_delta_lora_b', 'model.layers.18.attention.wq.mindpet_delta_lora_a', 'model.layers.18.attention.wq.mindpet_delta_lora_b', 'model.layers.18.attention.wk.mindpet_delta_lora_a', 'model.layers.18.attention.wk.mindpet_delta_lora_b', 'model.layers.18.attention.wv.mindpet_delta_lora_a', 'model.layers.18.attention.wv.mindpet_delta_lora_b', 'model.layers.19.attention.wq.mindpet_delta_lora_a', 'model.layers.19.attention.wq.mindpet_delta_lora_b', 'model.layers.19.attention.wk.mindpet_delta_lora_a', 'model.layers.19.attention.wk.mindpet_delta_lora_b', 'model.layers.19.attention.wv.mindpet_delta_lora_a', 'model.layers.19.attention.wv.mindpet_delta_lora_b', 'model.layers.20.attention.wq.mindpet_delta_lora_a', 'model.layers.20.attention.wq.mindpet_delta_lora_b', 'model.layers.20.attention.wk.mindpet_delta_lora_a', 'model.layers.20.attention.wk.mindpet_delta_lora_b', 'model.layers.20.attention.wv.mindpet_delta_lora_a', 'model.layers.20.attention.wv.mindpet_delta_lora_b', 'model.layers.21.attention.wq.mindpet_delta_lora_a', 'model.layers.21.attention.wq.mindpet_delta_lora_b', 'model.layers.21.attention.wk.mindpet_delta_lora_a', 'model.layers.21.attention.wk.mindpet_delta_lora_b', 'model.layers.21.attention.wv.mindpet_delta_lora_a', 'model.layers.21.attention.wv.mindpet_delta_lora_b', 'model.layers.22.attention.wq.mindpet_delta_lora_a', 'model.layers.22.attention.wq.mindpet_delta_lora_b', 'model.layers.22.attention.wk.mindpet_delta_lora_a', 'model.layers.22.attention.wk.mindpet_delta_lora_b', 'model.layers.22.attention.wv.mindpet_delta_lora_a', 'model.layers.22.attention.wv.mindpet_delta_lora_b', 'model.layers.23.attention.wq.mindpet_delta_lora_a', 'model.layers.23.attention.wq.mindpet_delta_lora_b', 'model.layers.23.attention.wk.mindpet_delta_lora_a', 'model.layers.23.attention.wk.mindpet_delta_lora_b', 'model.layers.23.attention.wv.mindpet_delta_lora_a', 'model.layers.23.attention.wv.mindpet_delta_lora_b', 'model.layers.24.attention.wq.mindpet_delta_lora_a', 'model.layers.24.attention.wq.mindpet_delta_lora_b', 'model.layers.24.attention.wk.mindpet_delta_lora_a', 'model.layers.24.attention.wk.mindpet_delta_lora_b', 'model.layers.24.attention.wv.mindpet_delta_lora_a', 'model.layers.24.attention.wv.mindpet_delta_lora_b', 'model.layers.25.attention.wq.mindpet_delta_lora_a', 'model.layers.25.attention.wq.mindpet_delta_lora_b', 'model.layers.25.attention.wk.mindpet_delta_lora_a', 'model.layers.25.attention.wk.mindpet_delta_lora_b', 'model.layers.25.attention.wv.mindpet_delta_lora_a', 'model.layers.25.attention.wv.mindpet_delta_lora_b', 'model.layers.26.attention.wq.mindpet_delta_lora_a', 'model.layers.26.attention.wq.mindpet_delta_lora_b', 'model.layers.26.attention.wk.mindpet_delta_lora_a', 'model.layers.26.attention.wk.mindpet_delta_lora_b', 'model.layers.26.attention.wv.mindpet_delta_lora_a', 'model.layers.26.attention.wv.mindpet_delta_lora_b', 'model.layers.27.attention.wq.mindpet_delta_lora_a', 'model.layers.27.attention.wq.mindpet_delta_lora_b', 'model.layers.27.attention.wk.mindpet_delta_lora_a', 'model.layers.27.attention.wk.mindpet_delta_lora_b', 'model.layers.27.attention.wv.mindpet_delta_lora_a', 'model.layers.27.attention.wv.mindpet_delta_lora_b', 'model.layers.28.attention.wq.mindpet_delta_lora_a', 'model.layers.28.attention.wq.mindpet_delta_lora_b', 'model.layers.28.attention.wk.mindpet_delta_lora_a', 'model.layers.28.attention.wk.mindpet_delta_lora_b', 'model.layers.28.attention.wv.mindpet_delta_lora_a', 'model.layers.28.attention.wv.mindpet_delta_lora_b', 'model.layers.29.attention.wq.mindpet_delta_lora_a', 'model.layers.29.attention.wq.mindpet_delta_lora_b', 'model.layers.29.attention.wk.mindpet_delta_lora_a', 'model.layers.29.attention.wk.mindpet_delta_lora_b', 'model.layers.29.attention.wv.mindpet_delta_lora_a', 'model.layers.29.attention.wv.mindpet_delta_lora_b', 'model.layers.30.attention.wq.mindpet_delta_lora_a', 'model.layers.30.attention.wq.mindpet_delta_lora_b', 'model.layers.30.attention.wk.mindpet_delta_lora_a', 'model.layers.30.attention.wk.mindpet_delta_lora_b', 'model.layers.30.attention.wv.mindpet_delta_lora_a', 'model.layers.30.attention.wv.mindpet_delta_lora_b', 'model.layers.31.attention.wq.mindpet_delta_lora_a', 'model.layers.31.attention.wq.mindpet_delta_lora_b', 'model.layers.31.attention.wk.mindpet_delta_lora_a', 'model.layers.31.attention.wk.mindpet_delta_lora_b', 'model.layers.31.attention.wv.mindpet_delta_lora_a', 'model.layers.31.attention.wv.mindpet_delta_lora_b'], [])
[INFO] 2024-06-14 10:57:43,925 [mindformers/trainer/base_trainer.py:765] training_process: .........Starting Training Model..........
[INFO] 2024-06-14 10:57:43,931 [mindformers/trainer/base_trainer.py:768] training_process: .........Model Compiling, Please Wait a Moment...........
[INFO] 2024-06-14 10:58:34,059 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[    2/  625], loss: 2.709, per_step_time: 25060ms, lr: 0.0, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:34,059 [mindformers/core/callback/callback.py:324] print_output_info:    0.3% |                                                  | 0.07981 samples/s/p  4:20:12 }
[INFO] 2024-06-14 10:58:35,545 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[    4/  625], loss: 2.639, per_step_time: 441ms, lr: 1.9999885e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:35,546 [mindformers/core/callback/callback.py:324] print_output_info:    0.6% |                                                  | 4.52960 samples/s/p  0:04:34 }
[INFO] 2024-06-14 10:58:36,412 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[    6/  625], loss: 2.516, per_step_time: 430ms, lr: 1.9998974e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:36,412 [mindformers/core/callback/callback.py:324] print_output_info:    1.0% |                                                  | 4.64895 samples/s/p  0:04:26 }
[INFO] 2024-06-14 10:58:37,272 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[    8/  625], loss: 2.745, per_step_time: 427ms, lr: 1.999715e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:37,272 [mindformers/core/callback/callback.py:324] print_output_info:    1.3% |                                                  | 4.68214 samples/s/p  0:04:23 }
[INFO] 2024-06-14 10:58:38,132 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   10/  625], loss: 2.405, per_step_time: 427ms, lr: 1.9994412e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:38,132 [mindformers/core/callback/callback.py:324] print_output_info:    1.6% |                                                  | 4.68071 samples/s/p  0:04:22 }
[INFO] 2024-06-14 10:58:38,992 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   12/  625], loss: 2.561, per_step_time: 427ms, lr: 1.9990763e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:38,992 [mindformers/core/callback/callback.py:324] print_output_info:    1.9% |                                                  | 4.68351 samples/s/p  0:04:21 }
[INFO] 2024-06-14 10:58:39,853 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   14/  625], loss: 2.280, per_step_time: 427ms, lr: 1.99862e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:39,855 [mindformers/core/callback/callback.py:324] print_output_info:    2.2% |                                                 | 4.67483 samples/s/p  0:04:21 }
[INFO] 2024-06-14 10:58:40,726 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   16/  625], loss: 2.301, per_step_time: 430ms, lr: 1.9980731e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:40,726 [mindformers/core/callback/callback.py:324] print_output_info:    2.6% |                                                 | 4.64747 samples/s/p  0:04:22 }
[INFO] 2024-06-14 10:58:41,586 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   18/  625], loss: 2.191, per_step_time: 427ms, lr: 1.9974346e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:41,587 [mindformers/core/callback/callback.py:324] print_output_info:    2.9% |                                                 | 4.67968 samples/s/p  0:04:19 }
[INFO] 2024-06-14 10:58:42,452 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   20/  625], loss: 2.278, per_step_time: 430ms, lr: 1.9967056e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:42,453 [mindformers/core/callback/callback.py:324] print_output_info:    3.2% |                                                 | 4.64896 samples/s/p  0:04:20 }
[INFO] 2024-06-14 10:58:43,321 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   22/  625], loss: 2.416, per_step_time: 431ms, lr: 1.9958854e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:43,321 [mindformers/core/callback/callback.py:324] print_output_info:    3.5% |                                                 | 4.63531 samples/s/p  0:04:20 }
[INFO] 2024-06-14 10:58:44,185 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   24/  625], loss: 2.048, per_step_time: 428ms, lr: 1.9949744e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:44,185 [mindformers/core/callback/callback.py:324] print_output_info:    3.8% |                                                 | 4.66230 samples/s/p  0:04:17 }
[INFO] 2024-06-14 10:58:45,051 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   26/  625], loss: 2.160, per_step_time: 430ms, lr: 1.9939727e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:45,052 [mindformers/core/callback/callback.py:324] print_output_info:    4.2% |                                                | 4.64652 samples/s/p  0:04:17 }
[INFO] 2024-06-14 10:58:45,918 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   28/  625], loss: 2.213, per_step_time: 430ms, lr: 1.9928804e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:45,919 [mindformers/core/callback/callback.py:324] print_output_info:    4.5% |                                                | 4.64790 samples/s/p  0:04:16 }
[INFO] 2024-06-14 10:58:46,783 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   30/  625], loss: 2.168, per_step_time: 429ms, lr: 1.9916975e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:46,783 [mindformers/core/callback/callback.py:324] print_output_info:    4.8% |                                                | 4.65987 samples/s/p  0:04:15 }
[INFO] 2024-06-14 10:58:47,646 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   32/  625], loss: 2.279, per_step_time: 428ms, lr: 1.9904242e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:47,647 [mindformers/core/callback/callback.py:324] print_output_info:    5.1% |                                                | 4.66338 samples/s/p  0:04:14 }
[INFO] 2024-06-14 10:58:48,508 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   34/  625], loss: 2.281, per_step_time: 428ms, lr: 1.989061e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:48,509 [mindformers/core/callback/callback.py:324] print_output_info:    5.4% |                                                | 4.67228 samples/s/p  0:04:12 }
[INFO] 2024-06-14 10:58:49,375 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   36/  625], loss: 2.263, per_step_time: 430ms, lr: 1.987607e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:49,376 [mindformers/core/callback/callback.py:324] print_output_info:    5.8% |                                                | 4.64890 samples/s/p  0:04:13 }
[INFO] 2024-06-14 10:58:50,236 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   38/  625], loss: 2.248, per_step_time: 427ms, lr: 1.9860634e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:50,237 [mindformers/core/callback/callback.py:324] print_output_info:    6.1% |                                               | 4.68176 samples/s/p  0:04:10 }
[INFO] 2024-06-14 10:58:51,098 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   40/  625], loss: 2.185, per_step_time: 427ms, lr: 1.9844298e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:51,099 [mindformers/core/callback/callback.py:324] print_output_info:    6.4% |                                               | 4.67464 samples/s/p  0:04:10 }
[INFO] 2024-06-14 10:58:51,963 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   42/  625], loss: 2.082, per_step_time: 429ms, lr: 1.9827066e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:51,964 [mindformers/core/callback/callback.py:324] print_output_info:    6.7% |                                               | 4.65556 samples/s/p  0:04:10 }
[INFO] 2024-06-14 10:58:52,823 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   44/  625], loss: 2.050, per_step_time: 426ms, lr: 1.980894e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:52,823 [mindformers/core/callback/callback.py:324] print_output_info:    7.0% |                                               | 4.68875 samples/s/p  0:04:07 }
[INFO] 2024-06-14 10:58:53,693 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   46/  625], loss: 2.180, per_step_time: 432ms, lr: 1.978992e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:53,694 [mindformers/core/callback/callback.py:324] print_output_info:    7.4% |                                               | 4.62887 samples/s/p  0:04:10 }
[INFO] 2024-06-14 10:58:54,548 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   48/  625], loss: 2.318, per_step_time: 424ms, lr: 1.9770008e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:54,549 [mindformers/core/callback/callback.py:324] print_output_info:    7.7% |                                               | 4.71021 samples/s/p  0:04:04 }
[INFO] 2024-06-14 10:58:55,405 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   50/  625], loss: 2.221, per_step_time: 425ms, lr: 1.9749208e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:55,405 [mindformers/core/callback/callback.py:324] print_output_info:    8.0% |                                              | 4.70030 samples/s/p  0:04:04 }
[INFO] 2024-06-14 10:58:56,261 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   52/  625], loss: 2.323, per_step_time: 425ms, lr: 1.9727522e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:56,262 [mindformers/core/callback/callback.py:324] print_output_info:    8.3% |                                              | 4.70470 samples/s/p  0:04:03 }
[INFO] 2024-06-14 10:58:57,130 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   54/  625], loss: 2.076, per_step_time: 431ms, lr: 1.9704948e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:57,130 [mindformers/core/callback/callback.py:324] print_output_info:    8.6% |                                              | 4.63569 samples/s/p  0:04:06 }
[INFO] 2024-06-14 10:58:58,013 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   56/  625], loss: 2.198, per_step_time: 438ms, lr: 1.9681494e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:58,013 [mindformers/core/callback/callback.py:324] print_output_info:    9.0% |                                              | 4.56094 samples/s/p  0:04:09 }
[INFO] 2024-06-14 10:58:58,881 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   58/  625], loss: 2.228, per_step_time: 431ms, lr: 1.965716e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:58,881 [mindformers/core/callback/callback.py:324] print_output_info:    9.3% |                                              | 4.63735 samples/s/p  0:04:04 }
[INFO] 2024-06-14 10:58:59,746 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   60/  625], loss: 1.997, per_step_time: 429ms, lr: 1.9631947e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:58:59,747 [mindformers/core/callback/callback.py:324] print_output_info:    9.6% |                                              | 4.65210 samples/s/p  0:04:02 }
[INFO] 2024-06-14 10:59:00,609 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   62/  625], loss: 2.338, per_step_time: 428ms, lr: 1.9605859e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:00,611 [mindformers/core/callback/callback.py:324] print_output_info:    9.9% |                                              | 4.67002 samples/s/p  0:04:01 }
[INFO] 2024-06-14 10:59:01,539 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   64/  625], loss: 2.319, per_step_time: 461ms, lr: 1.9578898e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:01,539 [mindformers/core/callback/callback.py:324] print_output_info:   10.2% |                                             | 4.33413 samples/s/p  0:04:18 }
[INFO] 2024-06-14 10:59:02,396 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   66/  625], loss: 2.388, per_step_time: 425ms, lr: 1.9551066e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:02,396 [mindformers/core/callback/callback.py:324] print_output_info:   10.6% |                                             | 4.70117 samples/s/p  0:03:57 }
[INFO] 2024-06-14 10:59:03,253 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   68/  625], loss: 2.202, per_step_time: 425ms, lr: 1.9522371e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:03,253 [mindformers/core/callback/callback.py:324] print_output_info:   10.9% |                                             | 4.70539 samples/s/p  0:03:56 }
[INFO] 2024-06-14 10:59:04,108 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   70/  625], loss: 2.292, per_step_time: 424ms, lr: 1.9492809e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:04,109 [mindformers/core/callback/callback.py:324] print_output_info:   11.2% |                                             | 4.70706 samples/s/p  0:03:55 }
[INFO] 2024-06-14 10:59:04,971 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   72/  625], loss: 2.164, per_step_time: 428ms, lr: 1.9462386e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:04,971 [mindformers/core/callback/callback.py:324] print_output_info:   11.5% |                                             | 4.67067 samples/s/p  0:03:56 }
[INFO] 2024-06-14 10:59:05,843 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   74/  625], loss: 2.223, per_step_time: 433ms, lr: 1.9431107e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:05,845 [mindformers/core/callback/callback.py:324] print_output_info:   11.8% |                                             | 4.61783 samples/s/p  0:03:58 }
[INFO] 2024-06-14 10:59:06,708 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   76/  625], loss: 2.103, per_step_time: 428ms, lr: 1.939897e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:06,708 [mindformers/core/callback/callback.py:324] print_output_info:   12.2% |                                            | 4.66467 samples/s/p  0:03:55 }
[INFO] 2024-06-14 10:59:07,578 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   78/  625], loss: 2.215, per_step_time: 432ms, lr: 1.9365983e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:07,579 [mindformers/core/callback/callback.py:324] print_output_info:   12.5% |                                            | 4.62721 samples/s/p  0:03:56 }
[INFO] 2024-06-14 10:59:08,453 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   80/  625], loss: 2.155, per_step_time: 434ms, lr: 1.9332148e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:08,454 [mindformers/core/callback/callback.py:324] print_output_info:   12.8% |                                            | 4.60077 samples/s/p  0:03:56 }
[INFO] 2024-06-14 10:59:09,309 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   82/  625], loss: 2.098, per_step_time: 425ms, lr: 1.929747e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:09,310 [mindformers/core/callback/callback.py:324] print_output_info:   13.1% |                                            | 4.70563 samples/s/p  0:03:50 }
[INFO] 2024-06-14 10:59:10,179 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   84/  625], loss: 1.905, per_step_time: 432ms, lr: 1.9261948e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:10,180 [mindformers/core/callback/callback.py:324] print_output_info:   13.4% |                                            | 4.62830 samples/s/p  0:03:53 }
[INFO] 2024-06-14 10:59:11,038 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   86/  625], loss: 2.012, per_step_time: 426ms, lr: 1.922559e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:11,040 [mindformers/core/callback/callback.py:324] print_output_info:   13.8% |                                            | 4.68741 samples/s/p  0:03:49 }
[INFO] 2024-06-14 10:59:11,898 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   88/  625], loss: 2.241, per_step_time: 426ms, lr: 1.9188397e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:11,899 [mindformers/core/callback/callback.py:324] print_output_info:   14.1% |                                           | 4.69204 samples/s/p  0:03:48 }
[INFO] 2024-06-14 10:59:12,755 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   90/  625], loss: 2.043, per_step_time: 425ms, lr: 1.9150375e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:12,756 [mindformers/core/callback/callback.py:324] print_output_info:   14.4% |                                           | 4.69994 samples/s/p  0:03:47 }
[INFO] 2024-06-14 10:59:13,611 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   92/  625], loss: 2.221, per_step_time: 425ms, lr: 1.9111525e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:13,612 [mindformers/core/callback/callback.py:324] print_output_info:   14.7% |                                           | 4.70507 samples/s/p  0:03:46 }
[INFO] 2024-06-14 10:59:14,470 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   94/  625], loss: 2.286, per_step_time: 426ms, lr: 1.9071855e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:14,470 [mindformers/core/callback/callback.py:324] print_output_info:   15.0% |                                           | 4.69203 samples/s/p  0:03:46 }
[INFO] 2024-06-14 10:59:15,336 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   96/  625], loss: 2.225, per_step_time: 430ms, lr: 1.9031364e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:15,336 [mindformers/core/callback/callback.py:324] print_output_info:   15.4% |                                           | 4.65035 samples/s/p  0:03:47 }
[INFO] 2024-06-14 10:59:16,216 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[   98/  625], loss: 2.233, per_step_time: 437ms, lr: 1.899006e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:16,218 [mindformers/core/callback/callback.py:324] print_output_info:   15.7% |                                           | 4.57467 samples/s/p  0:03:50 }
[INFO] 2024-06-14 10:59:17,084 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  100/  625], loss: 2.314, per_step_time: 430ms, lr: 1.8947945e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:17,085 [mindformers/core/callback/callback.py:324] print_output_info:   16.0% |                                          | 4.65005 samples/s/p  0:03:45 }
[INFO] 2024-06-14 10:59:17,948 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  102/  625], loss: 1.899, per_step_time: 429ms, lr: 1.8905026e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:17,949 [mindformers/core/callback/callback.py:324] print_output_info:   16.3% |                                          | 4.66158 samples/s/p  0:03:44 }
[INFO] 2024-06-14 10:59:18,802 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  104/  625], loss: 2.200, per_step_time: 423ms, lr: 1.8861305e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:18,803 [mindformers/core/callback/callback.py:324] print_output_info:   16.6% |                                          | 4.71770 samples/s/p  0:03:40 }
[INFO] 2024-06-14 10:59:19,656 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  106/  625], loss: 2.201, per_step_time: 423ms, lr: 1.8816785e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:19,656 [mindformers/core/callback/callback.py:324] print_output_info:   17.0% |                                          | 4.71975 samples/s/p  0:03:39 }
[INFO] 2024-06-14 10:59:20,515 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  108/  625], loss: 2.183, per_step_time: 426ms, lr: 1.8771472e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:20,516 [mindformers/core/callback/callback.py:324] print_output_info:   17.3% |                                          | 4.68678 samples/s/p  0:03:40 }
[INFO] 2024-06-14 10:59:21,385 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  110/  625], loss: 2.224, per_step_time: 431ms, lr: 1.8725375e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:21,387 [mindformers/core/callback/callback.py:324] print_output_info:   17.6% |                                          | 4.63176 samples/s/p  0:03:42 }
[INFO] 2024-06-14 10:59:22,261 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  112/  625], loss: 2.266, per_step_time: 434ms, lr: 1.8678491e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:22,262 [mindformers/core/callback/callback.py:324] print_output_info:   17.9% |                                          | 4.60238 samples/s/p  0:03:42 }
[INFO] 2024-06-14 10:59:23,117 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  114/  625], loss: 1.901, per_step_time: 424ms, lr: 1.863083e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:23,117 [mindformers/core/callback/callback.py:324] print_output_info:   18.2% |                                         | 4.70687 samples/s/p  0:03:37 }
[INFO] 2024-06-14 10:59:23,972 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  116/  625], loss: 2.175, per_step_time: 424ms, lr: 1.8582396e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:23,972 [mindformers/core/callback/callback.py:324] print_output_info:   18.6% |                                         | 4.71243 samples/s/p  0:03:36 }
[INFO] 2024-06-14 10:59:24,836 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  118/  625], loss: 2.218, per_step_time: 429ms, lr: 1.8533194e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:24,837 [mindformers/core/callback/callback.py:324] print_output_info:   18.9% |                                         | 4.65823 samples/s/p  0:03:37 }
[INFO] 2024-06-14 10:59:25,699 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  120/  625], loss: 2.120, per_step_time: 428ms, lr: 1.8483226e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:25,699 [mindformers/core/callback/callback.py:324] print_output_info:   19.2% |                                         | 4.66677 samples/s/p  0:03:36 }
[INFO] 2024-06-14 10:59:26,561 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  122/  625], loss: 2.050, per_step_time: 428ms, lr: 1.8432502e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:26,563 [mindformers/core/callback/callback.py:324] print_output_info:   19.5% |                                         | 4.66957 samples/s/p  0:03:35 }
[INFO] 2024-06-14 10:59:27,435 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  124/  625], loss: 2.139, per_step_time: 433ms, lr: 1.838102e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:27,435 [mindformers/core/callback/callback.py:324] print_output_info:   19.8% |                                         | 4.61601 samples/s/p  0:03:37 }
[INFO] 2024-06-14 10:59:28,303 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  126/  625], loss: 2.116, per_step_time: 431ms, lr: 1.8328792e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:28,303 [mindformers/core/callback/callback.py:324] print_output_info:   20.2% |                                        | 4.63983 samples/s/p  0:03:35 }
[INFO] 2024-06-14 10:59:29,158 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  128/  625], loss: 2.019, per_step_time: 423ms, lr: 1.8275821e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:29,159 [mindformers/core/callback/callback.py:324] print_output_info:   20.5% |                                        | 4.71848 samples/s/p  0:03:30 }
[INFO] 2024-06-14 10:59:30,018 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  130/  625], loss: 2.243, per_step_time: 427ms, lr: 1.8222114e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:30,019 [mindformers/core/callback/callback.py:324] print_output_info:   20.8% |                                        | 4.68337 samples/s/p  0:03:31 }
[INFO] 2024-06-14 10:59:30,890 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  132/  625], loss: 2.310, per_step_time: 433ms, lr: 1.8167673e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:30,891 [mindformers/core/callback/callback.py:324] print_output_info:   21.1% |                                        | 4.61646 samples/s/p  0:03:33 }
[INFO] 2024-06-14 10:59:31,779 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  134/  625], loss: 2.064, per_step_time: 441ms, lr: 1.8112505e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:31,780 [mindformers/core/callback/callback.py:324] print_output_info:   21.4% |                                        | 4.53406 samples/s/p  0:03:36 }
[INFO] 2024-06-14 10:59:32,640 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  136/  625], loss: 2.130, per_step_time: 426ms, lr: 1.8056617e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:32,640 [mindformers/core/callback/callback.py:324] print_output_info:   21.8% |                                        | 4.68517 samples/s/p  0:03:28 }
[INFO] 2024-06-14 10:59:33,497 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  138/  625], loss: 2.107, per_step_time: 425ms, lr: 1.8000013e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:33,497 [mindformers/core/callback/callback.py:324] print_output_info:   22.1% |                                       | 4.70056 samples/s/p  0:03:27 }
[INFO] 2024-06-14 10:59:34,350 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  140/  625], loss: 2.162, per_step_time: 423ms, lr: 1.7942699e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:34,350 [mindformers/core/callback/callback.py:324] print_output_info:   22.4% |                                       | 4.72220 samples/s/p  0:03:25 }
[INFO] 2024-06-14 10:59:35,204 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  142/  625], loss: 1.891, per_step_time: 424ms, lr: 1.788468e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:35,205 [mindformers/core/callback/callback.py:324] print_output_info:   22.7% |                                       | 4.71328 samples/s/p  0:03:24 }
[INFO] 2024-06-14 10:59:36,059 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  144/  625], loss: 2.112, per_step_time: 424ms, lr: 1.7825967e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:36,059 [mindformers/core/callback/callback.py:324] print_output_info:   23.0% |                                       | 4.71507 samples/s/p  0:03:24 }
[INFO] 2024-06-14 10:59:36,918 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  146/  625], loss: 2.156, per_step_time: 426ms, lr: 1.7766559e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:36,919 [mindformers/core/callback/callback.py:324] print_output_info:   23.4% |                                       | 4.68793 samples/s/p  0:03:24 }
[INFO] 2024-06-14 10:59:37,790 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  148/  625], loss: 2.008, per_step_time: 432ms, lr: 1.7706465e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:37,791 [mindformers/core/callback/callback.py:324] print_output_info:   23.7% |                                       | 4.62170 samples/s/p  0:03:26 }
[INFO] 2024-06-14 10:59:38,647 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  150/  625], loss: 2.122, per_step_time: 425ms, lr: 1.7645689e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:38,647 [mindformers/core/callback/callback.py:324] print_output_info:   24.0% |                                      | 4.70122 samples/s/p  0:03:22 }
[INFO] 2024-06-14 10:59:39,511 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  152/  625], loss: 2.088, per_step_time: 429ms, lr: 1.7584243e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:39,512 [mindformers/core/callback/callback.py:324] print_output_info:   24.3% |                                      | 4.65859 samples/s/p  0:03:23 }
[INFO] 2024-06-14 10:59:40,373 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  154/  625], loss: 2.291, per_step_time: 428ms, lr: 1.7522128e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:40,374 [mindformers/core/callback/callback.py:324] print_output_info:   24.6% |                                      | 4.67279 samples/s/p  0:03:21 }
[INFO] 2024-06-14 10:59:41,236 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  156/  625], loss: 2.127, per_step_time: 428ms, lr: 1.7459352e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:41,237 [mindformers/core/callback/callback.py:324] print_output_info:   25.0% |                                      | 4.66569 samples/s/p  0:03:21 }
[INFO] 2024-06-14 10:59:42,107 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  158/  625], loss: 1.901, per_step_time: 432ms, lr: 1.739592e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:42,109 [mindformers/core/callback/callback.py:324] print_output_info:   25.3% |                                      | 4.62670 samples/s/p  0:03:21 }
[INFO] 2024-06-14 10:59:42,968 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  160/  625], loss: 2.024, per_step_time: 427ms, lr: 1.733184e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:42,969 [mindformers/core/callback/callback.py:324] print_output_info:   25.6% |                                      | 4.68285 samples/s/p  0:03:18 }
[INFO] 2024-06-14 10:59:43,830 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  162/  625], loss: 2.163, per_step_time: 428ms, lr: 1.726712e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:43,831 [mindformers/core/callback/callback.py:324] print_output_info:   25.9% |                                      | 4.67243 samples/s/p  0:03:18 }
[INFO] 2024-06-14 10:59:44,689 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  164/  625], loss: 1.919, per_step_time: 426ms, lr: 1.720176e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:44,689 [mindformers/core/callback/callback.py:324] print_output_info:   26.2% |                                     | 4.68954 samples/s/p  0:03:16 }
[INFO] 2024-06-14 10:59:45,562 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  166/  625], loss: 2.167, per_step_time: 423ms, lr: 1.7135775e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:45,563 [mindformers/core/callback/callback.py:324] print_output_info:   26.6% |                                     | 4.72374 samples/s/p  0:03:14 }
[INFO] 2024-06-14 10:59:46,417 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  168/  625], loss: 1.992, per_step_time: 424ms, lr: 1.706917e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:46,418 [mindformers/core/callback/callback.py:324] print_output_info:   26.9% |                                     | 4.70966 samples/s/p  0:03:14 }
[INFO] 2024-06-14 10:59:47,277 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  170/  625], loss: 2.020, per_step_time: 426ms, lr: 1.7001945e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:47,278 [mindformers/core/callback/callback.py:324] print_output_info:   27.2% |                                     | 4.68502 samples/s/p  0:03:14 }
[INFO] 2024-06-14 10:59:48,141 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  172/  625], loss: 2.204, per_step_time: 428ms, lr: 1.6934111e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:48,141 [mindformers/core/callback/callback.py:324] print_output_info:   27.5% |                                     | 4.66577 samples/s/p  0:03:14 }
[INFO] 2024-06-14 10:59:49,017 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  174/  625], loss: 1.881, per_step_time: 435ms, lr: 1.6865677e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:49,017 [mindformers/core/callback/callback.py:324] print_output_info:   27.8% |                                     | 4.59641 samples/s/p  0:03:16 }
[INFO] 2024-06-14 10:59:49,871 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  176/  625], loss: 1.997, per_step_time: 424ms, lr: 1.679665e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:49,872 [mindformers/core/callback/callback.py:324] print_output_info:   28.2% |                                    | 4.71362 samples/s/p  0:03:10 }
[INFO] 2024-06-14 10:59:50,725 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  178/  625], loss: 1.904, per_step_time: 423ms, lr: 1.6727032e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:50,726 [mindformers/core/callback/callback.py:324] print_output_info:   28.5% |                                    | 4.72165 samples/s/p  0:03:09 }
[INFO] 2024-06-14 10:59:51,579 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  180/  625], loss: 2.004, per_step_time: 424ms, lr: 1.6656835e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:51,580 [mindformers/core/callback/callback.py:324] print_output_info:   28.8% |                                    | 4.71654 samples/s/p  0:03:08 }
[INFO] 2024-06-14 10:59:52,444 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  182/  625], loss: 2.180, per_step_time: 429ms, lr: 1.6586066e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:52,445 [mindformers/core/callback/callback.py:324] print_output_info:   29.1% |                                    | 4.65817 samples/s/p  0:03:10 }
[INFO] 2024-06-14 10:59:53,314 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  184/  625], loss: 2.050, per_step_time: 431ms, lr: 1.651473e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:53,314 [mindformers/core/callback/callback.py:324] print_output_info:   29.4% |                                    | 4.63728 samples/s/p  0:03:10 }
[INFO] 2024-06-14 10:59:54,177 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  186/  625], loss: 2.011, per_step_time: 428ms, lr: 1.6442835e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:54,178 [mindformers/core/callback/callback.py:324] print_output_info:   29.8% |                                    | 4.66311 samples/s/p  0:03:08 }
[INFO] 2024-06-14 10:59:55,050 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  188/  625], loss: 2.350, per_step_time: 433ms, lr: 1.6370386e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:55,051 [mindformers/core/callback/callback.py:324] print_output_info:   30.1% |                                   | 4.61617 samples/s/p  0:03:09 }
[INFO] 2024-06-14 10:59:55,916 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  190/  625], loss: 2.247, per_step_time: 429ms, lr: 1.6297396e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:55,916 [mindformers/core/callback/callback.py:324] print_output_info:   30.4% |                                   | 4.65184 samples/s/p  0:03:07 }
[INFO] 2024-06-14 10:59:56,774 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  192/  625], loss: 1.844, per_step_time: 426ms, lr: 1.6223867e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:56,775 [mindformers/core/callback/callback.py:324] print_output_info:   30.7% |                                   | 4.69312 samples/s/p  0:03:04 }
[INFO] 2024-06-14 10:59:57,635 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  194/  625], loss: 2.143, per_step_time: 427ms, lr: 1.6149806e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:57,637 [mindformers/core/callback/callback.py:324] print_output_info:   31.0% |                                   | 4.67770 samples/s/p  0:03:04 }
[INFO] 2024-06-14 10:59:58,494 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  196/  625], loss: 1.971, per_step_time: 425ms, lr: 1.6075228e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:58,494 [mindformers/core/callback/callback.py:324] print_output_info:   31.4% |                                   | 4.69744 samples/s/p  0:03:02 }
[INFO] 2024-06-14 10:59:59,347 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  198/  625], loss: 2.258, per_step_time: 423ms, lr: 1.6000133e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 10:59:59,348 [mindformers/core/callback/callback.py:324] print_output_info:   31.7% |                                   | 4.71920 samples/s/p  0:03:00 }
[INFO] 2024-06-14 11:00:00,205 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  200/  625], loss: 1.980, per_step_time: 425ms, lr: 1.5924528e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:00,205 [mindformers/core/callback/callback.py:324] print_output_info:   32.0% |                                  | 4.69705 samples/s/p  0:03:00 }
[INFO] 2024-06-14 11:00:01,066 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  202/  625], loss: 2.431, per_step_time: 427ms, lr: 1.5848425e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:01,067 [mindformers/core/callback/callback.py:324] print_output_info:   32.3% |                                  | 4.67491 samples/s/p  0:03:00 }
[INFO] 2024-06-14 11:00:01,922 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  204/  625], loss: 1.937, per_step_time: 424ms, lr: 1.5771835e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:01,922 [mindformers/core/callback/callback.py:324] print_output_info:   32.6% |                                  | 4.70743 samples/s/p  0:02:58 }
[INFO] 2024-06-14 11:00:02,783 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  206/  625], loss: 2.143, per_step_time: 427ms, lr: 1.5694757e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:02,784 [mindformers/core/callback/callback.py:324] print_output_info:   33.0% |                                  | 4.67948 samples/s/p  0:02:59 }
[INFO] 2024-06-14 11:00:03,641 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  208/  625], loss: 1.881, per_step_time: 426ms, lr: 1.5617205e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:03,642 [mindformers/core/callback/callback.py:324] print_output_info:   33.3% |                                  | 4.69348 samples/s/p  0:02:57 }
[INFO] 2024-06-14 11:00:04,509 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  210/  625], loss: 1.910, per_step_time: 430ms, lr: 1.5539183e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:04,509 [mindformers/core/callback/callback.py:324] print_output_info:   33.6% |                                  | 4.64263 samples/s/p  0:02:58 }
[INFO] 2024-06-14 11:00:05,390 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  212/  625], loss: 2.020, per_step_time: 437ms, lr: 1.54607e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:05,390 [mindformers/core/callback/callback.py:324] print_output_info:   33.9% |                                  | 4.57124 samples/s/p  0:03:00 }
[INFO] 2024-06-14 11:00:06,250 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  214/  625], loss: 2.081, per_step_time: 426ms, lr: 1.5381767e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:06,250 [mindformers/core/callback/callback.py:324] print_output_info:   34.2% |                                 | 4.68485 samples/s/p  0:02:55 }
[INFO] 2024-06-14 11:00:07,114 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  216/  625], loss: 1.988, per_step_time: 427ms, lr: 1.530239e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:07,114 [mindformers/core/callback/callback.py:324] print_output_info:   34.6% |                                 | 4.67859 samples/s/p  0:02:54 }
[INFO] 2024-06-14 11:00:07,971 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  218/  625], loss: 2.206, per_step_time: 425ms, lr: 1.5222575e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:07,972 [mindformers/core/callback/callback.py:324] print_output_info:   34.9% |                                 | 4.70165 samples/s/p  0:02:53 }
[INFO] 2024-06-14 11:00:08,830 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  220/  625], loss: 2.279, per_step_time: 426ms, lr: 1.5142333e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:08,830 [mindformers/core/callback/callback.py:324] print_output_info:   35.2% |                                 | 4.69275 samples/s/p  0:02:52 }
[INFO] 2024-06-14 11:00:09,696 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  222/  625], loss: 2.162, per_step_time: 430ms, lr: 1.5061672e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:09,696 [mindformers/core/callback/callback.py:324] print_output_info:   35.5% |                                 | 4.65104 samples/s/p  0:02:53 }
[INFO] 2024-06-14 11:00:10,564 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  224/  625], loss: 2.106, per_step_time: 431ms, lr: 1.4980596e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:10,564 [mindformers/core/callback/callback.py:324] print_output_info:   35.8% |                                 | 4.63910 samples/s/p  0:02:52 }
[INFO] 2024-06-14 11:00:11,433 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  226/  625], loss: 2.340, per_step_time: 431ms, lr: 1.48991185e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:11,434 [mindformers/core/callback/callback.py:324] print_output_info:   36.2% |                                | 4.63023 samples/s/p  0:02:52 }
[INFO] 2024-06-14 11:00:12,301 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  228/  625], loss: 2.255, per_step_time: 430ms, lr: 1.4817247e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:12,301 [mindformers/core/callback/callback.py:324] print_output_info:   36.5% |                                | 4.64286 samples/s/p  0:02:51 }
[INFO] 2024-06-14 11:00:13,156 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  230/  625], loss: 2.061, per_step_time: 424ms, lr: 1.47349865e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:13,160 [mindformers/core/callback/callback.py:324] print_output_info:   36.8% |                                | 4.70861 samples/s/p  0:02:47 }
[INFO] 2024-06-14 11:00:14,018 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  232/  625], loss: 2.376, per_step_time: 426ms, lr: 1.4652348e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:14,019 [mindformers/core/callback/callback.py:324] print_output_info:   37.1% |                                | 4.68959 samples/s/p  0:02:47 }
[INFO] 2024-06-14 11:00:14,878 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  234/  625], loss: 1.900, per_step_time: 426ms, lr: 1.45693375e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:14,878 [mindformers/core/callback/callback.py:324] print_output_info:   37.4% |                                | 4.68504 samples/s/p  0:02:46 }
[INFO] 2024-06-14 11:00:15,733 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  236/  625], loss: 2.124, per_step_time: 424ms, lr: 1.4485967e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:15,733 [mindformers/core/callback/callback.py:324] print_output_info:   37.8% |                                | 4.71170 samples/s/p  0:02:45 }
[INFO] 2024-06-14 11:00:16,591 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  238/  625], loss: 2.067, per_step_time: 426ms, lr: 1.4402243e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:16,592 [mindformers/core/callback/callback.py:324] print_output_info:   38.1% |                               | 4.68883 samples/s/p  0:02:45 }
[INFO] 2024-06-14 11:00:17,451 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  240/  625], loss: 2.113, per_step_time: 427ms, lr: 1.4318173e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:17,452 [mindformers/core/callback/callback.py:324] print_output_info:   38.4% |                               | 4.68304 samples/s/p  0:02:44 }
[INFO] 2024-06-14 11:00:18,313 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  242/  625], loss: 2.142, per_step_time: 428ms, lr: 1.42337685e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:18,315 [mindformers/core/callback/callback.py:324] print_output_info:   38.7% |                               | 4.67243 samples/s/p  0:02:43 }
[INFO] 2024-06-14 11:00:19,176 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  244/  625], loss: 2.139, per_step_time: 427ms, lr: 1.41490345e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:19,176 [mindformers/core/callback/callback.py:324] print_output_info:   39.0% |                               | 4.67651 samples/s/p  0:02:42 }
[INFO] 2024-06-14 11:00:20,048 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  246/  625], loss: 2.143, per_step_time: 433ms, lr: 1.4063983e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:20,048 [mindformers/core/callback/callback.py:324] print_output_info:   39.4% |                               | 4.61742 samples/s/p  0:02:44 }
[INFO] 2024-06-14 11:00:20,909 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  248/  625], loss: 2.140, per_step_time: 426ms, lr: 1.3978619e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:20,909 [mindformers/core/callback/callback.py:324] print_output_info:   39.7% |                               | 4.68502 samples/s/p  0:02:40 }
[INFO] 2024-06-14 11:00:21,767 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  250/  625], loss: 2.005, per_step_time: 426ms, lr: 1.3892954e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:21,768 [mindformers/core/callback/callback.py:324] print_output_info:   40.0% |                              | 4.69222 samples/s/p  0:02:39 }
[INFO] 2024-06-14 11:00:22,624 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  252/  625], loss: 2.219, per_step_time: 424ms, lr: 1.3806995e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:22,624 [mindformers/core/callback/callback.py:324] print_output_info:   40.3% |                              | 4.70595 samples/s/p  0:02:38 }
[INFO] 2024-06-14 11:00:23,483 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  254/  625], loss: 2.088, per_step_time: 426ms, lr: 1.3720752e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:23,485 [mindformers/core/callback/callback.py:324] print_output_info:   40.6% |                              | 4.68462 samples/s/p  0:02:38 }
[INFO] 2024-06-14 11:00:24,354 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  256/  625], loss: 2.465, per_step_time: 431ms, lr: 1.3634231e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:24,354 [mindformers/core/callback/callback.py:324] print_output_info:   41.0% |                              | 4.63348 samples/s/p  0:02:39 }
[INFO] 2024-06-14 11:00:25,217 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  258/  625], loss: 2.293, per_step_time: 428ms, lr: 1.35474465e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:25,217 [mindformers/core/callback/callback.py:324] print_output_info:   41.3% |                              | 4.66445 samples/s/p  0:02:37 }
[INFO] 2024-06-14 11:00:26,071 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  260/  625], loss: 1.840, per_step_time: 424ms, lr: 1.34604015e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:26,071 [mindformers/core/callback/callback.py:324] print_output_info:   41.6% |                              | 4.71630 samples/s/p  0:02:34 }
[INFO] 2024-06-14 11:00:26,934 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  262/  625], loss: 2.065, per_step_time: 428ms, lr: 1.3373108e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:26,934 [mindformers/core/callback/callback.py:324] print_output_info:   41.9% |                              | 4.66638 samples/s/p  0:02:35 }
[INFO] 2024-06-14 11:00:27,802 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  264/  625], loss: 2.216, per_step_time: 430ms, lr: 1.3285573e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:27,802 [mindformers/core/callback/callback.py:324] print_output_info:   42.2% |                             | 4.64068 samples/s/p  0:02:35 }
[INFO] 2024-06-14 11:00:28,662 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  266/  625], loss: 2.323, per_step_time: 427ms, lr: 1.3197807e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:28,664 [mindformers/core/callback/callback.py:324] print_output_info:   42.6% |                             | 4.68116 samples/s/p  0:02:33 }
[INFO] 2024-06-14 11:00:29,519 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  268/  625], loss: 1.972, per_step_time: 424ms, lr: 1.3109819e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:29,519 [mindformers/core/callback/callback.py:324] print_output_info:   42.9% |                             | 4.70880 samples/s/p  0:02:31 }
[INFO] 2024-06-14 11:00:30,394 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  270/  625], loss: 2.274, per_step_time: 434ms, lr: 1.3021614e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:30,394 [mindformers/core/callback/callback.py:324] print_output_info:   43.2% |                             | 4.60283 samples/s/p  0:02:34 }
[INFO] 2024-06-14 11:00:31,255 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  272/  625], loss: 2.062, per_step_time: 428ms, lr: 1.2933207e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:31,256 [mindformers/core/callback/callback.py:324] print_output_info:   43.5% |                             | 4.67254 samples/s/p  0:02:31 }
[INFO] 2024-06-14 11:00:32,113 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  274/  625], loss: 1.851, per_step_time: 425ms, lr: 1.2844603e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:32,114 [mindformers/core/callback/callback.py:324] print_output_info:   43.8% |                             | 4.69540 samples/s/p  0:02:29 }
[INFO] 2024-06-14 11:00:32,985 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  276/  625], loss: 2.156, per_step_time: 433ms, lr: 1.2755811e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:32,985 [mindformers/core/callback/callback.py:324] print_output_info:   44.2% |                            | 4.61880 samples/s/p  0:02:31 }
[INFO] 2024-06-14 11:00:33,842 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  278/  625], loss: 1.962, per_step_time: 425ms, lr: 1.2666843e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:33,844 [mindformers/core/callback/callback.py:324] print_output_info:   44.5% |                            | 4.69956 samples/s/p  0:02:27 }
[INFO] 2024-06-14 11:00:34,712 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  280/  625], loss: 2.117, per_step_time: 431ms, lr: 1.2577704e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:34,712 [mindformers/core/callback/callback.py:324] print_output_info:   44.8% |                            | 4.63522 samples/s/p  0:02:28 }
[INFO] 2024-06-14 11:00:35,571 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  282/  625], loss: 2.008, per_step_time: 426ms, lr: 1.2488406e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:35,571 [mindformers/core/callback/callback.py:324] print_output_info:   45.1% |                            | 4.69031 samples/s/p  0:02:26 }
[INFO] 2024-06-14 11:00:36,426 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  284/  625], loss: 1.972, per_step_time: 424ms, lr: 1.2398958e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:36,426 [mindformers/core/callback/callback.py:324] print_output_info:   45.4% |                            | 4.71257 samples/s/p  0:02:24 }
[INFO] 2024-06-14 11:00:37,287 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  286/  625], loss: 2.218, per_step_time: 427ms, lr: 1.2309367e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:37,287 [mindformers/core/callback/callback.py:324] print_output_info:   45.8% |                            | 4.67776 samples/s/p  0:02:24 }
[INFO] 2024-06-14 11:00:38,148 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  288/  625], loss: 2.230, per_step_time: 427ms, lr: 1.22196425e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:38,149 [mindformers/core/callback/callback.py:324] print_output_info:   46.1% |                           | 4.67460 samples/s/p  0:02:24 }
[INFO] 2024-06-14 11:00:39,024 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  290/  625], loss: 2.092, per_step_time: 435ms, lr: 1.2129796e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:39,026 [mindformers/core/callback/callback.py:324] print_output_info:   46.4% |                           | 4.59747 samples/s/p  0:02:25 }
[INFO] 2024-06-14 11:00:39,901 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  292/  625], loss: 2.215, per_step_time: 434ms, lr: 1.2039834e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:39,901 [mindformers/core/callback/callback.py:324] print_output_info:   46.7% |                           | 4.60227 samples/s/p  0:02:24 }
[INFO] 2024-06-14 11:00:40,763 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  294/  625], loss: 1.821, per_step_time: 428ms, lr: 1.1949766e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:40,763 [mindformers/core/callback/callback.py:324] print_output_info:   47.0% |                           | 4.67234 samples/s/p  0:02:21 }
[INFO] 2024-06-14 11:00:41,625 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  296/  625], loss: 2.185, per_step_time: 428ms, lr: 1.1859604e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:41,626 [mindformers/core/callback/callback.py:324] print_output_info:   47.4% |                           | 4.66898 samples/s/p  0:02:20 }
[INFO] 2024-06-14 11:00:42,479 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  298/  625], loss: 2.025, per_step_time: 423ms, lr: 1.1769353e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:42,479 [mindformers/core/callback/callback.py:324] print_output_info:   47.7% |                           | 4.71732 samples/s/p  0:02:18 }
[INFO] 2024-06-14 11:00:43,338 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  300/  625], loss: 1.830, per_step_time: 426ms, lr: 1.1679025e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:43,338 [mindformers/core/callback/callback.py:324] print_output_info:   48.0% |                          | 4.69088 samples/s/p  0:02:18 }
[INFO] 2024-06-14 11:00:44,202 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  302/  625], loss: 2.278, per_step_time: 429ms, lr: 1.1588627e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:44,204 [mindformers/core/callback/callback.py:324] print_output_info:   48.3% |                          | 4.66083 samples/s/p  0:02:18 }
[INFO] 2024-06-14 11:00:45,061 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  304/  625], loss: 2.352, per_step_time: 425ms, lr: 1.1498169e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:45,061 [mindformers/core/callback/callback.py:324] print_output_info:   48.6% |                          | 4.69584 samples/s/p  0:02:16 }
[INFO] 2024-06-14 11:00:45,917 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  306/  625], loss: 2.249, per_step_time: 425ms, lr: 1.1407663e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:45,917 [mindformers/core/callback/callback.py:324] print_output_info:   49.0% |                          | 4.70482 samples/s/p  0:02:15 }
[INFO] 2024-06-14 11:00:46,785 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  308/  625], loss: 2.048, per_step_time: 430ms, lr: 1.1317113e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:46,785 [mindformers/core/callback/callback.py:324] print_output_info:   49.3% |                          | 4.64186 samples/s/p  0:02:16 }
[INFO] 2024-06-14 11:00:47,655 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  310/  625], loss: 2.109, per_step_time: 432ms, lr: 1.1226531e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:47,656 [mindformers/core/callback/callback.py:324] print_output_info:   49.6% |                          | 4.62520 samples/s/p  0:02:16 }
[INFO] 2024-06-14 11:00:48,509 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  312/  625], loss: 2.002, per_step_time: 424ms, lr: 1.1135928e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:48,510 [mindformers/core/callback/callback.py:324] print_output_info:   49.9% |                          | 4.71512 samples/s/p  0:02:12 }
[INFO] 2024-06-14 11:00:49,363 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  314/  625], loss: 1.936, per_step_time: 423ms, lr: 1.1045309e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:49,365 [mindformers/core/callback/callback.py:324] print_output_info:   50.2% |                         | 4.71812 samples/s/p  0:02:11 }
[INFO] 2024-06-14 11:00:50,219 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  316/  625], loss: 2.151, per_step_time: 424ms, lr: 1.0954687e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:50,220 [mindformers/core/callback/callback.py:324] print_output_info:   50.6% |                         | 4.71077 samples/s/p  0:02:11 }
[INFO] 2024-06-14 11:00:51,075 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  318/  625], loss: 2.164, per_step_time: 424ms, lr: 1.086407e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:51,075 [mindformers/core/callback/callback.py:324] print_output_info:   50.9% |                         | 4.70862 samples/s/p  0:02:10 }
[INFO] 2024-06-14 11:00:51,932 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  320/  625], loss: 2.272, per_step_time: 425ms, lr: 1.0773465e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:51,932 [mindformers/core/callback/callback.py:324] print_output_info:   51.2% |                         | 4.69898 samples/s/p  0:02:09 }
[INFO] 2024-06-14 11:00:52,797 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  322/  625], loss: 2.102, per_step_time: 429ms, lr: 1.0682885e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:52,798 [mindformers/core/callback/callback.py:324] print_output_info:   51.5% |                         | 4.65117 samples/s/p  0:02:10 }
[INFO] 2024-06-14 11:00:53,655 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  324/  625], loss: 2.083, per_step_time: 425ms, lr: 1.05923355e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:53,655 [mindformers/core/callback/callback.py:324] print_output_info:   51.8% |                         | 4.69972 samples/s/p  0:02:08 }
[INFO] 2024-06-14 11:00:54,532 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  326/  625], loss: 2.145, per_step_time: 435ms, lr: 1.0501828e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:54,533 [mindformers/core/callback/callback.py:324] print_output_info:   52.2% |                        | 4.59259 samples/s/p  0:02:10 }
[INFO] 2024-06-14 11:00:55,391 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  328/  625], loss: 1.978, per_step_time: 426ms, lr: 1.0411371e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:55,392 [mindformers/core/callback/callback.py:324] print_output_info:   52.5% |                        | 4.69283 samples/s/p  0:02:06 }
[INFO] 2024-06-14 11:00:56,247 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  330/  625], loss: 2.179, per_step_time: 424ms, lr: 1.0320971e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:56,247 [mindformers/core/callback/callback.py:324] print_output_info:   52.8% |                        | 4.70607 samples/s/p  0:02:05 }
[INFO] 2024-06-14 11:00:57,122 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  332/  625], loss: 1.797, per_step_time: 434ms, lr: 1.0230644e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:57,123 [mindformers/core/callback/callback.py:324] print_output_info:   53.1% |                        | 4.59969 samples/s/p  0:02:07 }
[INFO] 2024-06-14 11:00:57,984 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  334/  625], loss: 2.193, per_step_time: 427ms, lr: 1.0140394e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:57,985 [mindformers/core/callback/callback.py:324] print_output_info:   53.4% |                        | 4.67331 samples/s/p  0:02:04 }
[INFO] 2024-06-14 11:00:58,848 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  336/  625], loss: 2.020, per_step_time: 428ms, lr: 1.005023e-05, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:58,848 [mindformers/core/callback/callback.py:324] print_output_info:   53.8% |                        | 4.66571 samples/s/p  0:02:03 }
[INFO] 2024-06-14 11:00:59,708 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  338/  625], loss: 2.047, per_step_time: 427ms, lr: 9.960164e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:00:59,710 [mindformers/core/callback/callback.py:324] print_output_info:   54.1% |                       | 4.67970 samples/s/p  0:02:02 }
[INFO] 2024-06-14 11:01:00,571 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  340/  625], loss: 2.114, per_step_time: 427ms, lr: 9.870201e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:00,572 [mindformers/core/callback/callback.py:324] print_output_info:   54.4% |                       | 4.67344 samples/s/p  0:02:01 }
[INFO] 2024-06-14 11:01:01,426 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  342/  625], loss: 2.346, per_step_time: 424ms, lr: 9.780355e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:01,427 [mindformers/core/callback/callback.py:324] print_output_info:   54.7% |                       | 4.70955 samples/s/p  0:02:00 }
[INFO] 2024-06-14 11:01:02,282 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  344/  625], loss: 2.125, per_step_time: 424ms, lr: 9.6906315e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:02,282 [mindformers/core/callback/callback.py:324] print_output_info:   55.0% |                       | 4.70982 samples/s/p  0:01:59 }
[INFO] 2024-06-14 11:01:03,137 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  346/  625], loss: 2.325, per_step_time: 424ms, lr: 9.601039e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:03,137 [mindformers/core/callback/callback.py:324] print_output_info:   55.4% |                       | 4.71008 samples/s/p  0:01:58 }
[INFO] 2024-06-14 11:01:04,000 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  348/  625], loss: 2.174, per_step_time: 428ms, lr: 9.511591e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:04,001 [mindformers/core/callback/callback.py:324] print_output_info:   55.7% |                       | 4.66497 samples/s/p  0:01:58 }
[INFO] 2024-06-14 11:01:04,855 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  350/  625], loss: 1.954, per_step_time: 424ms, lr: 9.422292e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:04,858 [mindformers/core/callback/callback.py:324] print_output_info:   56.0% |                      | 4.70829 samples/s/p  0:01:56 }
[INFO] 2024-06-14 11:01:05,721 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  352/  625], loss: 2.144, per_step_time: 429ms, lr: 9.333155e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:05,722 [mindformers/core/callback/callback.py:324] print_output_info:   56.3% |                      | 4.66089 samples/s/p  0:01:57 }
[INFO] 2024-06-14 11:01:06,586 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  354/  625], loss: 2.124, per_step_time: 429ms, lr: 9.244187e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:06,586 [mindformers/core/callback/callback.py:324] print_output_info:   56.6% |                      | 4.65645 samples/s/p  0:01:56 }
[INFO] 2024-06-14 11:01:07,450 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  356/  625], loss: 2.263, per_step_time: 429ms, lr: 9.155395e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:07,450 [mindformers/core/callback/callback.py:324] print_output_info:   57.0% |                      | 4.66084 samples/s/p  0:01:55 }
[INFO] 2024-06-14 11:01:08,310 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  358/  625], loss: 2.167, per_step_time: 426ms, lr: 9.066792e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:08,310 [mindformers/core/callback/callback.py:324] print_output_info:   57.3% |                      | 4.68546 samples/s/p  0:01:53 }
[INFO] 2024-06-14 11:01:09,167 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  360/  625], loss: 1.948, per_step_time: 426ms, lr: 8.9783825e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:09,168 [mindformers/core/callback/callback.py:324] print_output_info:   57.6% |                      | 4.69401 samples/s/p  0:01:52 }
[INFO] 2024-06-14 11:01:10,024 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  362/  625], loss: 2.022, per_step_time: 425ms, lr: 8.890179e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:10,026 [mindformers/core/callback/callback.py:324] print_output_info:   57.9% |                      | 4.69970 samples/s/p  0:01:51 }
[INFO] 2024-06-14 11:01:10,879 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  364/  625], loss: 2.255, per_step_time: 424ms, lr: 8.802191e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:10,880 [mindformers/core/callback/callback.py:324] print_output_info:   58.2% |                     | 4.71504 samples/s/p  0:01:50 }
[INFO] 2024-06-14 11:01:11,735 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  366/  625], loss: 2.117, per_step_time: 424ms, lr: 8.7144235e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:11,735 [mindformers/core/callback/callback.py:324] print_output_info:   58.6% |                     | 4.70802 samples/s/p  0:01:50 }
[INFO] 2024-06-14 11:01:12,590 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  368/  625], loss: 2.035, per_step_time: 424ms, lr: 8.626889e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:12,590 [mindformers/core/callback/callback.py:324] print_output_info:   58.9% |                     | 4.71116 samples/s/p  0:01:49 }
[INFO] 2024-06-14 11:01:13,448 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  370/  625], loss: 2.050, per_step_time: 426ms, lr: 8.539596e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:13,448 [mindformers/core/callback/callback.py:324] print_output_info:   59.2% |                     | 4.69224 samples/s/p  0:01:48 }
[INFO] 2024-06-14 11:01:14,304 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  372/  625], loss: 2.337, per_step_time: 424ms, lr: 8.452551e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:14,304 [mindformers/core/callback/callback.py:324] print_output_info:   59.5% |                     | 4.70588 samples/s/p  0:01:47 }
[INFO] 2024-06-14 11:01:15,158 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  374/  625], loss: 1.910, per_step_time: 423ms, lr: 8.365766e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:15,160 [mindformers/core/callback/callback.py:324] print_output_info:   59.8% |                     | 4.72045 samples/s/p  0:01:46 }
[INFO] 2024-06-14 11:01:16,026 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  376/  625], loss: 1.971, per_step_time: 429ms, lr: 8.279245e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:16,026 [mindformers/core/callback/callback.py:324] print_output_info:   60.2% |                    | 4.65225 samples/s/p  0:01:47 }
[INFO] 2024-06-14 11:01:16,890 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  378/  625], loss: 1.913, per_step_time: 429ms, lr: 8.193001e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:16,891 [mindformers/core/callback/callback.py:324] print_output_info:   60.5% |                    | 4.65584 samples/s/p  0:01:46 }
[INFO] 2024-06-14 11:01:17,760 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  380/  625], loss: 2.143, per_step_time: 431ms, lr: 8.107044e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:17,760 [mindformers/core/callback/callback.py:324] print_output_info:   60.8% |                    | 4.63372 samples/s/p  0:01:45 }
[INFO] 2024-06-14 11:01:18,613 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  382/  625], loss: 2.119, per_step_time: 423ms, lr: 8.021378e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:18,613 [mindformers/core/callback/callback.py:324] print_output_info:   61.1% |                    | 4.72343 samples/s/p  0:01:42 }
[INFO] 2024-06-14 11:01:19,479 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  384/  625], loss: 2.139, per_step_time: 430ms, lr: 7.936014e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:19,479 [mindformers/core/callback/callback.py:324] print_output_info:   61.4% |                    | 4.64959 samples/s/p  0:01:43 }
[INFO] 2024-06-14 11:01:20,340 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  386/  625], loss: 2.115, per_step_time: 427ms, lr: 7.850963e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:20,342 [mindformers/core/callback/callback.py:324] print_output_info:   61.8% |                    | 4.67436 samples/s/p  0:01:42 }
[INFO] 2024-06-14 11:01:21,201 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  388/  625], loss: 2.097, per_step_time: 426ms, lr: 7.766229e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:21,201 [mindformers/core/callback/callback.py:324] print_output_info:   62.1% |                   | 4.68597 samples/s/p  0:01:41 }
[INFO] 2024-06-14 11:01:22,064 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  390/  625], loss: 2.313, per_step_time: 428ms, lr: 7.681824e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:22,065 [mindformers/core/callback/callback.py:324] print_output_info:   62.4% |                   | 4.66390 samples/s/p  0:01:40 }
[INFO] 2024-06-14 11:01:22,921 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  392/  625], loss: 2.132, per_step_time: 425ms, lr: 7.597754e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:22,921 [mindformers/core/callback/callback.py:324] print_output_info:   62.7% |                   | 4.70200 samples/s/p  0:01:39 }
[INFO] 2024-06-14 11:01:23,777 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  394/  625], loss: 1.814, per_step_time: 425ms, lr: 7.5140288e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:23,777 [mindformers/core/callback/callback.py:324] print_output_info:   63.0% |                   | 4.70570 samples/s/p  0:01:38 }
[INFO] 2024-06-14 11:01:24,642 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  396/  625], loss: 1.732, per_step_time: 429ms, lr: 7.430659e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:24,642 [mindformers/core/callback/callback.py:324] print_output_info:   63.4% |                   | 4.65453 samples/s/p  0:01:38 }
[INFO] 2024-06-14 11:01:25,501 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  398/  625], loss: 1.979, per_step_time: 426ms, lr: 7.3476494e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:25,502 [mindformers/core/callback/callback.py:324] print_output_info:   63.7% |                   | 4.69294 samples/s/p  0:01:36 }
[INFO] 2024-06-14 11:01:26,364 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  400/  625], loss: 1.940, per_step_time: 428ms, lr: 7.265011e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:26,364 [mindformers/core/callback/callback.py:324] print_output_info:   64.0% |                  | 4.67104 samples/s/p  0:01:36 }
[INFO] 2024-06-14 11:01:27,222 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  402/  625], loss: 2.225, per_step_time: 425ms, lr: 7.182751e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:27,222 [mindformers/core/callback/callback.py:324] print_output_info:   64.3% |                  | 4.69521 samples/s/p  0:01:34 }
[INFO] 2024-06-14 11:01:28,079 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  404/  625], loss: 2.085, per_step_time: 425ms, lr: 7.100879e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:28,080 [mindformers/core/callback/callback.py:324] print_output_info:   64.6% |                  | 4.69575 samples/s/p  0:01:34 }
[INFO] 2024-06-14 11:01:28,933 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  406/  625], loss: 2.102, per_step_time: 423ms, lr: 7.019402e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:28,933 [mindformers/core/callback/callback.py:324] print_output_info:   65.0% |                  | 4.71892 samples/s/p  0:01:32 }
[INFO] 2024-06-14 11:01:29,787 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  408/  625], loss: 2.258, per_step_time: 424ms, lr: 6.938326e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:29,788 [mindformers/core/callback/callback.py:324] print_output_info:   65.3% |                  | 4.71360 samples/s/p  0:01:32 }
[INFO] 2024-06-14 11:01:30,643 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  410/  625], loss: 2.038, per_step_time: 425ms, lr: 6.8576646e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:30,645 [mindformers/core/callback/callback.py:324] print_output_info:   65.6% |                  | 4.70558 samples/s/p  0:01:31 }
[INFO] 2024-06-14 11:01:31,509 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  412/  625], loss: 2.099, per_step_time: 429ms, lr: 6.7774217e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:31,510 [mindformers/core/callback/callback.py:324] print_output_info:   65.9% |                  | 4.65770 samples/s/p  0:01:31 }
[INFO] 2024-06-14 11:01:32,373 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  414/  625], loss: 2.242, per_step_time: 428ms, lr: 6.697608e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:32,373 [mindformers/core/callback/callback.py:324] print_output_info:   66.2% |                 | 4.66246 samples/s/p  0:01:30 }
[INFO] 2024-06-14 11:01:33,227 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  416/  625], loss: 2.053, per_step_time: 424ms, lr: 6.618231e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:33,228 [mindformers/core/callback/callback.py:324] print_output_info:   66.6% |                 | 4.71274 samples/s/p  0:01:28 }
[INFO] 2024-06-14 11:01:34,091 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  418/  625], loss: 1.949, per_step_time: 429ms, lr: 6.5392987e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:34,092 [mindformers/core/callback/callback.py:324] print_output_info:   66.9% |                 | 4.66059 samples/s/p  0:01:28 }
[INFO] 2024-06-14 11:01:34,949 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  420/  625], loss: 2.064, per_step_time: 425ms, lr: 6.4608166e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:34,949 [mindformers/core/callback/callback.py:324] print_output_info:   67.2% |                 | 4.69806 samples/s/p  0:01:27 }
[INFO] 2024-06-14 11:01:35,803 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  422/  625], loss: 2.026, per_step_time: 424ms, lr: 6.3827947e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:35,804 [mindformers/core/callback/callback.py:324] print_output_info:   67.5% |                 | 4.71677 samples/s/p  0:01:26 }
[INFO] 2024-06-14 11:01:36,661 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  424/  625], loss: 2.279, per_step_time: 425ms, lr: 6.3052403e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:36,661 [mindformers/core/callback/callback.py:324] print_output_info:   67.8% |                 | 4.69585 samples/s/p  0:01:25 }
[INFO] 2024-06-14 11:01:37,517 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  426/  625], loss: 1.909, per_step_time: 424ms, lr: 6.2281633e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:37,517 [mindformers/core/callback/callback.py:324] print_output_info:   68.2% |                | 4.70704 samples/s/p  0:01:24 }
[INFO] 2024-06-14 11:01:38,371 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  428/  625], loss: 2.169, per_step_time: 424ms, lr: 6.151571e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:38,372 [mindformers/core/callback/callback.py:324] print_output_info:   68.5% |                | 4.71585 samples/s/p  0:01:23 }
[INFO] 2024-06-14 11:01:39,233 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  430/  625], loss: 2.250, per_step_time: 427ms, lr: 6.07547e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:39,234 [mindformers/core/callback/callback.py:324] print_output_info:   68.8% |                | 4.67810 samples/s/p  0:01:23 }
[INFO] 2024-06-14 11:01:40,091 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  432/  625], loss: 2.024, per_step_time: 425ms, lr: 5.9998665e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:40,091 [mindformers/core/callback/callback.py:324] print_output_info:   69.1% |                | 4.69627 samples/s/p  0:01:22 }
[INFO] 2024-06-14 11:01:40,962 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  434/  625], loss: 2.123, per_step_time: 432ms, lr: 5.9247714e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:40,963 [mindformers/core/callback/callback.py:324] print_output_info:   69.4% |                | 4.62118 samples/s/p  0:01:22 }
[INFO] 2024-06-14 11:01:41,826 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  436/  625], loss: 2.084, per_step_time: 428ms, lr: 5.850192e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:41,826 [mindformers/core/callback/callback.py:324] print_output_info:   69.8% |                | 4.66598 samples/s/p  0:01:21 }
[INFO] 2024-06-14 11:01:42,715 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  438/  625], loss: 2.195, per_step_time: 441ms, lr: 5.7761317e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:42,716 [mindformers/core/callback/callback.py:324] print_output_info:   70.1% |               | 4.53066 samples/s/p  0:01:22 }
[INFO] 2024-06-14 11:01:43,572 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  440/  625], loss: 2.158, per_step_time: 425ms, lr: 5.702603e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:43,573 [mindformers/core/callback/callback.py:324] print_output_info:   70.4% |               | 4.69958 samples/s/p  0:01:18 }
[INFO] 2024-06-14 11:01:44,438 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  442/  625], loss: 2.078, per_step_time: 430ms, lr: 5.629612e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:44,439 [mindformers/core/callback/callback.py:324] print_output_info:   70.7% |               | 4.65094 samples/s/p  0:01:18 }
[INFO] 2024-06-14 11:01:45,296 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  444/  625], loss: 2.064, per_step_time: 426ms, lr: 5.557163e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:45,297 [mindformers/core/callback/callback.py:324] print_output_info:   71.0% |               | 4.69370 samples/s/p  0:01:17 }
[INFO] 2024-06-14 11:01:46,166 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  446/  625], loss: 1.897, per_step_time: 432ms, lr: 5.485268e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:46,169 [mindformers/core/callback/callback.py:324] print_output_info:   71.4% |               | 4.62844 samples/s/p  0:01:17 }
[INFO] 2024-06-14 11:01:47,023 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  448/  625], loss: 1.968, per_step_time: 424ms, lr: 5.413932e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:47,024 [mindformers/core/callback/callback.py:324] print_output_info:   71.7% |               | 4.70986 samples/s/p  0:01:15 }
[INFO] 2024-06-14 11:01:47,880 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  450/  625], loss: 2.065, per_step_time: 425ms, lr: 5.3431622e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:47,881 [mindformers/core/callback/callback.py:324] print_output_info:   72.0% |              | 4.70122 samples/s/p  0:01:14 }
[INFO] 2024-06-14 11:01:48,735 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  452/  625], loss: 1.902, per_step_time: 424ms, lr: 5.2729647e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:48,735 [mindformers/core/callback/callback.py:324] print_output_info:   72.3% |              | 4.71113 samples/s/p  0:01:13 }
[INFO] 2024-06-14 11:01:49,606 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  454/  625], loss: 2.143, per_step_time: 432ms, lr: 5.2033474e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:49,607 [mindformers/core/callback/callback.py:324] print_output_info:   72.6% |              | 4.62334 samples/s/p  0:01:13 }
[INFO] 2024-06-14 11:01:50,469 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  456/  625], loss: 1.906, per_step_time: 428ms, lr: 5.1343195e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:50,469 [mindformers/core/callback/callback.py:324] print_output_info:   73.0% |              | 4.66833 samples/s/p  0:01:12 }
[INFO] 2024-06-14 11:01:51,328 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  458/  625], loss: 2.213, per_step_time: 426ms, lr: 5.0658864e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:51,329 [mindformers/core/callback/callback.py:324] print_output_info:   73.3% |              | 4.68956 samples/s/p  0:01:11 }
[INFO] 2024-06-14 11:01:52,191 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  460/  625], loss: 2.130, per_step_time: 428ms, lr: 4.9980554e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:52,192 [mindformers/core/callback/callback.py:324] print_output_info:   73.6% |              | 4.66936 samples/s/p  0:01:10 }
[INFO] 2024-06-14 11:01:53,059 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  462/  625], loss: 1.909, per_step_time: 430ms, lr: 4.9308323e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:53,059 [mindformers/core/callback/callback.py:324] print_output_info:   73.9% |              | 4.64235 samples/s/p  0:01:10 }
[INFO] 2024-06-14 11:01:53,929 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  464/  625], loss: 2.427, per_step_time: 432ms, lr: 4.8642232e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:53,930 [mindformers/core/callback/callback.py:324] print_output_info:   74.2% |             | 4.62412 samples/s/p  0:01:09 }
[INFO] 2024-06-14 11:01:54,792 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  466/  625], loss: 2.287, per_step_time: 428ms, lr: 4.798238e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:54,793 [mindformers/core/callback/callback.py:324] print_output_info:   74.6% |             | 4.66724 samples/s/p  0:01:08 }
[INFO] 2024-06-14 11:01:55,659 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  468/  625], loss: 2.021, per_step_time: 430ms, lr: 4.7328813e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:55,659 [mindformers/core/callback/callback.py:324] print_output_info:   74.9% |             | 4.64779 samples/s/p  0:01:07 }
[INFO] 2024-06-14 11:01:56,523 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  470/  625], loss: 2.129, per_step_time: 429ms, lr: 4.668159e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:56,525 [mindformers/core/callback/callback.py:324] print_output_info:   75.2% |             | 4.66099 samples/s/p  0:01:06 }
[INFO] 2024-06-14 11:01:57,383 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  472/  625], loss: 2.062, per_step_time: 426ms, lr: 4.6040795e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:57,384 [mindformers/core/callback/callback.py:324] print_output_info:   75.5% |             | 4.68769 samples/s/p  0:01:05 }
[INFO] 2024-06-14 11:01:58,238 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  474/  625], loss: 2.122, per_step_time: 424ms, lr: 4.540648e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:58,239 [mindformers/core/callback/callback.py:324] print_output_info:   75.8% |             | 4.70865 samples/s/p  0:01:04 }
[INFO] 2024-06-14 11:01:59,094 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  476/  625], loss: 2.058, per_step_time: 425ms, lr: 4.477871e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:59,095 [mindformers/core/callback/callback.py:324] print_output_info:   76.2% |            | 4.70464 samples/s/p  0:01:03 }
[INFO] 2024-06-14 11:01:59,951 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  478/  625], loss: 1.863, per_step_time: 425ms, lr: 4.415757e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:01:59,952 [mindformers/core/callback/callback.py:324] print_output_info:   76.5% |            | 4.69878 samples/s/p  0:01:02 }
[INFO] 2024-06-14 11:02:00,808 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  480/  625], loss: 2.097, per_step_time: 424ms, lr: 4.3543096e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:00,808 [mindformers/core/callback/callback.py:324] print_output_info:   76.8% |            | 4.70654 samples/s/p  0:01:01 }
[INFO] 2024-06-14 11:02:01,667 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  482/  625], loss: 1.933, per_step_time: 426ms, lr: 4.2935358e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:01,668 [mindformers/core/callback/callback.py:324] print_output_info:   77.1% |            | 4.68961 samples/s/p  0:01:00 }
[INFO] 2024-06-14 11:02:02,541 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  484/  625], loss: 2.424, per_step_time: 433ms, lr: 4.2334414e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:02,541 [mindformers/core/callback/callback.py:324] print_output_info:   77.4% |            | 4.61259 samples/s/p  0:01:01 }
[INFO] 2024-06-14 11:02:03,394 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  486/  625], loss: 2.037, per_step_time: 423ms, lr: 4.1740327e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:03,394 [mindformers/core/callback/callback.py:324] print_output_info:   77.8% |            | 4.72127 samples/s/p  0:00:58 }
[INFO] 2024-06-14 11:02:04,248 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  488/  625], loss: 1.980, per_step_time: 424ms, lr: 4.115317e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:04,248 [mindformers/core/callback/callback.py:324] print_output_info:   78.1% |           | 4.71571 samples/s/p  0:00:58 }
[INFO] 2024-06-14 11:02:05,103 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  490/  625], loss: 2.036, per_step_time: 424ms, lr: 4.0573004e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:05,103 [mindformers/core/callback/callback.py:324] print_output_info:   78.4% |           | 4.71047 samples/s/p  0:00:57 }
[INFO] 2024-06-14 11:02:05,968 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  492/  625], loss: 2.267, per_step_time: 429ms, lr: 3.999987e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:05,968 [mindformers/core/callback/callback.py:324] print_output_info:   78.7% |           | 4.65281 samples/s/p  0:00:57 }
[INFO] 2024-06-14 11:02:06,831 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  494/  625], loss: 2.131, per_step_time: 428ms, lr: 3.943383e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:06,833 [mindformers/core/callback/callback.py:324] print_output_info:   79.0% |           | 4.66269 samples/s/p  0:00:56 }
[INFO] 2024-06-14 11:02:07,710 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  496/  625], loss: 2.006, per_step_time: 435ms, lr: 3.887493e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:07,710 [mindformers/core/callback/callback.py:324] print_output_info:   79.4% |           | 4.58800 samples/s/p  0:00:56 }
[INFO] 2024-06-14 11:02:08,573 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  498/  625], loss: 2.367, per_step_time: 428ms, lr: 3.8323265e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:08,573 [mindformers/core/callback/callback.py:324] print_output_info:   79.7% |           | 4.66685 samples/s/p  0:00:54 }
[INFO] 2024-06-14 11:02:09,431 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  500/  625], loss: 2.191, per_step_time: 426ms, lr: 3.777885e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:09,431 [mindformers/core/callback/callback.py:324] print_output_info:   80.0% |          | 4.69066 samples/s/p  0:00:53 }
[INFO] 2024-06-14 11:02:10,290 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  502/  625], loss: 2.396, per_step_time: 426ms, lr: 3.724177e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:10,291 [mindformers/core/callback/callback.py:324] print_output_info:   80.3% |          | 4.68793 samples/s/p  0:00:52 }
[INFO] 2024-06-14 11:02:11,158 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  504/  625], loss: 2.191, per_step_time: 431ms, lr: 3.6712065e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:11,159 [mindformers/core/callback/callback.py:324] print_output_info:   80.6% |          | 4.63764 samples/s/p  0:00:52 }
[INFO] 2024-06-14 11:02:12,019 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  506/  625], loss: 2.175, per_step_time: 427ms, lr: 3.6189776e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:12,020 [mindformers/core/callback/callback.py:324] print_output_info:   81.0% |          | 4.67853 samples/s/p  0:00:50 }
[INFO] 2024-06-14 11:02:12,884 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  508/  625], loss: 2.094, per_step_time: 429ms, lr: 3.5674984e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:12,884 [mindformers/core/callback/callback.py:324] print_output_info:   81.3% |          | 4.65600 samples/s/p  0:00:50 }
[INFO] 2024-06-14 11:02:13,739 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  510/  625], loss: 2.040, per_step_time: 424ms, lr: 3.516773e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:13,740 [mindformers/core/callback/callback.py:324] print_output_info:   81.6% |          | 4.70894 samples/s/p  0:00:48 }
[INFO] 2024-06-14 11:02:14,597 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  512/  625], loss: 2.101, per_step_time: 425ms, lr: 3.4668067e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:14,597 [mindformers/core/callback/callback.py:324] print_output_info:   81.9% |          | 4.69773 samples/s/p  0:00:48 }
[INFO] 2024-06-14 11:02:15,451 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  514/  625], loss: 1.859, per_step_time: 424ms, lr: 3.4176028e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:15,452 [mindformers/core/callback/callback.py:324] print_output_info:   82.2% |         | 4.71132 samples/s/p  0:00:47 }
[INFO] 2024-06-14 11:02:16,313 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  516/  625], loss: 2.025, per_step_time: 427ms, lr: 3.3691672e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:16,313 [mindformers/core/callback/callback.py:324] print_output_info:   82.6% |         | 4.67602 samples/s/p  0:00:46 }
[INFO] 2024-06-14 11:02:17,174 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  518/  625], loss: 2.176, per_step_time: 427ms, lr: 3.3215056e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:17,177 [mindformers/core/callback/callback.py:324] print_output_info:   82.9% |         | 4.67466 samples/s/p  0:00:45 }
[INFO] 2024-06-14 11:02:18,039 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  520/  625], loss: 2.298, per_step_time: 428ms, lr: 3.2746239e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:18,040 [mindformers/core/callback/callback.py:324] print_output_info:   83.2% |         | 4.66443 samples/s/p  0:00:45 }
[INFO] 2024-06-14 11:02:18,909 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  522/  625], loss: 2.025, per_step_time: 431ms, lr: 3.2285252e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:18,909 [mindformers/core/callback/callback.py:324] print_output_info:   83.5% |         | 4.63023 samples/s/p  0:00:44 }
[INFO] 2024-06-14 11:02:19,773 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  524/  625], loss: 1.996, per_step_time: 429ms, lr: 3.1832142e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:19,773 [mindformers/core/callback/callback.py:324] print_output_info:   83.8% |         | 4.66200 samples/s/p  0:00:43 }
[INFO] 2024-06-14 11:02:20,636 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  526/  625], loss: 2.027, per_step_time: 428ms, lr: 3.1386953e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:20,636 [mindformers/core/callback/callback.py:324] print_output_info:   84.2% |        | 4.66963 samples/s/p  0:00:42 }
[INFO] 2024-06-14 11:02:21,491 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  528/  625], loss: 2.019, per_step_time: 424ms, lr: 3.0949732e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:21,491 [mindformers/core/callback/callback.py:324] print_output_info:   84.5% |        | 4.70734 samples/s/p  0:00:41 }
[INFO] 2024-06-14 11:02:22,349 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  530/  625], loss: 2.020, per_step_time: 426ms, lr: 3.052053e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:22,350 [mindformers/core/callback/callback.py:324] print_output_info:   84.8% |        | 4.69225 samples/s/p  0:00:40 }
[INFO] 2024-06-14 11:02:23,208 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  532/  625], loss: 2.105, per_step_time: 425ms, lr: 3.0099382e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:23,208 [mindformers/core/callback/callback.py:324] print_output_info:   85.1% |        | 4.69583 samples/s/p  0:00:39 }
[INFO] 2024-06-14 11:02:24,069 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  534/  625], loss: 2.057, per_step_time: 427ms, lr: 2.9686348e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:24,070 [mindformers/core/callback/callback.py:324] print_output_info:   85.4% |        | 4.67383 samples/s/p  0:00:38 }
[INFO] 2024-06-14 11:02:24,932 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  536/  625], loss: 2.122, per_step_time: 428ms, lr: 2.9281446e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:24,933 [mindformers/core/callback/callback.py:324] print_output_info:   85.8% |        | 4.66762 samples/s/p  0:00:38 }
[INFO] 2024-06-14 11:02:25,787 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  538/  625], loss: 2.170, per_step_time: 424ms, lr: 2.8884726e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:25,788 [mindformers/core/callback/callback.py:324] print_output_info:   86.1% |       | 4.70885 samples/s/p  0:00:36 }
[INFO] 2024-06-14 11:02:26,642 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  540/  625], loss: 2.143, per_step_time: 424ms, lr: 2.849624e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:26,642 [mindformers/core/callback/callback.py:324] print_output_info:   86.4% |       | 4.71413 samples/s/p  0:00:36 }
[INFO] 2024-06-14 11:02:27,499 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  542/  625], loss: 1.965, per_step_time: 425ms, lr: 2.811601e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:27,500 [mindformers/core/callback/callback.py:324] print_output_info:   86.7% |       | 4.70001 samples/s/p  0:00:35 }
[INFO] 2024-06-14 11:02:28,361 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  544/  625], loss: 2.018, per_step_time: 427ms, lr: 2.774409e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:28,361 [mindformers/core/callback/callback.py:324] print_output_info:   87.0% |       | 4.67562 samples/s/p  0:00:34 }
[INFO] 2024-06-14 11:02:29,224 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  546/  625], loss: 1.866, per_step_time: 428ms, lr: 2.738051e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:29,225 [mindformers/core/callback/callback.py:324] print_output_info:   87.4% |       | 4.66451 samples/s/p  0:00:33 }
[INFO] 2024-06-14 11:02:30,084 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  548/  625], loss: 2.327, per_step_time: 426ms, lr: 2.702529e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:30,084 [mindformers/core/callback/callback.py:324] print_output_info:   87.7% |       | 4.68929 samples/s/p  0:00:32 }
[INFO] 2024-06-14 11:02:30,952 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  550/  625], loss: 1.927, per_step_time: 431ms, lr: 2.6678485e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:30,973 [mindformers/core/callback/callback.py:324] print_output_info:   88.0% |      | 4.63786 samples/s/p  0:00:32 }
[INFO] 2024-06-14 11:02:31,826 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  552/  625], loss: 2.202, per_step_time: 423ms, lr: 2.634014e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:31,826 [mindformers/core/callback/callback.py:324] print_output_info:   88.3% |      | 4.72211 samples/s/p  0:00:30 }
[INFO] 2024-06-14 11:02:32,685 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  554/  625], loss: 2.099, per_step_time: 426ms, lr: 2.6010273e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:32,686 [mindformers/core/callback/callback.py:324] print_output_info:   88.6% |      | 4.69019 samples/s/p  0:00:30 }
[INFO] 2024-06-14 11:02:33,550 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  556/  625], loss: 2.097, per_step_time: 429ms, lr: 2.5688921e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:33,551 [mindformers/core/callback/callback.py:324] print_output_info:   89.0% |      | 4.65964 samples/s/p  0:00:29 }
[INFO] 2024-06-14 11:02:34,414 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  558/  625], loss: 2.084, per_step_time: 429ms, lr: 2.5376128e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:34,415 [mindformers/core/callback/callback.py:324] print_output_info:   89.3% |      | 4.66009 samples/s/p  0:00:28 }
[INFO] 2024-06-14 11:02:35,273 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  560/  625], loss: 2.029, per_step_time: 426ms, lr: 2.507189e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:35,274 [mindformers/core/callback/callback.py:324] print_output_info:   89.6% |      | 4.68851 samples/s/p  0:00:27 }
[INFO] 2024-06-14 11:02:36,135 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  562/  625], loss: 1.920, per_step_time: 427ms, lr: 2.477628e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:36,135 [mindformers/core/callback/callback.py:324] print_output_info:   89.9% |      | 4.67759 samples/s/p  0:00:26 }
[INFO] 2024-06-14 11:02:36,993 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  564/  625], loss: 2.198, per_step_time: 426ms, lr: 2.448931e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:36,994 [mindformers/core/callback/callback.py:324] print_output_info:   90.2% |     | 4.69072 samples/s/p  0:00:26 }
[INFO] 2024-06-14 11:02:37,854 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  566/  625], loss: 2.315, per_step_time: 427ms, lr: 2.4211004e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:37,855 [mindformers/core/callback/callback.py:324] print_output_info:   90.6% |     | 4.67946 samples/s/p  0:00:25 }
[INFO] 2024-06-14 11:02:38,733 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  568/  625], loss: 2.131, per_step_time: 436ms, lr: 2.3941404e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:38,733 [mindformers/core/callback/callback.py:324] print_output_info:   90.9% |     | 4.58599 samples/s/p  0:00:24 }
[INFO] 2024-06-14 11:02:39,590 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  570/  625], loss: 2.102, per_step_time: 426ms, lr: 2.3680527e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:39,591 [mindformers/core/callback/callback.py:324] print_output_info:   91.2% |     | 4.69455 samples/s/p  0:00:23 }
[INFO] 2024-06-14 11:02:40,444 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  572/  625], loss: 2.156, per_step_time: 423ms, lr: 2.34284e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:40,445 [mindformers/core/callback/callback.py:324] print_output_info:   91.5% |     | 4.71862 samples/s/p  0:00:22 }
[INFO] 2024-06-14 11:02:41,298 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  574/  625], loss: 2.269, per_step_time: 424ms, lr: 2.3185044e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:41,299 [mindformers/core/callback/callback.py:324] print_output_info:   91.8% |     | 4.71415 samples/s/p  0:00:21 }
[INFO] 2024-06-14 11:02:42,160 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  576/  625], loss: 2.171, per_step_time: 427ms, lr: 2.2950505e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:42,160 [mindformers/core/callback/callback.py:324] print_output_info:   92.2% |    | 4.67437 samples/s/p  0:00:20 }
[INFO] 2024-06-14 11:02:43,031 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  578/  625], loss: 2.046, per_step_time: 432ms, lr: 2.272477e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:43,032 [mindformers/core/callback/callback.py:324] print_output_info:   92.5% |    | 4.62777 samples/s/p  0:00:20 }
[INFO] 2024-06-14 11:02:43,891 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  580/  625], loss: 1.847, per_step_time: 427ms, lr: 2.2507902e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:43,892 [mindformers/core/callback/callback.py:324] print_output_info:   92.8% |    | 4.68361 samples/s/p  0:00:19 }
[INFO] 2024-06-14 11:02:44,746 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  582/  625], loss: 1.909, per_step_time: 424ms, lr: 2.2299891e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:44,747 [mindformers/core/callback/callback.py:324] print_output_info:   93.1% |    | 4.71157 samples/s/p  0:00:18 }
[INFO] 2024-06-14 11:02:45,601 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  584/  625], loss: 2.144, per_step_time: 424ms, lr: 2.2100787e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:45,601 [mindformers/core/callback/callback.py:324] print_output_info:   93.4% |    | 4.71187 samples/s/p  0:00:17 }
[INFO] 2024-06-14 11:02:46,467 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  586/  625], loss: 2.221, per_step_time: 430ms, lr: 2.1910591e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:46,468 [mindformers/core/callback/callback.py:324] print_output_info:   93.8% |    | 4.64707 samples/s/p  0:00:16 }
[INFO] 2024-06-14 11:02:47,337 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  588/  625], loss: 1.999, per_step_time: 431ms, lr: 2.1729327e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:47,337 [mindformers/core/callback/callback.py:324] print_output_info:   94.1% |   | 4.63196 samples/s/p  0:00:15 }
[INFO] 2024-06-14 11:02:48,202 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  590/  625], loss: 2.224, per_step_time: 429ms, lr: 2.1557007e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:48,205 [mindformers/core/callback/callback.py:324] print_output_info:   94.4% |   | 4.65294 samples/s/p  0:00:15 }
[INFO] 2024-06-14 11:02:49,065 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  592/  625], loss: 2.299, per_step_time: 427ms, lr: 2.1393655e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:49,066 [mindformers/core/callback/callback.py:324] print_output_info:   94.7% |   | 4.67678 samples/s/p  0:00:14 }
[INFO] 2024-06-14 11:02:49,921 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  594/  625], loss: 2.095, per_step_time: 425ms, lr: 2.1239277e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:49,921 [mindformers/core/callback/callback.py:324] print_output_info:   95.0% |   | 4.70470 samples/s/p  0:00:13 }
[INFO] 2024-06-14 11:02:50,781 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  596/  625], loss: 2.202, per_step_time: 426ms, lr: 2.1093917e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:50,781 [mindformers/core/callback/callback.py:324] print_output_info:   95.4% |   | 4.68390 samples/s/p  0:00:12 }
[INFO] 2024-06-14 11:02:51,648 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  598/  625], loss: 2.070, per_step_time: 430ms, lr: 2.0957564e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:51,649 [mindformers/core/callback/callback.py:324] print_output_info:   95.7% |   | 4.64311 samples/s/p  0:00:11 }
[INFO] 2024-06-14 11:02:52,506 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  600/  625], loss: 2.215, per_step_time: 425ms, lr: 2.083023e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:52,506 [mindformers/core/callback/callback.py:324] print_output_info:   96.0% |  | 4.69725 samples/s/p  0:00:10 }
[INFO] 2024-06-14 11:02:53,380 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  602/  625], loss: 2.175, per_step_time: 434ms, lr: 2.0711943e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:53,381 [mindformers/core/callback/callback.py:324] print_output_info:   96.3% |  | 4.60781 samples/s/p  0:00:09 }
[INFO] 2024-06-14 11:02:54,244 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  604/  625], loss: 2.053, per_step_time: 428ms, lr: 2.060272e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:54,244 [mindformers/core/callback/callback.py:324] print_output_info:   96.6% |  | 4.66614 samples/s/p  0:00:09 }
[INFO] 2024-06-14 11:02:55,099 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  606/  625], loss: 1.810, per_step_time: 424ms, lr: 2.0502544e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:55,100 [mindformers/core/callback/callback.py:324] print_output_info:   97.0% |  | 4.71276 samples/s/p  0:00:08 }
[INFO] 2024-06-14 11:02:55,960 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  608/  625], loss: 2.056, per_step_time: 426ms, lr: 2.0411446e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:55,960 [mindformers/core/callback/callback.py:324] print_output_info:   97.3% |  | 4.68403 samples/s/p  0:00:07 }
[INFO] 2024-06-14 11:02:56,823 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  610/  625], loss: 2.223, per_step_time: 428ms, lr: 2.032944e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:56,823 [mindformers/core/callback/callback.py:324] print_output_info:   97.6% |  | 4.66792 samples/s/p  0:00:06 }
[INFO] 2024-06-14 11:02:57,692 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  612/  625], loss: 2.132, per_step_time: 432ms, lr: 2.025652e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:57,693 [mindformers/core/callback/callback.py:324] print_output_info:   97.9% |  | 4.62916 samples/s/p  0:00:05 }
[INFO] 2024-06-14 11:02:58,562 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  614/  625], loss: 1.951, per_step_time: 431ms, lr: 2.0192701e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:58,564 [mindformers/core/callback/callback.py:324] print_output_info:   98.2% | | 4.63146 samples/s/p  0:00:04 }
[INFO] 2024-06-14 11:02:59,424 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  616/  625], loss: 2.088, per_step_time: 426ms, lr: 2.0137977e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:02:59,424 [mindformers/core/callback/callback.py:324] print_output_info:   98.6% | | 4.68398 samples/s/p  0:00:03 }
[INFO] 2024-06-14 11:03:00,288 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  618/  625], loss: 2.017, per_step_time: 429ms, lr: 2.0092375e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:03:00,288 [mindformers/core/callback/callback.py:324] print_output_info:   98.9% | | 4.66143 samples/s/p  0:00:03 }
[INFO] 2024-06-14 11:03:01,171 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  620/  625], loss: 2.174, per_step_time: 438ms, lr: 2.0055882e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:03:01,171 [mindformers/core/callback/callback.py:324] print_output_info:   99.2% | | 4.56142 samples/s/p  0:00:02 }
[INFO] 2024-06-14 11:03:02,029 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  622/  625], loss: 2.043, per_step_time: 426ms, lr: 2.0028513e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:03:02,029 [mindformers/core/callback/callback.py:324] print_output_info:   99.5% | | 4.69224 samples/s/p  0:00:01 }
[INFO] 2024-06-14 11:03:02,888 [mindformers/core/callback/callback.py:314] print_output_info: { Epoch:[  1/  1], step:[  624/  625], loss: 2.025, per_step_time: 426ms, lr: 2.0010261e-06, overflow cond: False, loss_scale: 32768.0
[INFO] 2024-06-14 11:03:02,889 [mindformers/core/callback/callback.py:324] print_output_info:   99.8% | | 4.68855 samples/s/p  0:00:00 }
[INFO] 2024-06-14 11:03:02,893 [mindformers/core/callback/callback.py:561] _save_ckpt: ......Saving ckpt......
[INFO] 2024-06-14 11:03:18,319 [mindformers/trainer/base_trainer.py:774] training_process: .........Training Over!.............
[INFO] 2024-06-14 11:06:15,206 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/output'
[INFO] 2024-06-14 11:06:15,208 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: baichuan2_7b
[INFO] 2024-06-14 11:06:15,208 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
[INFO] 2024-06-14 11:06:15,209 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 8, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
[INFO] 2024-06-14 11:06:15,209 [mindformers/trainer/base_trainer.py:233] _check_global_batch_size_for_auto_parallel: The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 2
[INFO] 2024-06-14 11:06:15,209 [mindformers/trainer/base_trainer.py:237] _check_global_batch_size_for_auto_parallel: global_batch_size = batch_size_per_card * device_num * gradient_accumulation_steps = 2 = 2 * 1 * 1
[INFO] 2024-06-14 11:06:15,210 [mindformers/trainer/base_trainer.py:246] _check_global_batch_size_for_auto_parallel: parallel_config will be change to default config: [ParallelConfig]
_recompute:[ParallelConfig]
_recompute:True
_select_recompute:False
_parallel_optimizer_comm_recompute:False
_mp_comm_recompute:True
_recompute_slice_activation:True

select_recompute:False
use_seq_parallel:False
_gradient_aggregation_group:4
_embed_dp_mp_config:[ParallelConfig]
_dp_mp_config:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_vocab_emb_dp:True
use_seq_parallel:False
select_recompute:False

_pp_config:[ParallelConfig]
_pipeline_stage:1
_micro_batch_num:1

_moe_config:[ParallelConfig]
_dpmp:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_expert_parallel:1
use_seq_parallel:False
select_recompute:False

.
[INFO] 2024-06-14 11:06:15,210 [mindformers/trainer/base_trainer.py:388] create_network: .........Build Network From Config..........
[WARNING] 2024-06-14 11:06:15,211 [mindformers/models/llama/llama_config.py:185] __init__: Argument `compute_in_2d` is deprecated.
[INFO] 2024-06-14 11:06:15,211 [mindformers/version_control.py:60] decorator: The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
[INFO] 2024-06-14 11:06:15,211 [mindformers/version_control.py:64] decorator: 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
[INFO] 2024-06-14 11:06:15,211 [mindformers/version_control.py:70] decorator: The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.
[INFO] 2024-06-14 11:06:15,212 [mindformers/version_control.py:73] decorator: The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
[WARNING] 2024-06-14 11:06:43,808 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:06:46,275 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:06:48,793 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:06:51,313 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:06:53,863 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:06:56,402 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:06:59,018 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:07:01,602 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:07:04,162 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-14 11:07:06,722 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[INFO] 2024-06-14 11:08:09,675 [mindformers/models/base_model.py:117] load_checkpoint: model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[INFO] 2024-06-14 11:08:09,690 [mindformers/trainer/base_trainer.py:539] count_parameters: Network Parameters: 7505 M.
[INFO] 2024-06-14 11:08:10,291 [mindformers/trainer/utils.py:733] load_ckpt: .............Start load checkpoint from checkpoint..................
[INFO] 2024-06-14 11:10:07,921 [mindformers/trainer/utils.py:767] load_ckpt: Network parameters are not loaded: ([], [])
[INFO] 2024-06-14 11:10:07,968 [mindformers/generation/text_generator.py:1103] generate: Generation Config is: {'max_length': 512, 'max_new_tokens': 64, 'num_beams': 1, 'do_sample': True, 'use_past': False, 'temperature': 1.0, 'top_k': 5, 'top_p': 0.85, 'repetition_penalty': 1.05, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 0, 'bos_token_id': 1, 'eos_token_id': 2, '_from_model_config': True}
[INFO] 2024-06-14 11:10:07,969 [mindformers/generation/text_generator.py:174] _get_generation_mode: The generation mode will be **SAMPLE**.
[INFO] 2024-06-14 11:11:11,979 [mindformers/generation/text_generator.py:724] _sample: total time: 64.01002478599548 s; generated tokens: 49 tokens; generate speed: 0.7655050933634465 tokens/s
[INFO] 2024-06-14 11:11:12,025 [mindformers/trainer/base_trainer.py:946] predict_process: output result is: [{'text_generation_text': ['<reserved_106><reserved_107>']}]
[INFO] 2024-06-14 11:11:12,025 [mindformers/trainer/base_trainer.py:947] predict_process: output result is saved at: text_generation_result.txt
[INFO] 2024-06-14 11:11:12,025 [mindformers/trainer/base_trainer.py:948] predict_process: .........Predict Over!.............
[INFO] 2024-06-14 11:11:12,026 [mindformers/research/baichuan2/run_baichuan2.py:182] main: [{'text_generation_text': ['<reserved_106><reserved_107>']}]
