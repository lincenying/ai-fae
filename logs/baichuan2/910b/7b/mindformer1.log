[WARNING] DEVICE(1164279,ffffba82b840,python):2024-06-14-10:49:12.163.422 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_memory_adapter.cc:103] Initialize] Reserved memory size for other components(1073741824) is less than recommend size(4007485696), It may lead to Out Of Memory in HCCL or other components, Please double check context key 'variable_memory_max_size'/'max_device_memory'
[WARNING] HCCL_ADPT(1164279,ffffba82b840,python):2024-06-14-10:49:38.085.118 [mindspore/ccsrc/plugin/device/ascend/hal/hccl_adapter/hccl_adapter.cc:63] GenHcclOptions] The environment variable DEPLOY_MODE is not set. Now set to default value 0
2024-06-14 10:49:38,123 - mindformers[mindformers/tools/utils.py:155] - INFO - set output path to '/home/ma-user/work/mindformers/research/output'
2024-06-14 10:49:38,123 - mindformers[mindformers/trainer/base_trainer.py:90] - INFO - Now Running Task is: text_generation, Model is: baichuan2_7b_lora
2024-06-14 10:49:38,124 - mindformers[mindformers/trainer/base_trainer.py:131] - WARNING - Input model name is not in the supported list or unspecified.
2024-06-14 10:49:38,124 - mindformers[mindformers/trainer/base_trainer.py:132] - WARNING - See the list of supported task and model name: OrderedDict([('general', OrderedDict([('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/general/run_general_task.yaml')])), ('masked_image_modeling', OrderedDict([('mae_vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/mae/run_mae_vit_base_p16_224_800ep.yaml')])), ('image_classification', OrderedDict([('vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('mindspore/vit_base_p16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('mindspore/swin_base_p4w7', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/vit/run_vit_base_p16_224_100ep.yaml')])), ('fill_mask', OrderedDict([('bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_base_uncased.yaml'), ('bert_tiny_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bert/run_bert_tiny_uncased.yaml')])), ('contrastive_language_image_pretrain', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('blip2_stage1_vit_g', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml'), ('blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml'), ('blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml')])), ('image_to_text_retrieval', OrderedDict([('blip2_stage1_evaluator', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml')])), ('zero_shot_image_classification', OrderedDict([('clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_b_16', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14@336', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml'), ('blip2_stage1_classification', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml')])), ('image_to_text_generation', OrderedDict([('itt_blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml'), ('itt_blip2_stage2_vit_g_llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/clip/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml')])), ('translation', OrderedDict([('t5_small', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml'), ('t5_tiny', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_tiny_on_wmt16.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/t5/run_t5_small_on_wmt16.yaml')])), ('text_classification', OrderedDict([('txtcls_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml'), ('txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('mindspore/txtcls_bert_base_uncased_mnli', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('gpt2_txtcls', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_txtcls.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/txtcls/run_txtcls_bert_base_uncased.yaml')])), ('token_classification', OrderedDict([('tokcls_bert_base_chinese', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml'), ('tokcls_bert_base_chinese_cluener', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/tokcls/run_tokcls_bert_base_chinese.yaml')])), ('question_answering', OrderedDict([('qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('qa_bert_base_uncased_squad', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('mindspore/qa_bert_base_uncased', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/qa/run_qa_bert_base_uncased.yaml')])), ('text_generation', OrderedDict([('gpt2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml'), ('gpt2_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_lora.yaml'), ('gpt2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_13b.yaml'), ('gpt2_52b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_52b.yaml'), ('gpt2_xl', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl.yaml'), ('gpt2_xl_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2_xl_lora.yaml'), ('llama_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b.yaml'), ('llama_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_13b.yaml'), ('llama_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_65b.yaml'), ('llama2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_7b.yaml'), ('llama2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_13b.yaml'), ('llama2_70b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama2/run_llama2_70b.yaml'), ('codellama_34b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codellama/run_codellama_34b_910b.yaml'), ('llama_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/llama/run_llama_7b_lora.yaml'), ('pangualpha_2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_2_6b.yaml'), ('pangualpha_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/pangualpha/run_pangualpha_13b.yaml'), ('glm_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_finetune.yaml'), ('glm_6b_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_infer.yaml'), ('glm_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora.yaml'), ('glm_6b_lora_chat', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm/run_glm_6b_lora_infer.yaml'), ('glm2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b.yaml'), ('glm2_6b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_lora.yaml'), ('glm2_6b_ptuning2', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm2/run_glm2_6b_ptuning2.yaml'), ('glm3_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/glm3/run_glm3_6b.yaml'), ('codegeex2_6b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/codegeex2/run_codegeex2_6b.yaml'), ('bloom_560m', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_560m.yaml'), ('bloom_7.1b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_7.1b.yaml'), ('bloom_65b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_65b.yaml'), ('bloom_176b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/bloom/run_bloom_176b.yaml'), ('baichuan_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan/run_baichuan_7b.yaml'), ('baichuan2_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_7b.yaml'), ('baichuan2_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/baichuan2/run_baichuan2_13b.yaml'), ('ziya_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/ziya/run_ziya_13b.yaml'), ('skywork_13b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/skywork/run_skywork_13b.yaml'), ('internlm_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b.yaml'), ('internlm_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/internlm/run_internlm_7b_lora.yaml'), ('qwen_7b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b.yaml'), ('qwen_7b_lora', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/research/qwen/run_qwen_7b_lora.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml')])), ('segment_anything', OrderedDict([('sam_vit_b', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-b.yaml'), ('sam_vit_l', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-l.yaml'), ('sam_vit_h', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml'), ('common', '/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/sam/run_sam_vit-h.yaml')]))])
2024-06-14 10:49:38,125 - mindformers[mindformers/trainer/base_trainer.py:133] - WARNING - The default model config: /home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/configs/gpt2/run_gpt2.yaml will now be used for the text_generation task 
2024-06-14 10:49:38,125 - mindformers[mindformers/core/parallel_config.py:45] - INFO - initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
2024-06-14 10:49:38,126 - mindformers[mindformers/core/parallel_config.py:51] - INFO - initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
2024-06-14 10:49:38,126 - mindformers[mindformers/trainer/base_trainer.py:196] - INFO - The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 16 = 2 * 8 * 1 * 1
2024-06-14 10:49:38,126 - mindformers[mindformers/trainer/base_trainer.py:624] - INFO - .........Build Dataset For Train..........
2024-06-14 10:49:38,126 - mindformers[mindformers/trainer/base_trainer.py:353] - INFO - .........Build Dataset From Config..........
2024-06-14 10:49:38,127 - mindformers[mindformers/dataset/causal_language_model_dataset.py:166] - INFO - Now Create Causal Language Model Dataset.
2024-06-14 10:49:38,133 - mindformers[mindformers/trainer/utils.py:155] - INFO - Will be Training epochs:1, sink_size:2
2024-06-14 10:49:38,133 - mindformers[mindformers/trainer/utils.py:157] - INFO - Create training dataset finish, dataset size:625
2024-06-14 10:49:38,134 - mindformers[mindformers/trainer/base_trainer.py:657] - INFO - .........Build Net For Train..........
2024-06-14 10:49:38,134 - mindformers[mindformers/trainer/base_trainer.py:388] - INFO - .........Build Network From Config..........
2024-06-14 10:49:38,134 - mindformers[mindformers/models/llama/llama_config.py:185] - WARNING - Argument `compute_in_2d` is deprecated.
2024-06-14 10:49:38,135 - mindformers[mindformers/version_control.py:60] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
2024-06-14 10:49:38,135 - mindformers[mindformers/version_control.py:64] - INFO - 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
2024-06-14 10:49:38,135 - mindformers[mindformers/version_control.py:73] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
2024-06-14 10:49:38,174 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:49:38.175.236 [mindspore/common/parameter.py:786] This interface may be deleted in the future.
2024-06-14 10:49:38,195 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,215 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,236 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,258 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,279 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,299 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,319 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,340 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,360 - mindformers[mindformers/modules/layers.py:554] - WARNING - The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
2024-06-14 10:49:38,830 - mindformers[mindformers/models/base_model.py:117] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
2024-06-14 10:49:39,137 - mindformers[mindformers/models/base_model.py:117] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[INFO] 2024-06-14 10:49:39,138 [1164279] [SDK] : Start to freeze model for delta, mode: lora, include list: None, exclude list: None
[INFO] 2024-06-14 10:49:39,139 [1164279] [SDK] : Start to freeze model, include list: ['*'], exclude list: ['*mindpet_delta_lora*']
[INFO] 2024-06-14 10:49:39,145 [1164279] [SDK] : End to freeze model.
[INFO] 2024-06-14 10:49:39,146 [1164279] [SDK] : End to freeze model for delta.
2024-06-14 10:49:39,160 - mindformers[mindformers/trainer/base_trainer.py:539] - INFO - Network Parameters: 0 M.
2024-06-14 10:49:39,160 - mindformers[mindformers/trainer/base_trainer.py:682] - INFO - .........Build Optimizer For Train..........
2024-06-14 10:49:39,161 - mindformers[mindformers/trainer/base_trainer.py:435] - INFO - .........Build Optimizer From Config..........
2024-06-14 10:49:39,161 - mindformers[mindformers/trainer/base_trainer.py:468] - INFO - .........Build LR Schedule From Config..........
2024-06-14 10:49:39,166 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:74] - WARNING - dynamic_lr_schedule will be reset and invalid when layer_scale is False.
2024-06-14 10:49:39,171 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:113] - INFO - Param groups = {
  "decay": {
    "weight_decay": 0.0001,
    "params": [
      "model.layers.0.attention.wq.mindpet_delta_lora_a",
      "model.layers.0.attention.wq.mindpet_delta_lora_b",
      "model.layers.0.attention.wk.mindpet_delta_lora_a",
      "model.layers.0.attention.wk.mindpet_delta_lora_b",
      "model.layers.0.attention.wv.mindpet_delta_lora_a",
      "model.layers.0.attention.wv.mindpet_delta_lora_b",
      "model.layers.1.attention.wq.mindpet_delta_lora_a",
      "model.layers.1.attention.wq.mindpet_delta_lora_b",
      "model.layers.1.attention.wk.mindpet_delta_lora_a",
      "model.layers.1.attention.wk.mindpet_delta_lora_b",
      "model.layers.1.attention.wv.mindpet_delta_lora_a",
      "model.layers.1.attention.wv.mindpet_delta_lora_b",
      "model.layers.2.attention.wq.mindpet_delta_lora_a",
      "model.layers.2.attention.wq.mindpet_delta_lora_b",
      "model.layers.2.attention.wk.mindpet_delta_lora_a",
      "model.layers.2.attention.wk.mindpet_delta_lora_b",
      "model.layers.2.attention.wv.mindpet_delta_lora_a",
      "model.layers.2.attention.wv.mindpet_delta_lora_b",
      "model.layers.3.attention.wq.mindpet_delta_lora_a",
      "model.layers.3.attention.wq.mindpet_delta_lora_b",
      "model.layers.3.attention.wk.mindpet_delta_lora_a",
      "model.layers.3.attention.wk.mindpet_delta_lora_b",
      "model.layers.3.attention.wv.mindpet_delta_lora_a",
      "model.layers.3.attention.wv.mindpet_delta_lora_b",
      "model.layers.4.attention.wq.mindpet_delta_lora_a",
      "model.layers.4.attention.wq.mindpet_delta_lora_b",
      "model.layers.4.attention.wk.mindpet_delta_lora_a",
      "model.layers.4.attention.wk.mindpet_delta_lora_b",
      "model.layers.4.attention.wv.mindpet_delta_lora_a",
      "model.layers.4.attention.wv.mindpet_delta_lora_b",
      "model.layers.5.attention.wq.mindpet_delta_lora_a",
      "model.layers.5.attention.wq.mindpet_delta_lora_b",
      "model.layers.5.attention.wk.mindpet_delta_lora_a",
      "model.layers.5.attention.wk.mindpet_delta_lora_b",
      "model.layers.5.attention.wv.mindpet_delta_lora_a",
      "model.layers.5.attention.wv.mindpet_delta_lora_b",
      "model.layers.6.attention.wq.mindpet_delta_lora_a",
      "model.layers.6.attention.wq.mindpet_delta_lora_b",
      "model.layers.6.attention.wk.mindpet_delta_lora_a",
      "model.layers.6.attention.wk.mindpet_delta_lora_b",
      "model.layers.6.attention.wv.mindpet_delta_lora_a",
      "model.layers.6.attention.wv.mindpet_delta_lora_b",
      "model.layers.7.attention.wq.mindpet_delta_lora_a",
      "model.layers.7.attention.wq.mindpet_delta_lora_b",
      "model.layers.7.attention.wk.mindpet_delta_lora_a",
      "model.layers.7.attention.wk.mindpet_delta_lora_b",
      "model.layers.7.attention.wv.mindpet_delta_lora_a",
      "model.layers.7.attention.wv.mindpet_delta_lora_b",
      "model.layers.8.attention.wq.mindpet_delta_lora_a",
      "model.layers.8.attention.wq.mindpet_delta_lora_b",
      "model.layers.8.attention.wk.mindpet_delta_lora_a",
      "model.layers.8.attention.wk.mindpet_delta_lora_b",
      "model.layers.8.attention.wv.mindpet_delta_lora_a",
      "model.layers.8.attention.wv.mindpet_delta_lora_b",
      "model.layers.9.attention.wq.mindpet_delta_lora_a",
      "model.layers.9.attention.wq.mindpet_delta_lora_b",
      "model.layers.9.attention.wk.mindpet_delta_lora_a",
      "model.layers.9.attention.wk.mindpet_delta_lora_b",
      "model.layers.9.attention.wv.mindpet_delta_lora_a",
      "model.layers.9.attention.wv.mindpet_delta_lora_b",
      "model.layers.10.attention.wq.mindpet_delta_lora_a",
      "model.layers.10.attention.wq.mindpet_delta_lora_b",
      "model.layers.10.attention.wk.mindpet_delta_lora_a",
      "model.layers.10.attention.wk.mindpet_delta_lora_b",
      "model.layers.10.attention.wv.mindpet_delta_lora_a",
      "model.layers.10.attention.wv.mindpet_delta_lora_b",
      "model.layers.11.attention.wq.mindpet_delta_lora_a",
      "model.layers.11.attention.wq.mindpet_delta_lora_b",
      "model.layers.11.attention.wk.mindpet_delta_lora_a",
      "model.layers.11.attention.wk.mindpet_delta_lora_b",
      "model.layers.11.attention.wv.mindpet_delta_lora_a",
      "model.layers.11.attention.wv.mindpet_delta_lora_b",
      "model.layers.12.attention.wq.mindpet_delta_lora_a",
      "model.layers.12.attention.wq.mindpet_delta_lora_b",
      "model.layers.12.attention.wk.mindpet_delta_lora_a",
      "model.layers.12.attention.wk.mindpet_delta_lora_b",
      "model.layers.12.attention.wv.mindpet_delta_lora_a",
      "model.layers.12.attention.wv.mindpet_delta_lora_b",
      "model.layers.13.attention.wq.mindpet_delta_lora_a",
      "model.layers.13.attention.wq.mindpet_delta_lora_b",
      "model.layers.13.attention.wk.mindpet_delta_lora_a",
      "model.layers.13.attention.wk.mindpet_delta_lora_b",
      "model.layers.13.attention.wv.mindpet_delta_lora_a",
      "model.layers.13.attention.wv.mindpet_delta_lora_b",
      "model.layers.14.attention.wq.mindpet_delta_lora_a",
      "model.layers.14.attention.wq.mindpet_delta_lora_b",
      "model.layers.14.attention.wk.mindpet_delta_lora_a",
      "model.layers.14.attention.wk.mindpet_delta_lora_b",
      "model.layers.14.attention.wv.mindpet_delta_lora_a",
      "model.layers.14.attention.wv.mindpet_delta_lora_b",
      "model.layers.15.attention.wq.mindpet_delta_lora_a",
      "model.layers.15.attention.wq.mindpet_delta_lora_b",
      "model.layers.15.attention.wk.mindpet_delta_lora_a",
      "model.layers.15.attention.wk.mindpet_delta_lora_b",
      "model.layers.15.attention.wv.mindpet_delta_lora_a",
      "model.layers.15.attention.wv.mindpet_delta_lora_b",
      "model.layers.16.attention.wq.mindpet_delta_lora_a",
      "model.layers.16.attention.wq.mindpet_delta_lora_b",
      "model.layers.16.attention.wk.mindpet_delta_lora_a",
      "model.layers.16.attention.wk.mindpet_delta_lora_b",
      "model.layers.16.attention.wv.mindpet_delta_lora_a",
      "model.layers.16.attention.wv.mindpet_delta_lora_b",
      "model.layers.17.attention.wq.mindpet_delta_lora_a",
      "model.layers.17.attention.wq.mindpet_delta_lora_b",
      "model.layers.17.attention.wk.mindpet_delta_lora_a",
      "model.layers.17.attention.wk.mindpet_delta_lora_b",
      "model.layers.17.attention.wv.mindpet_delta_lora_a",
      "model.layers.17.attention.wv.mindpet_delta_lora_b",
      "model.layers.18.attention.wq.mindpet_delta_lora_a",
      "model.layers.18.attention.wq.mindpet_delta_lora_b",
      "model.layers.18.attention.wk.mindpet_delta_lora_a",
      "model.layers.18.attention.wk.mindpet_delta_lora_b",
      "model.layers.18.attention.wv.mindpet_delta_lora_a",
      "model.layers.18.attention.wv.mindpet_delta_lora_b",
      "model.layers.19.attention.wq.mindpet_delta_lora_a",
      "model.layers.19.attention.wq.mindpet_delta_lora_b",
      "model.layers.19.attention.wk.mindpet_delta_lora_a",
      "model.layers.19.attention.wk.mindpet_delta_lora_b",
      "model.layers.19.attention.wv.mindpet_delta_lora_a",
      "model.layers.19.attention.wv.mindpet_delta_lora_b",
      "model.layers.20.attention.wq.mindpet_delta_lora_a",
      "model.layers.20.attention.wq.mindpet_delta_lora_b",
      "model.layers.20.attention.wk.mindpet_delta_lora_a",
      "model.layers.20.attention.wk.mindpet_delta_lora_b",
      "model.layers.20.attention.wv.mindpet_delta_lora_a",
      "model.layers.20.attention.wv.mindpet_delta_lora_b",
      "model.layers.21.attention.wq.mindpet_delta_lora_a",
      "model.layers.21.attention.wq.mindpet_delta_lora_b",
      "model.layers.21.attention.wk.mindpet_delta_lora_a",
      "model.layers.21.attention.wk.mindpet_delta_lora_b",
      "model.layers.21.attention.wv.mindpet_delta_lora_a",
      "model.layers.21.attention.wv.mindpet_delta_lora_b",
      "model.layers.22.attention.wq.mindpet_delta_lora_a",
      "model.layers.22.attention.wq.mindpet_delta_lora_b",
      "model.layers.22.attention.wk.mindpet_delta_lora_a",
      "model.layers.22.attention.wk.mindpet_delta_lora_b",
      "model.layers.22.attention.wv.mindpet_delta_lora_a",
      "model.layers.22.attention.wv.mindpet_delta_lora_b",
      "model.layers.23.attention.wq.mindpet_delta_lora_a",
      "model.layers.23.attention.wq.mindpet_delta_lora_b",
      "model.layers.23.attention.wk.mindpet_delta_lora_a",
      "model.layers.23.attention.wk.mindpet_delta_lora_b",
      "model.layers.23.attention.wv.mindpet_delta_lora_a",
      "model.layers.23.attention.wv.mindpet_delta_lora_b",
      "model.layers.24.attention.wq.mindpet_delta_lora_a",
      "model.layers.24.attention.wq.mindpet_delta_lora_b",
      "model.layers.24.attention.wk.mindpet_delta_lora_a",
      "model.layers.24.attention.wk.mindpet_delta_lora_b",
      "model.layers.24.attention.wv.mindpet_delta_lora_a",
      "model.layers.24.attention.wv.mindpet_delta_lora_b",
      "model.layers.25.attention.wq.mindpet_delta_lora_a",
      "model.layers.25.attention.wq.mindpet_delta_lora_b",
      "model.layers.25.attention.wk.mindpet_delta_lora_a",
      "model.layers.25.attention.wk.mindpet_delta_lora_b",
      "model.layers.25.attention.wv.mindpet_delta_lora_a",
      "model.layers.25.attention.wv.mindpet_delta_lora_b",
      "model.layers.26.attention.wq.mindpet_delta_lora_a",
      "model.layers.26.attention.wq.mindpet_delta_lora_b",
      "model.layers.26.attention.wk.mindpet_delta_lora_a",
      "model.layers.26.attention.wk.mindpet_delta_lora_b",
      "model.layers.26.attention.wv.mindpet_delta_lora_a",
      "model.layers.26.attention.wv.mindpet_delta_lora_b",
      "model.layers.27.attention.wq.mindpet_delta_lora_a",
      "model.layers.27.attention.wq.mindpet_delta_lora_b",
      "model.layers.27.attention.wk.mindpet_delta_lora_a",
      "model.layers.27.attention.wk.mindpet_delta_lora_b",
      "model.layers.27.attention.wv.mindpet_delta_lora_a",
      "model.layers.27.attention.wv.mindpet_delta_lora_b",
      "model.layers.28.attention.wq.mindpet_delta_lora_a",
      "model.layers.28.attention.wq.mindpet_delta_lora_b",
      "model.layers.28.attention.wk.mindpet_delta_lora_a",
      "model.layers.28.attention.wk.mindpet_delta_lora_b",
      "model.layers.28.attention.wv.mindpet_delta_lora_a",
      "model.layers.28.attention.wv.mindpet_delta_lora_b",
      "model.layers.29.attention.wq.mindpet_delta_lora_a",
      "model.layers.29.attention.wq.mindpet_delta_lora_b",
      "model.layers.29.attention.wk.mindpet_delta_lora_a",
      "model.layers.29.attention.wk.mindpet_delta_lora_b",
      "model.layers.29.attention.wv.mindpet_delta_lora_a",
      "model.layers.29.attention.wv.mindpet_delta_lora_b",
      "model.layers.30.attention.wq.mindpet_delta_lora_a",
      "model.layers.30.attention.wq.mindpet_delta_lora_b",
      "model.layers.30.attention.wk.mindpet_delta_lora_a",
      "model.layers.30.attention.wk.mindpet_delta_lora_b",
      "model.layers.30.attention.wv.mindpet_delta_lora_a",
      "model.layers.30.attention.wv.mindpet_delta_lora_b",
      "model.layers.31.attention.wq.mindpet_delta_lora_a",
      "model.layers.31.attention.wq.mindpet_delta_lora_b",
      "model.layers.31.attention.wk.mindpet_delta_lora_a",
      "model.layers.31.attention.wk.mindpet_delta_lora_b",
      "model.layers.31.attention.wv.mindpet_delta_lora_a",
      "model.layers.31.attention.wv.mindpet_delta_lora_b"
    ]
  }
}
2024-06-14 10:49:39,300 - mindformers[mindformers/trainer/base_trainer.py:688] - INFO - .........Build Running Wrapper From Config For Train..........
2024-06-14 10:49:39,301 - mindformers[mindformers/trainer/base_trainer.py:505] - INFO - .........Build Model Wrapper for Train From Config..........
2024-06-14 10:49:39,310 - mindformers[mindformers/trainer/base_trainer.py:695] - INFO - .........Build Callbacks For Train..........
2024-06-14 10:49:39,312 - mindformers[mindformers/core/callback/callback.py:531] - INFO - Integrated_save is changed to False when using auto_parallel.
2024-06-14 10:49:39,313 - mindformers[mindformers/trainer/base_trainer.py:730] - INFO - .........Starting Init Train Model..........
2024-06-14 10:49:39,313 - mindformers[mindformers/trainer/utils.py:342] - INFO - .........Building model.........
[WARNING] UTILS(1164279,ffffba82b840,python):2024-06-14-10:49:45.172.783 [mindspore/ccsrc/utils/parallel_context.cc:268] ParallelParameterContextRestoreShape] The parameter scale_sense's parameter_shape in param_info is empty
[WARNING] UTILS(1164279,ffffba82b840,python):2024-06-14-10:49:45.173.934 [mindspore/ccsrc/utils/parallel_context.cc:268] ParallelParameterContextRestoreShape] The parameter current_iterator_step's parameter_shape in param_info is empty
[WARNING] UTILS(1164279,ffffba82b840,python):2024-06-14-10:49:45.173.981 [mindspore/ccsrc/utils/parallel_context.cc:268] ParallelParameterContextRestoreShape] The parameter last_overflow_iterator_step's parameter_shape in param_info is empty
[WARNING] PARALLEL(1164279,ffffba82b840,python):2024-06-14-10:50:05.329.032 [mindspore/ccsrc/frontend/parallel/ops_info/reduce_method_info.cc:457] CheckStrategy] ArgMaxWithValueInfo55 CheckStrategy for ArgMaxWithValueInfo, the strategy corresponding to axis is not one, real strategy is  8, the output index may be not compatible with the stand alone Primitive
-\|/-\2024-06-14 10:53:44,608 - mindformers[mindformers/trainer/utils.py:355] - INFO - /home/ma-user/work/mindformers/research/output is_share_disk: False
2024-06-14 10:53:44,613 - mindformers[mindformers/trainer/utils.py:356] - INFO - world_size: 8
2024-06-14 10:53:44,623 - mindformers[mindformers/trainer/utils.py:537] - INFO - .........Collecting strategy.........
2024-06-14 10:53:44,632 - mindformers[mindformers/trainer/utils.py:544] - INFO - pipeline_stage = 1, strategy using ./output/strategy/ckpt_strategy_rank_0_rank_0.ckpt
2024-06-14 10:53:44,636 - mindformers[mindformers/trainer/utils.py:400] - INFO - Make soft link of checkpoint file from /home/ma-user/work/mindformers/research/baichuan2/models to ./output/softlink_ckpt/models
2024-06-14 10:53:44,647 - mindformers[mindformers/trainer/utils.py:584] - INFO - .........Transforming ckpt.........
2024-06-14 10:53:44,650 - mindformers[mindformers/trainer/utils.py:585] - INFO - Src ckpt strategy: None
2024-06-14 10:53:44,655 - mindformers[mindformers/trainer/utils.py:586] - INFO - Src ckpt: ./output/softlink_ckpt/models
2024-06-14 10:53:44,658 - mindformers[mindformers/trainer/utils.py:587] - INFO - Dst ckpt strategy: ./output/strategy/ckpt_strategy_rank_0_rank_0.ckpt
2024-06-14 10:53:44,660 - mindformers[mindformers/trainer/utils.py:588] - INFO - Dst ckpt: ./output/transformed_checkpoint/models
2024-06-14 10:57:32,584 - mindformers[mindformers/trainer/utils.py:595] - INFO - .........Transform succeed!.........
2024-06-14 10:57:32,587 - mindformers[mindformers/trainer/utils.py:727] - INFO - Transforming checkpoint: |▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮▮|100%
2024-06-14 10:57:32,588 - mindformers[mindformers/trainer/utils.py:733] - INFO - .............Start load checkpoint from checkpoint..................
2024-06-14 10:57:32,588 - mindformers[mindformers/trainer/utils.py:251] - INFO - When distributed loads are sliced weights,load_checkpoint should be a checkpoint directory containing the directory of rank_{0-*},The directory structure is as follows: **checkpoint_root_dir/rank_{0-*}/**.ckpt
2024-06-14 10:57:40,206 - mindformers[mindformers/trainer/utils.py:264] - INFO - Distribute load is success.
2024-06-14 10:57:40,207 - mindformers[mindformers/trainer/utils.py:740] - INFO - loaded checkpoint: ./output/transformed_checkpoint/models
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.380.349 [mindspore/train/serialization.py:183] The type of model.layers.0.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.381.458 [mindspore/train/serialization.py:183] The type of model.layers.0.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.461.055 [mindspore/train/serialization.py:183] The type of model.layers.1.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.462.009 [mindspore/train/serialization.py:183] The type of model.layers.1.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.565.630 [mindspore/train/serialization.py:183] The type of model.layers.2.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.566.631 [mindspore/train/serialization.py:183] The type of model.layers.2.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.662.377 [mindspore/train/serialization.py:183] The type of model.layers.3.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.663.397 [mindspore/train/serialization.py:183] The type of model.layers.3.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.759.778 [mindspore/train/serialization.py:183] The type of model.layers.4.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.760.823 [mindspore/train/serialization.py:183] The type of model.layers.4.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.860.956 [mindspore/train/serialization.py:183] The type of model.layers.5.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.861.953 [mindspore/train/serialization.py:183] The type of model.layers.5.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.958.661 [mindspore/train/serialization.py:183] The type of model.layers.6.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:40.959.682 [mindspore/train/serialization.py:183] The type of model.layers.6.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.626.28 [mindspore/train/serialization.py:183] The type of model.layers.7.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.637.47 [mindspore/train/serialization.py:183] The type of model.layers.7.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.161.383 [mindspore/train/serialization.py:183] The type of model.layers.8.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.162.408 [mindspore/train/serialization.py:183] The type of model.layers.8.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.262.144 [mindspore/train/serialization.py:183] The type of model.layers.9.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.263.170 [mindspore/train/serialization.py:183] The type of model.layers.9.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.361.608 [mindspore/train/serialization.py:183] The type of model.layers.10.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.362.604 [mindspore/train/serialization.py:183] The type of model.layers.10.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.463.274 [mindspore/train/serialization.py:183] The type of model.layers.11.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.464.296 [mindspore/train/serialization.py:183] The type of model.layers.11.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.561.724 [mindspore/train/serialization.py:183] The type of model.layers.12.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.562.753 [mindspore/train/serialization.py:183] The type of model.layers.12.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.660.762 [mindspore/train/serialization.py:183] The type of model.layers.13.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.661.827 [mindspore/train/serialization.py:183] The type of model.layers.13.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.767.839 [mindspore/train/serialization.py:183] The type of model.layers.14.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.768.874 [mindspore/train/serialization.py:183] The type of model.layers.14.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.864.865 [mindspore/train/serialization.py:183] The type of model.layers.15.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.865.889 [mindspore/train/serialization.py:183] The type of model.layers.15.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.976.763 [mindspore/train/serialization.py:183] The type of model.layers.16.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:41.977.773 [mindspore/train/serialization.py:183] The type of model.layers.16.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.743.62 [mindspore/train/serialization.py:183] The type of model.layers.17.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.753.62 [mindspore/train/serialization.py:183] The type of model.layers.17.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.174.685 [mindspore/train/serialization.py:183] The type of model.layers.18.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.175.714 [mindspore/train/serialization.py:183] The type of model.layers.18.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.281.638 [mindspore/train/serialization.py:183] The type of model.layers.19.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.282.638 [mindspore/train/serialization.py:183] The type of model.layers.19.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.378.650 [mindspore/train/serialization.py:183] The type of model.layers.20.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.379.703 [mindspore/train/serialization.py:183] The type of model.layers.20.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.482.967 [mindspore/train/serialization.py:183] The type of model.layers.21.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.483.997 [mindspore/train/serialization.py:183] The type of model.layers.21.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.588.305 [mindspore/train/serialization.py:183] The type of model.layers.22.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.589.329 [mindspore/train/serialization.py:183] The type of model.layers.22.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.685.755 [mindspore/train/serialization.py:183] The type of model.layers.23.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.686.860 [mindspore/train/serialization.py:183] The type of model.layers.23.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.792.978 [mindspore/train/serialization.py:183] The type of model.layers.24.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.794.011 [mindspore/train/serialization.py:183] The type of model.layers.24.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.888.372 [mindspore/train/serialization.py:183] The type of model.layers.25.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.889.407 [mindspore/train/serialization.py:183] The type of model.layers.25.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.990.808 [mindspore/train/serialization.py:183] The type of model.layers.26.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:42.991.817 [mindspore/train/serialization.py:183] The type of model.layers.26.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.234.952 [mindspore/train/serialization.py:183] The type of model.layers.27.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.235.956 [mindspore/train/serialization.py:183] The type of model.layers.27.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.328.509 [mindspore/train/serialization.py:183] The type of model.layers.28.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.329.510 [mindspore/train/serialization.py:183] The type of model.layers.28.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.427.663 [mindspore/train/serialization.py:183] The type of model.layers.29.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.428.798 [mindspore/train/serialization.py:183] The type of model.layers.29.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.530.327 [mindspore/train/serialization.py:183] The type of model.layers.30.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.531.460 [mindspore/train/serialization.py:183] The type of model.layers.30.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.634.290 [mindspore/train/serialization.py:183] The type of model.layers.31.attention_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.635.371 [mindspore/train/serialization.py:183] The type of model.layers.31.ffn_norm.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.737.687 [mindspore/train/serialization.py:183] The type of model.norm_out.weight:Float16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.044 [mindspore/train/serialization.py:1378] For 'load_param_into_net', 192 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.266 [mindspore/train/serialization.py:1383] model.layers.0.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.361 [mindspore/train/serialization.py:1383] model.layers.0.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.460 [mindspore/train/serialization.py:1383] model.layers.0.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.533 [mindspore/train/serialization.py:1383] model.layers.0.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.599 [mindspore/train/serialization.py:1383] model.layers.0.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.664 [mindspore/train/serialization.py:1383] model.layers.0.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.728 [mindspore/train/serialization.py:1383] model.layers.1.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.792 [mindspore/train/serialization.py:1383] model.layers.1.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.854 [mindspore/train/serialization.py:1383] model.layers.1.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.916 [mindspore/train/serialization.py:1383] model.layers.1.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.913.977 [mindspore/train/serialization.py:1383] model.layers.1.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.037 [mindspore/train/serialization.py:1383] model.layers.1.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.098 [mindspore/train/serialization.py:1383] model.layers.2.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.159 [mindspore/train/serialization.py:1383] model.layers.2.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.220 [mindspore/train/serialization.py:1383] model.layers.2.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.280 [mindspore/train/serialization.py:1383] model.layers.2.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.340 [mindspore/train/serialization.py:1383] model.layers.2.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.401 [mindspore/train/serialization.py:1383] model.layers.2.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.461 [mindspore/train/serialization.py:1383] model.layers.3.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.521 [mindspore/train/serialization.py:1383] model.layers.3.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.581 [mindspore/train/serialization.py:1383] model.layers.3.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.641 [mindspore/train/serialization.py:1383] model.layers.3.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.701 [mindspore/train/serialization.py:1383] model.layers.3.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.762 [mindspore/train/serialization.py:1383] model.layers.3.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.820 [mindspore/train/serialization.py:1383] model.layers.4.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.890 [mindspore/train/serialization.py:1383] model.layers.4.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.914.953 [mindspore/train/serialization.py:1383] model.layers.4.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.014 [mindspore/train/serialization.py:1383] model.layers.4.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.075 [mindspore/train/serialization.py:1383] model.layers.4.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.137 [mindspore/train/serialization.py:1383] model.layers.4.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.197 [mindspore/train/serialization.py:1383] model.layers.5.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.257 [mindspore/train/serialization.py:1383] model.layers.5.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.317 [mindspore/train/serialization.py:1383] model.layers.5.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.377 [mindspore/train/serialization.py:1383] model.layers.5.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.437 [mindspore/train/serialization.py:1383] model.layers.5.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.497 [mindspore/train/serialization.py:1383] model.layers.5.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.557 [mindspore/train/serialization.py:1383] model.layers.6.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.616 [mindspore/train/serialization.py:1383] model.layers.6.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.675 [mindspore/train/serialization.py:1383] model.layers.6.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.734 [mindspore/train/serialization.py:1383] model.layers.6.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.793 [mindspore/train/serialization.py:1383] model.layers.6.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.853 [mindspore/train/serialization.py:1383] model.layers.6.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.912 [mindspore/train/serialization.py:1383] model.layers.7.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.915.972 [mindspore/train/serialization.py:1383] model.layers.7.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.032 [mindspore/train/serialization.py:1383] model.layers.7.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.091 [mindspore/train/serialization.py:1383] model.layers.7.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.150 [mindspore/train/serialization.py:1383] model.layers.7.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.217 [mindspore/train/serialization.py:1383] model.layers.7.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.279 [mindspore/train/serialization.py:1383] model.layers.8.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.338 [mindspore/train/serialization.py:1383] model.layers.8.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.421 [mindspore/train/serialization.py:1383] model.layers.8.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.486 [mindspore/train/serialization.py:1383] model.layers.8.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.548 [mindspore/train/serialization.py:1383] model.layers.8.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.609 [mindspore/train/serialization.py:1383] model.layers.8.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.671 [mindspore/train/serialization.py:1383] model.layers.9.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.731 [mindspore/train/serialization.py:1383] model.layers.9.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.791 [mindspore/train/serialization.py:1383] model.layers.9.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.850 [mindspore/train/serialization.py:1383] model.layers.9.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.911 [mindspore/train/serialization.py:1383] model.layers.9.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.916.972 [mindspore/train/serialization.py:1383] model.layers.9.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.034 [mindspore/train/serialization.py:1383] model.layers.10.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.095 [mindspore/train/serialization.py:1383] model.layers.10.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.157 [mindspore/train/serialization.py:1383] model.layers.10.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.218 [mindspore/train/serialization.py:1383] model.layers.10.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.278 [mindspore/train/serialization.py:1383] model.layers.10.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.338 [mindspore/train/serialization.py:1383] model.layers.10.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.398 [mindspore/train/serialization.py:1383] model.layers.11.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.459 [mindspore/train/serialization.py:1383] model.layers.11.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.520 [mindspore/train/serialization.py:1383] model.layers.11.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.581 [mindspore/train/serialization.py:1383] model.layers.11.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.650 [mindspore/train/serialization.py:1383] model.layers.11.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.712 [mindspore/train/serialization.py:1383] model.layers.11.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.772 [mindspore/train/serialization.py:1383] model.layers.12.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.830 [mindspore/train/serialization.py:1383] model.layers.12.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.890 [mindspore/train/serialization.py:1383] model.layers.12.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.917.948 [mindspore/train/serialization.py:1383] model.layers.12.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.009 [mindspore/train/serialization.py:1383] model.layers.12.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.067 [mindspore/train/serialization.py:1383] model.layers.12.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.127 [mindspore/train/serialization.py:1383] model.layers.13.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.187 [mindspore/train/serialization.py:1383] model.layers.13.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.245 [mindspore/train/serialization.py:1383] model.layers.13.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.305 [mindspore/train/serialization.py:1383] model.layers.13.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.365 [mindspore/train/serialization.py:1383] model.layers.13.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.424 [mindspore/train/serialization.py:1383] model.layers.13.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.483 [mindspore/train/serialization.py:1383] model.layers.14.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.542 [mindspore/train/serialization.py:1383] model.layers.14.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.601 [mindspore/train/serialization.py:1383] model.layers.14.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.660 [mindspore/train/serialization.py:1383] model.layers.14.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.718 [mindspore/train/serialization.py:1383] model.layers.14.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.776 [mindspore/train/serialization.py:1383] model.layers.14.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.835 [mindspore/train/serialization.py:1383] model.layers.15.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.894 [mindspore/train/serialization.py:1383] model.layers.15.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.918.960 [mindspore/train/serialization.py:1383] model.layers.15.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.021 [mindspore/train/serialization.py:1383] model.layers.15.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.080 [mindspore/train/serialization.py:1383] model.layers.15.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.138 [mindspore/train/serialization.py:1383] model.layers.15.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.197 [mindspore/train/serialization.py:1383] model.layers.16.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.256 [mindspore/train/serialization.py:1383] model.layers.16.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.314 [mindspore/train/serialization.py:1383] model.layers.16.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.373 [mindspore/train/serialization.py:1383] model.layers.16.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.431 [mindspore/train/serialization.py:1383] model.layers.16.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.489 [mindspore/train/serialization.py:1383] model.layers.16.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.547 [mindspore/train/serialization.py:1383] model.layers.17.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.605 [mindspore/train/serialization.py:1383] model.layers.17.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.662 [mindspore/train/serialization.py:1383] model.layers.17.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.720 [mindspore/train/serialization.py:1383] model.layers.17.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.779 [mindspore/train/serialization.py:1383] model.layers.17.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.837 [mindspore/train/serialization.py:1383] model.layers.17.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.895 [mindspore/train/serialization.py:1383] model.layers.18.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.919.953 [mindspore/train/serialization.py:1383] model.layers.18.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.012 [mindspore/train/serialization.py:1383] model.layers.18.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.071 [mindspore/train/serialization.py:1383] model.layers.18.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.130 [mindspore/train/serialization.py:1383] model.layers.18.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.188 [mindspore/train/serialization.py:1383] model.layers.18.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.254 [mindspore/train/serialization.py:1383] model.layers.19.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.316 [mindspore/train/serialization.py:1383] model.layers.19.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.394 [mindspore/train/serialization.py:1383] model.layers.19.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.461 [mindspore/train/serialization.py:1383] model.layers.19.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.523 [mindspore/train/serialization.py:1383] model.layers.19.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.585 [mindspore/train/serialization.py:1383] model.layers.19.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.646 [mindspore/train/serialization.py:1383] model.layers.20.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.706 [mindspore/train/serialization.py:1383] model.layers.20.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.765 [mindspore/train/serialization.py:1383] model.layers.20.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.825 [mindspore/train/serialization.py:1383] model.layers.20.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.886 [mindspore/train/serialization.py:1383] model.layers.20.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.920.945 [mindspore/train/serialization.py:1383] model.layers.20.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.005 [mindspore/train/serialization.py:1383] model.layers.21.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.064 [mindspore/train/serialization.py:1383] model.layers.21.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.124 [mindspore/train/serialization.py:1383] model.layers.21.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.184 [mindspore/train/serialization.py:1383] model.layers.21.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.244 [mindspore/train/serialization.py:1383] model.layers.21.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.304 [mindspore/train/serialization.py:1383] model.layers.21.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.363 [mindspore/train/serialization.py:1383] model.layers.22.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.422 [mindspore/train/serialization.py:1383] model.layers.22.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.480 [mindspore/train/serialization.py:1383] model.layers.22.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.539 [mindspore/train/serialization.py:1383] model.layers.22.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.599 [mindspore/train/serialization.py:1383] model.layers.22.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.665 [mindspore/train/serialization.py:1383] model.layers.22.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.726 [mindspore/train/serialization.py:1383] model.layers.23.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.786 [mindspore/train/serialization.py:1383] model.layers.23.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.846 [mindspore/train/serialization.py:1383] model.layers.23.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.906 [mindspore/train/serialization.py:1383] model.layers.23.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.921.965 [mindspore/train/serialization.py:1383] model.layers.23.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.025 [mindspore/train/serialization.py:1383] model.layers.23.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.085 [mindspore/train/serialization.py:1383] model.layers.24.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.145 [mindspore/train/serialization.py:1383] model.layers.24.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.205 [mindspore/train/serialization.py:1383] model.layers.24.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.264 [mindspore/train/serialization.py:1383] model.layers.24.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.323 [mindspore/train/serialization.py:1383] model.layers.24.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.383 [mindspore/train/serialization.py:1383] model.layers.24.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.443 [mindspore/train/serialization.py:1383] model.layers.25.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.502 [mindspore/train/serialization.py:1383] model.layers.25.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.562 [mindspore/train/serialization.py:1383] model.layers.25.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.621 [mindspore/train/serialization.py:1383] model.layers.25.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.680 [mindspore/train/serialization.py:1383] model.layers.25.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.738 [mindspore/train/serialization.py:1383] model.layers.25.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.797 [mindspore/train/serialization.py:1383] model.layers.26.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.855 [mindspore/train/serialization.py:1383] model.layers.26.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.914 [mindspore/train/serialization.py:1383] model.layers.26.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.922.980 [mindspore/train/serialization.py:1383] model.layers.26.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.040 [mindspore/train/serialization.py:1383] model.layers.26.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.100 [mindspore/train/serialization.py:1383] model.layers.26.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.159 [mindspore/train/serialization.py:1383] model.layers.27.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.217 [mindspore/train/serialization.py:1383] model.layers.27.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.276 [mindspore/train/serialization.py:1383] model.layers.27.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.334 [mindspore/train/serialization.py:1383] model.layers.27.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.393 [mindspore/train/serialization.py:1383] model.layers.27.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.451 [mindspore/train/serialization.py:1383] model.layers.27.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.510 [mindspore/train/serialization.py:1383] model.layers.28.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.567 [mindspore/train/serialization.py:1383] model.layers.28.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.625 [mindspore/train/serialization.py:1383] model.layers.28.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.683 [mindspore/train/serialization.py:1383] model.layers.28.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.741 [mindspore/train/serialization.py:1383] model.layers.28.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.800 [mindspore/train/serialization.py:1383] model.layers.28.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.859 [mindspore/train/serialization.py:1383] model.layers.29.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.918 [mindspore/train/serialization.py:1383] model.layers.29.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.923.976 [mindspore/train/serialization.py:1383] model.layers.29.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.034 [mindspore/train/serialization.py:1383] model.layers.29.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.092 [mindspore/train/serialization.py:1383] model.layers.29.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.150 [mindspore/train/serialization.py:1383] model.layers.29.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.209 [mindspore/train/serialization.py:1383] model.layers.30.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.268 [mindspore/train/serialization.py:1383] model.layers.30.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.333 [mindspore/train/serialization.py:1383] model.layers.30.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.413 [mindspore/train/serialization.py:1383] model.layers.30.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.479 [mindspore/train/serialization.py:1383] model.layers.30.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.540 [mindspore/train/serialization.py:1383] model.layers.30.attention.wv.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.600 [mindspore/train/serialization.py:1383] model.layers.31.attention.wq.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.660 [mindspore/train/serialization.py:1383] model.layers.31.attention.wq.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.720 [mindspore/train/serialization.py:1383] model.layers.31.attention.wk.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.779 [mindspore/train/serialization.py:1383] model.layers.31.attention.wk.mindpet_delta_lora_b is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.838 [mindspore/train/serialization.py:1383] model.layers.31.attention.wv.mindpet_delta_lora_a is not loaded.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.924.897 [mindspore/train/serialization.py:1383] model.layers.31.attention.wv.mindpet_delta_lora_b is not loaded.
2024-06-14 10:57:43,925 - mindformers[mindformers/trainer/utils.py:767] - INFO - Network parameters are not loaded: (['model.layers.0.attention.wq.mindpet_delta_lora_a', 'model.layers.0.attention.wq.mindpet_delta_lora_b', 'model.layers.0.attention.wk.mindpet_delta_lora_a', 'model.layers.0.attention.wk.mindpet_delta_lora_b', 'model.layers.0.attention.wv.mindpet_delta_lora_a', 'model.layers.0.attention.wv.mindpet_delta_lora_b', 'model.layers.1.attention.wq.mindpet_delta_lora_a', 'model.layers.1.attention.wq.mindpet_delta_lora_b', 'model.layers.1.attention.wk.mindpet_delta_lora_a', 'model.layers.1.attention.wk.mindpet_delta_lora_b', 'model.layers.1.attention.wv.mindpet_delta_lora_a', 'model.layers.1.attention.wv.mindpet_delta_lora_b', 'model.layers.2.attention.wq.mindpet_delta_lora_a', 'model.layers.2.attention.wq.mindpet_delta_lora_b', 'model.layers.2.attention.wk.mindpet_delta_lora_a', 'model.layers.2.attention.wk.mindpet_delta_lora_b', 'model.layers.2.attention.wv.mindpet_delta_lora_a', 'model.layers.2.attention.wv.mindpet_delta_lora_b', 'model.layers.3.attention.wq.mindpet_delta_lora_a', 'model.layers.3.attention.wq.mindpet_delta_lora_b', 'model.layers.3.attention.wk.mindpet_delta_lora_a', 'model.layers.3.attention.wk.mindpet_delta_lora_b', 'model.layers.3.attention.wv.mindpet_delta_lora_a', 'model.layers.3.attention.wv.mindpet_delta_lora_b', 'model.layers.4.attention.wq.mindpet_delta_lora_a', 'model.layers.4.attention.wq.mindpet_delta_lora_b', 'model.layers.4.attention.wk.mindpet_delta_lora_a', 'model.layers.4.attention.wk.mindpet_delta_lora_b', 'model.layers.4.attention.wv.mindpet_delta_lora_a', 'model.layers.4.attention.wv.mindpet_delta_lora_b', 'model.layers.5.attention.wq.mindpet_delta_lora_a', 'model.layers.5.attention.wq.mindpet_delta_lora_b', 'model.layers.5.attention.wk.mindpet_delta_lora_a', 'model.layers.5.attention.wk.mindpet_delta_lora_b', 'model.layers.5.attention.wv.mindpet_delta_lora_a', 'model.layers.5.attention.wv.mindpet_delta_lora_b', 'model.layers.6.attention.wq.mindpet_delta_lora_a', 'model.layers.6.attention.wq.mindpet_delta_lora_b', 'model.layers.6.attention.wk.mindpet_delta_lora_a', 'model.layers.6.attention.wk.mindpet_delta_lora_b', 'model.layers.6.attention.wv.mindpet_delta_lora_a', 'model.layers.6.attention.wv.mindpet_delta_lora_b', 'model.layers.7.attention.wq.mindpet_delta_lora_a', 'model.layers.7.attention.wq.mindpet_delta_lora_b', 'model.layers.7.attention.wk.mindpet_delta_lora_a', 'model.layers.7.attention.wk.mindpet_delta_lora_b', 'model.layers.7.attention.wv.mindpet_delta_lora_a', 'model.layers.7.attention.wv.mindpet_delta_lora_b', 'model.layers.8.attention.wq.mindpet_delta_lora_a', 'model.layers.8.attention.wq.mindpet_delta_lora_b', 'model.layers.8.attention.wk.mindpet_delta_lora_a', 'model.layers.8.attention.wk.mindpet_delta_lora_b', 'model.layers.8.attention.wv.mindpet_delta_lora_a', 'model.layers.8.attention.wv.mindpet_delta_lora_b', 'model.layers.9.attention.wq.mindpet_delta_lora_a', 'model.layers.9.attention.wq.mindpet_delta_lora_b', 'model.layers.9.attention.wk.mindpet_delta_lora_a', 'model.layers.9.attention.wk.mindpet_delta_lora_b', 'model.layers.9.attention.wv.mindpet_delta_lora_a', 'model.layers.9.attention.wv.mindpet_delta_lora_b', 'model.layers.10.attention.wq.mindpet_delta_lora_a', 'model.layers.10.attention.wq.mindpet_delta_lora_b', 'model.layers.10.attention.wk.mindpet_delta_lora_a', 'model.layers.10.attention.wk.mindpet_delta_lora_b', 'model.layers.10.attention.wv.mindpet_delta_lora_a', 'model.layers.10.attention.wv.mindpet_delta_lora_b', 'model.layers.11.attention.wq.mindpet_delta_lora_a', 'model.layers.11.attention.wq.mindpet_delta_lora_b', 'model.layers.11.attention.wk.mindpet_delta_lora_a', 'model.layers.11.attention.wk.mindpet_delta_lora_b', 'model.layers.11.attention.wv.mindpet_delta_lora_a', 'model.layers.11.attention.wv.mindpet_delta_lora_b', 'model.layers.12.attention.wq.mindpet_delta_lora_a', 'model.layers.12.attention.wq.mindpet_delta_lora_b', 'model.layers.12.attention.wk.mindpet_delta_lora_a', 'model.layers.12.attention.wk.mindpet_delta_lora_b', 'model.layers.12.attention.wv.mindpet_delta_lora_a', 'model.layers.12.attention.wv.mindpet_delta_lora_b', 'model.layers.13.attention.wq.mindpet_delta_lora_a', 'model.layers.13.attention.wq.mindpet_delta_lora_b', 'model.layers.13.attention.wk.mindpet_delta_lora_a', 'model.layers.13.attention.wk.mindpet_delta_lora_b', 'model.layers.13.attention.wv.mindpet_delta_lora_a', 'model.layers.13.attention.wv.mindpet_delta_lora_b', 'model.layers.14.attention.wq.mindpet_delta_lora_a', 'model.layers.14.attention.wq.mindpet_delta_lora_b', 'model.layers.14.attention.wk.mindpet_delta_lora_a', 'model.layers.14.attention.wk.mindpet_delta_lora_b', 'model.layers.14.attention.wv.mindpet_delta_lora_a', 'model.layers.14.attention.wv.mindpet_delta_lora_b', 'model.layers.15.attention.wq.mindpet_delta_lora_a', 'model.layers.15.attention.wq.mindpet_delta_lora_b', 'model.layers.15.attention.wk.mindpet_delta_lora_a', 'model.layers.15.attention.wk.mindpet_delta_lora_b', 'model.layers.15.attention.wv.mindpet_delta_lora_a', 'model.layers.15.attention.wv.mindpet_delta_lora_b', 'model.layers.16.attention.wq.mindpet_delta_lora_a', 'model.layers.16.attention.wq.mindpet_delta_lora_b', 'model.layers.16.attention.wk.mindpet_delta_lora_a', 'model.layers.16.attention.wk.mindpet_delta_lora_b', 'model.layers.16.attention.wv.mindpet_delta_lora_a', 'model.layers.16.attention.wv.mindpet_delta_lora_b', 'model.layers.17.attention.wq.mindpet_delta_lora_a', 'model.layers.17.attention.wq.mindpet_delta_lora_b', 'model.layers.17.attention.wk.mindpet_delta_lora_a', 'model.layers.17.attention.wk.mindpet_delta_lora_b', 'model.layers.17.attention.wv.mindpet_delta_lora_a', 'model.layers.17.attention.wv.mindpet_delta_lora_b', 'model.layers.18.attention.wq.mindpet_delta_lora_a', 'model.layers.18.attention.wq.mindpet_delta_lora_b', 'model.layers.18.attention.wk.mindpet_delta_lora_a', 'model.layers.18.attention.wk.mindpet_delta_lora_b', 'model.layers.18.attention.wv.mindpet_delta_lora_a', 'model.layers.18.attention.wv.mindpet_delta_lora_b', 'model.layers.19.attention.wq.mindpet_delta_lora_a', 'model.layers.19.attention.wq.mindpet_delta_lora_b', 'model.layers.19.attention.wk.mindpet_delta_lora_a', 'model.layers.19.attention.wk.mindpet_delta_lora_b', 'model.layers.19.attention.wv.mindpet_delta_lora_a', 'model.layers.19.attention.wv.mindpet_delta_lora_b', 'model.layers.20.attention.wq.mindpet_delta_lora_a', 'model.layers.20.attention.wq.mindpet_delta_lora_b', 'model.layers.20.attention.wk.mindpet_delta_lora_a', 'model.layers.20.attention.wk.mindpet_delta_lora_b', 'model.layers.20.attention.wv.mindpet_delta_lora_a', 'model.layers.20.attention.wv.mindpet_delta_lora_b', 'model.layers.21.attention.wq.mindpet_delta_lora_a', 'model.layers.21.attention.wq.mindpet_delta_lora_b', 'model.layers.21.attention.wk.mindpet_delta_lora_a', 'model.layers.21.attention.wk.mindpet_delta_lora_b', 'model.layers.21.attention.wv.mindpet_delta_lora_a', 'model.layers.21.attention.wv.mindpet_delta_lora_b', 'model.layers.22.attention.wq.mindpet_delta_lora_a', 'model.layers.22.attention.wq.mindpet_delta_lora_b', 'model.layers.22.attention.wk.mindpet_delta_lora_a', 'model.layers.22.attention.wk.mindpet_delta_lora_b', 'model.layers.22.attention.wv.mindpet_delta_lora_a', 'model.layers.22.attention.wv.mindpet_delta_lora_b', 'model.layers.23.attention.wq.mindpet_delta_lora_a', 'model.layers.23.attention.wq.mindpet_delta_lora_b', 'model.layers.23.attention.wk.mindpet_delta_lora_a', 'model.layers.23.attention.wk.mindpet_delta_lora_b', 'model.layers.23.attention.wv.mindpet_delta_lora_a', 'model.layers.23.attention.wv.mindpet_delta_lora_b', 'model.layers.24.attention.wq.mindpet_delta_lora_a', 'model.layers.24.attention.wq.mindpet_delta_lora_b', 'model.layers.24.attention.wk.mindpet_delta_lora_a', 'model.layers.24.attention.wk.mindpet_delta_lora_b', 'model.layers.24.attention.wv.mindpet_delta_lora_a', 'model.layers.24.attention.wv.mindpet_delta_lora_b', 'model.layers.25.attention.wq.mindpet_delta_lora_a', 'model.layers.25.attention.wq.mindpet_delta_lora_b', 'model.layers.25.attention.wk.mindpet_delta_lora_a', 'model.layers.25.attention.wk.mindpet_delta_lora_b', 'model.layers.25.attention.wv.mindpet_delta_lora_a', 'model.layers.25.attention.wv.mindpet_delta_lora_b', 'model.layers.26.attention.wq.mindpet_delta_lora_a', 'model.layers.26.attention.wq.mindpet_delta_lora_b', 'model.layers.26.attention.wk.mindpet_delta_lora_a', 'model.layers.26.attention.wk.mindpet_delta_lora_b', 'model.layers.26.attention.wv.mindpet_delta_lora_a', 'model.layers.26.attention.wv.mindpet_delta_lora_b', 'model.layers.27.attention.wq.mindpet_delta_lora_a', 'model.layers.27.attention.wq.mindpet_delta_lora_b', 'model.layers.27.attention.wk.mindpet_delta_lora_a', 'model.layers.27.attention.wk.mindpet_delta_lora_b', 'model.layers.27.attention.wv.mindpet_delta_lora_a', 'model.layers.27.attention.wv.mindpet_delta_lora_b', 'model.layers.28.attention.wq.mindpet_delta_lora_a', 'model.layers.28.attention.wq.mindpet_delta_lora_b', 'model.layers.28.attention.wk.mindpet_delta_lora_a', 'model.layers.28.attention.wk.mindpet_delta_lora_b', 'model.layers.28.attention.wv.mindpet_delta_lora_a', 'model.layers.28.attention.wv.mindpet_delta_lora_b', 'model.layers.29.attention.wq.mindpet_delta_lora_a', 'model.layers.29.attention.wq.mindpet_delta_lora_b', 'model.layers.29.attention.wk.mindpet_delta_lora_a', 'model.layers.29.attention.wk.mindpet_delta_lora_b', 'model.layers.29.attention.wv.mindpet_delta_lora_a', 'model.layers.29.attention.wv.mindpet_delta_lora_b', 'model.layers.30.attention.wq.mindpet_delta_lora_a', 'model.layers.30.attention.wq.mindpet_delta_lora_b', 'model.layers.30.attention.wk.mindpet_delta_lora_a', 'model.layers.30.attention.wk.mindpet_delta_lora_b', 'model.layers.30.attention.wv.mindpet_delta_lora_a', 'model.layers.30.attention.wv.mindpet_delta_lora_b', 'model.layers.31.attention.wq.mindpet_delta_lora_a', 'model.layers.31.attention.wq.mindpet_delta_lora_b', 'model.layers.31.attention.wk.mindpet_delta_lora_a', 'model.layers.31.attention.wk.mindpet_delta_lora_b', 'model.layers.31.attention.wv.mindpet_delta_lora_a', 'model.layers.31.attention.wv.mindpet_delta_lora_b'], [])
2024-06-14 10:57:43,925 - mindformers[mindformers/trainer/base_trainer.py:765] - INFO - .........Starting Training Model..........
{'auto_trans_ckpt': True,
 'auto_tune': False,
 'autotune_per_step': 10,
 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),
               OrderedDict([('type', 'CheckpointMointor'),
                            ('prefix', 'baichuan2'),
                            ('save_checkpoint_steps', 1000),
                            ('keep_checkpoint_max', 5),
                            ('save_trainable_params', False),
                            ('integrated_save', False),
                            ('async_save', False)]),
               OrderedDict([('type', 'ObsMonitor')])],
 'context': {'device_id': 0,
             'device_target': 'Ascend',
             'enable_graph_kernel': False,
             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '
                                   '--enable_parallel_fusion=true '
                                   '--reduce_fuse_depth=8 '
                                   '--enable_auto_tensor_inplace=true',
             'max_call_depth': 10000,
             'save_graphs': False,
             'save_graphs_path': './graph'},
 'data_size': 625,
 'device_num': 8,
 'do_eval': False,
 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor')])],
 'eval_dataset': {'auto_tune': False,
                  'autotune_per_step': 10,
                  'batch_size': 16,
                  'data_loader': {'dataset_dir': '',
                                  'shuffle': False,
                                  'type': 'MindDataset'},
                  'do_eval': True,
                  'drop_remainder': False,
                  'filepath_prefix': './autotune',
                  'input_columns': ['input_ids'],
                  'num_parallel_workers': 8,
                  'numa_enable': False,
                  'prefetch_size': 1,
                  'profile': False,
                  'python_multiprocessing': False,
                  'repeat': 1,
                  'seed': 0},
 'eval_dataset_task': {'dataset_config': {'auto_tune': False,
                                          'autotune_per_step': 10,
                                          'batch_size': 16,
                                          'data_loader': {'dataset_dir': '',
                                                          'shuffle': False,
                                                          'type': 'MindDataset'},
                                          'do_eval': True,
                                          'drop_remainder': False,
                                          'filepath_prefix': './autotune',
                                          'input_columns': ['input_ids'],
                                          'num_parallel_workers': 8,
                                          'numa_enable': False,
                                          'prefetch_size': 1,
                                          'profile': False,
                                          'python_multiprocessing': False,
                                          'repeat': 1,
                                          'seed': 0},
                       'type': 'CausalLanguageModelDataset'},
 'eval_epoch_interval': 50,
 'eval_step_interval': -1,
 'filepath_prefix': './autotune',
 'init_start_profile': False,
 'layer_decay': 0.65,
 'layer_scale': False,
 'load_checkpoint': './output/transformed_checkpoint',
 'local_rank': 0,
 'lr_scale_factor': 256,
 'lr_schedule': {'learning_rate': 2e-05,
                 'lr_end': 2e-06,
                 'total_steps': 624,
                 'type': 'CosineWithWarmUpLR',
                 'warmup_steps': 0},
 'metric': {'type': 'PerplexityMetric'},
 'micro_batch_interleave_num': 1,
 'model': {'arch': {'type': 'Baichuan7BV2ForCausalLM'},
           'model_config': {'assistant_token_id': 196,
                            'batch_size': 1,
                            'bos_token_id': 1,
                            'checkpoint_name_or_path': None,
                            'compute_dtype': 'float16',
                            'compute_in_2d': True,
                            'do_sample': False,
                            'eos_token_id': 2,
                            'hidden_size': 4096,
                            'ignore_token_id': -100,
                            'layernorm_compute_type': 'float32',
                            'max_decode_length': 512,
                            'multiple_of': 256,
                            'num_heads': 32,
                            'num_layers': 32,
                            'offset': 0,
                            'pad_token_id': 0,
                            'param_init_type': 'float16',
                            'pet_config': {'lora_alpha': 32,
                                           'lora_dropout': 0.1,
                                           'lora_rank': 1,
                                           'pet_type': 'lora',
                                           'target_modules': '.*wq|.*wk|.*wv'},
                            'repetition_penalty': 1.05,
                            'rms_norm_eps': 1e-06,
                            'rotary_dtype': 'float32',
                            'seq_length': 512,
                            'softmax_compute_type': 'float32',
                            'temperature': 1.0,
                            'top_k': 5,
                            'top_p': 0.85,
                            'type': 'LlamaConfig',
                            'use_flash_attention': False,
                            'use_past': False,
                            'user_token_id': 195,
                            'vocab_size': 125696}},
 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xfffec6959a30>,
 'only_save_strategy': False,
 'optimizer': {'beta1': 0.9,
               'beta2': 0.98,
               'eps': 1e-08,
               'type': 'FP32StateAdamWeightDecay',
               'weight_decay': 0.0001},
 'output_dir': './output',
 'parallel': {'device_num': 8,
              'enable_alltoall': False,
              'enable_parallel_optimizer': True,
              'full_batch': True,
              'gradients_mean': False,
              'parallel_mode': 'semi_auto_parallel',
              'parallel_optimizer_config': {'gradient_accumulation_shard': False,
                                            'parallel_optimizer_threshold': 64},
              'search_mode': 'sharding_propagation',
              'strategy_ckpt_config': {'only_trainable_params': False,
                                       'save_file': './ckpt_strategy.ckpt'},
              'strategy_ckpt_save_file': './output/strategy/ckpt_strategy_rank_0_rank_0.ckpt'},
 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffe6806e610>,
 'processor': {'return_tensors': 'ms',
               'tokenizer': {'bos_token': '<s>',
                             'eos_token': '</s>',
                             'pad_token': '<unk>',
                             'type': 'Baichuan2Tokenizer',
                             'unk_token': '<unk>',
                             'vocab_file': './tokenizer.model'},
               'type': 'LlamaProcessor'},
 'profile': False,
 'profile_communication': False,
 'profile_memory': True,
 'profile_start_step': 1,
 'profile_stop_step': 10,
 'rank_id': 0,
 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xfffdc4274a30>,
 'remote_save_url': 'Please input obs url on AICC platform.',
 'resume_training': False,
 'run_mode': 'train',
 'runner_config': {'batch_size': 16,
                   'epochs': 312,
                   'gradient_accumulation_steps': 1,
                   'initial_epoch': 0,
                   'initial_step': 0,
                   'origin_epochs': 1,
                   'sink_mode': True,
                   'sink_size': 2},
 'runner_wrapper': {'scale_sense': DynamicLossScaleUpdateCell<>,
                    'type': 'MFTrainOneStepCell',
                    'use_clip_grad': True},
 'seed': 0,
 'src_strategy_path_or_dir': '',
 'train_dataset': {'auto_tune': False,
                   'autotune_per_step': 10,
                   'batch_size': 16,
                   'data_loader': {'dataset_dir': '/home/ma-user/work/mindformers/research/baichuan2/models/belle_512.mindrecord',
                                   'shuffle': True,
                                   'type': 'MindDataset'},
                   'do_eval': False,
                   'drop_remainder': True,
                   'filepath_prefix': './autotune',
                   'input_columns': ['input_ids', 'labels'],
                   'num_parallel_workers': 8,
                   'numa_enable': False,
                   'prefetch_size': 1,
                   'profile': False,
                   'python_multiprocessing': False,
                   'repeat': 1,
                   'seed': 0},
 'train_dataset_task': {'dataset_config': {'auto_tune': False,
                                           'autotune_per_step': 10,
                                           'batch_size': 16,
                                           'data_loader': {'dataset_dir': '/home/ma-user/work/mindformers/research/baichuan2/models/belle_512.mindrecord',
                                                           'shuffle': True,
                                                           'type': 'MindDataset'},
                                           'do_eval': False,
                                           'drop_remainder': True,
                                           'filepath_prefix': './autotune',
                                           'input_columns': ['input_ids',
                                                             'labels'],
                                           'num_parallel_workers': 8,
                                           'numa_enable': False,
                                           'prefetch_size': 1,
                                           'profile': False,
                                           'python_multiprocessing': False,
                                           'repeat': 1,
                                           'seed': 0},
                        'type': 'CausalLanguageModelDataset'},
 'trainer': {'model_name': 'baichuan2_7b_lora',
             'type': 'CausalLanguageModelingTrainer'},
 'use_parallel': True}
2024-06-14 10:57:43,931 - mindformers[mindformers/trainer/base_trainer.py:768] - INFO - .........Model Compiling, Please Wait a Moment...........
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.932.088 [mindspore/train/model.py:1106] For MFLossMonitor callback, {'epoch_begin', 'step_end', 'epoch_end', 'step_begin'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.932.221 [mindspore/train/model.py:1106] For Local2ObsMonitor callback, {'step_end', 'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.
[WARNING] ME(1164279:281473810872384,MainProcess):2024-06-14-10:57:43.932.598 [mindspore/train/model.py:651] In dataset_sink mode (dataset_size % sink_size) should equal to 0, it is suggested to pad/drop data or adjust sink_size. But got 'dataset_size': 625, 'sink_size': 2.
2024-06-14 10:58:34,059 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[    2/  625], loss: 2.709, per_step_time: 25060ms, lr: 0.0, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:34,059 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    0.3% |                                                  | 0.07981 samples/s/p  4:20:12 }
2024-06-14 10:58:35,545 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[    4/  625], loss: 2.639, per_step_time: 441ms, lr: 1.9999885e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:35,546 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    0.6% |                                                  | 4.52960 samples/s/p  0:04:34 }
2024-06-14 10:58:36,412 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[    6/  625], loss: 2.516, per_step_time: 430ms, lr: 1.9998974e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:36,412 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    1.0% |                                                  | 4.64895 samples/s/p  0:04:26 }
2024-06-14 10:58:37,272 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[    8/  625], loss: 2.745, per_step_time: 427ms, lr: 1.999715e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:37,272 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    1.3% |                                                  | 4.68214 samples/s/p  0:04:23 }
2024-06-14 10:58:38,132 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   10/  625], loss: 2.405, per_step_time: 427ms, lr: 1.9994412e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:38,132 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    1.6% |                                                  | 4.68071 samples/s/p  0:04:22 }
2024-06-14 10:58:38,992 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   12/  625], loss: 2.561, per_step_time: 427ms, lr: 1.9990763e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:38,992 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    1.9% |                                                  | 4.68351 samples/s/p  0:04:21 }
2024-06-14 10:58:39,853 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   14/  625], loss: 2.280, per_step_time: 427ms, lr: 1.99862e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:39,855 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    2.2% |█                                                 | 4.67483 samples/s/p  0:04:21 }
2024-06-14 10:58:40,726 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   16/  625], loss: 2.301, per_step_time: 430ms, lr: 1.9980731e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:40,726 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    2.6% |█                                                 | 4.64747 samples/s/p  0:04:22 }
2024-06-14 10:58:41,586 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   18/  625], loss: 2.191, per_step_time: 427ms, lr: 1.9974346e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:41,587 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    2.9% |█                                                 | 4.67968 samples/s/p  0:04:19 }
2024-06-14 10:58:42,452 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   20/  625], loss: 2.278, per_step_time: 430ms, lr: 1.9967056e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:42,453 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    3.2% |█                                                 | 4.64896 samples/s/p  0:04:20 }
2024-06-14 10:58:43,321 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   22/  625], loss: 2.416, per_step_time: 431ms, lr: 1.9958854e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:43,321 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    3.5% |█                                                 | 4.63531 samples/s/p  0:04:20 }
2024-06-14 10:58:44,185 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   24/  625], loss: 2.048, per_step_time: 428ms, lr: 1.9949744e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:44,185 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    3.8% |█                                                 | 4.66230 samples/s/p  0:04:17 }
2024-06-14 10:58:45,051 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   26/  625], loss: 2.160, per_step_time: 430ms, lr: 1.9939727e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:45,052 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    4.2% |██                                                | 4.64652 samples/s/p  0:04:17 }
2024-06-14 10:58:45,918 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   28/  625], loss: 2.213, per_step_time: 430ms, lr: 1.9928804e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:45,919 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    4.5% |██                                                | 4.64790 samples/s/p  0:04:16 }
2024-06-14 10:58:46,783 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   30/  625], loss: 2.168, per_step_time: 429ms, lr: 1.9916975e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:46,783 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    4.8% |██                                                | 4.65987 samples/s/p  0:04:15 }
2024-06-14 10:58:47,646 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   32/  625], loss: 2.279, per_step_time: 428ms, lr: 1.9904242e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:47,647 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    5.1% |██                                                | 4.66338 samples/s/p  0:04:14 }
2024-06-14 10:58:48,508 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   34/  625], loss: 2.281, per_step_time: 428ms, lr: 1.989061e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:48,509 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    5.4% |██                                                | 4.67228 samples/s/p  0:04:12 }
2024-06-14 10:58:49,375 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   36/  625], loss: 2.263, per_step_time: 430ms, lr: 1.987607e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:49,376 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    5.8% |██                                                | 4.64890 samples/s/p  0:04:13 }
2024-06-14 10:58:50,236 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   38/  625], loss: 2.248, per_step_time: 427ms, lr: 1.9860634e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:50,237 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    6.1% |███                                               | 4.68176 samples/s/p  0:04:10 }
2024-06-14 10:58:51,098 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   40/  625], loss: 2.185, per_step_time: 427ms, lr: 1.9844298e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:51,099 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    6.4% |███                                               | 4.67464 samples/s/p  0:04:10 }
2024-06-14 10:58:51,963 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   42/  625], loss: 2.082, per_step_time: 429ms, lr: 1.9827066e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:51,964 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    6.7% |███                                               | 4.65556 samples/s/p  0:04:10 }
2024-06-14 10:58:52,823 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   44/  625], loss: 2.050, per_step_time: 426ms, lr: 1.980894e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:52,823 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    7.0% |███                                               | 4.68875 samples/s/p  0:04:07 }
2024-06-14 10:58:53,693 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   46/  625], loss: 2.180, per_step_time: 432ms, lr: 1.978992e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:53,694 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    7.4% |███                                               | 4.62887 samples/s/p  0:04:10 }
2024-06-14 10:58:54,548 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   48/  625], loss: 2.318, per_step_time: 424ms, lr: 1.9770008e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:54,549 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    7.7% |███                                               | 4.71021 samples/s/p  0:04:04 }
2024-06-14 10:58:55,405 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   50/  625], loss: 2.221, per_step_time: 425ms, lr: 1.9749208e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:55,405 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    8.0% |████                                              | 4.70030 samples/s/p  0:04:04 }
2024-06-14 10:58:56,261 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   52/  625], loss: 2.323, per_step_time: 425ms, lr: 1.9727522e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:56,262 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    8.3% |████                                              | 4.70470 samples/s/p  0:04:03 }
2024-06-14 10:58:57,130 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   54/  625], loss: 2.076, per_step_time: 431ms, lr: 1.9704948e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:57,130 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    8.6% |████                                              | 4.63569 samples/s/p  0:04:06 }
2024-06-14 10:58:58,013 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   56/  625], loss: 2.198, per_step_time: 438ms, lr: 1.9681494e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:58,013 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    9.0% |████                                              | 4.56094 samples/s/p  0:04:09 }
2024-06-14 10:58:58,881 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   58/  625], loss: 2.228, per_step_time: 431ms, lr: 1.965716e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:58,881 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    9.3% |████                                              | 4.63735 samples/s/p  0:04:04 }
2024-06-14 10:58:59,746 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   60/  625], loss: 1.997, per_step_time: 429ms, lr: 1.9631947e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:58:59,747 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    9.6% |████                                              | 4.65210 samples/s/p  0:04:02 }
2024-06-14 10:59:00,609 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   62/  625], loss: 2.338, per_step_time: 428ms, lr: 1.9605859e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:00,611 - mindformers[mindformers/core/callback/callback.py:324] - INFO -    9.9% |████                                              | 4.67002 samples/s/p  0:04:01 }
2024-06-14 10:59:01,539 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   64/  625], loss: 2.319, per_step_time: 461ms, lr: 1.9578898e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:01,539 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   10.2% |█████                                             | 4.33413 samples/s/p  0:04:18 }
2024-06-14 10:59:02,396 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   66/  625], loss: 2.388, per_step_time: 425ms, lr: 1.9551066e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:02,396 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   10.6% |█████                                             | 4.70117 samples/s/p  0:03:57 }
2024-06-14 10:59:03,253 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   68/  625], loss: 2.202, per_step_time: 425ms, lr: 1.9522371e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:03,253 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   10.9% |█████                                             | 4.70539 samples/s/p  0:03:56 }
2024-06-14 10:59:04,108 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   70/  625], loss: 2.292, per_step_time: 424ms, lr: 1.9492809e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:04,109 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   11.2% |█████                                             | 4.70706 samples/s/p  0:03:55 }
2024-06-14 10:59:04,971 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   72/  625], loss: 2.164, per_step_time: 428ms, lr: 1.9462386e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:04,971 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   11.5% |█████                                             | 4.67067 samples/s/p  0:03:56 }
2024-06-14 10:59:05,843 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   74/  625], loss: 2.223, per_step_time: 433ms, lr: 1.9431107e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:05,845 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   11.8% |█████                                             | 4.61783 samples/s/p  0:03:58 }
2024-06-14 10:59:06,708 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   76/  625], loss: 2.103, per_step_time: 428ms, lr: 1.939897e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:06,708 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   12.2% |██████                                            | 4.66467 samples/s/p  0:03:55 }
2024-06-14 10:59:07,578 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   78/  625], loss: 2.215, per_step_time: 432ms, lr: 1.9365983e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:07,579 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   12.5% |██████                                            | 4.62721 samples/s/p  0:03:56 }
2024-06-14 10:59:08,453 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   80/  625], loss: 2.155, per_step_time: 434ms, lr: 1.9332148e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:08,454 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   12.8% |██████                                            | 4.60077 samples/s/p  0:03:56 }
2024-06-14 10:59:09,309 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   82/  625], loss: 2.098, per_step_time: 425ms, lr: 1.929747e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:09,310 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   13.1% |██████                                            | 4.70563 samples/s/p  0:03:50 }
2024-06-14 10:59:10,179 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   84/  625], loss: 1.905, per_step_time: 432ms, lr: 1.9261948e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:10,180 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   13.4% |██████                                            | 4.62830 samples/s/p  0:03:53 }
2024-06-14 10:59:11,038 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   86/  625], loss: 2.012, per_step_time: 426ms, lr: 1.922559e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:11,040 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   13.8% |██████                                            | 4.68741 samples/s/p  0:03:49 }
2024-06-14 10:59:11,898 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   88/  625], loss: 2.241, per_step_time: 426ms, lr: 1.9188397e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:11,899 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   14.1% |███████                                           | 4.69204 samples/s/p  0:03:48 }
2024-06-14 10:59:12,755 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   90/  625], loss: 2.043, per_step_time: 425ms, lr: 1.9150375e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:12,756 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   14.4% |███████                                           | 4.69994 samples/s/p  0:03:47 }
2024-06-14 10:59:13,611 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   92/  625], loss: 2.221, per_step_time: 425ms, lr: 1.9111525e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:13,612 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   14.7% |███████                                           | 4.70507 samples/s/p  0:03:46 }
2024-06-14 10:59:14,470 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   94/  625], loss: 2.286, per_step_time: 426ms, lr: 1.9071855e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:14,470 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   15.0% |███████                                           | 4.69203 samples/s/p  0:03:46 }
2024-06-14 10:59:15,336 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   96/  625], loss: 2.225, per_step_time: 430ms, lr: 1.9031364e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:15,336 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   15.4% |███████                                           | 4.65035 samples/s/p  0:03:47 }
2024-06-14 10:59:16,216 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[   98/  625], loss: 2.233, per_step_time: 437ms, lr: 1.899006e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:16,218 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   15.7% |███████                                           | 4.57467 samples/s/p  0:03:50 }
2024-06-14 10:59:17,084 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  100/  625], loss: 2.314, per_step_time: 430ms, lr: 1.8947945e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:17,085 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   16.0% |████████                                          | 4.65005 samples/s/p  0:03:45 }
2024-06-14 10:59:17,948 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  102/  625], loss: 1.899, per_step_time: 429ms, lr: 1.8905026e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:17,949 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   16.3% |████████                                          | 4.66158 samples/s/p  0:03:44 }
2024-06-14 10:59:18,802 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  104/  625], loss: 2.200, per_step_time: 423ms, lr: 1.8861305e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:18,803 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   16.6% |████████                                          | 4.71770 samples/s/p  0:03:40 }
2024-06-14 10:59:19,656 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  106/  625], loss: 2.201, per_step_time: 423ms, lr: 1.8816785e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:19,656 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   17.0% |████████                                          | 4.71975 samples/s/p  0:03:39 }
2024-06-14 10:59:20,515 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  108/  625], loss: 2.183, per_step_time: 426ms, lr: 1.8771472e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:20,516 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   17.3% |████████                                          | 4.68678 samples/s/p  0:03:40 }
2024-06-14 10:59:21,385 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  110/  625], loss: 2.224, per_step_time: 431ms, lr: 1.8725375e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:21,387 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   17.6% |████████                                          | 4.63176 samples/s/p  0:03:42 }
2024-06-14 10:59:22,261 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  112/  625], loss: 2.266, per_step_time: 434ms, lr: 1.8678491e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:22,262 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   17.9% |████████                                          | 4.60238 samples/s/p  0:03:42 }
2024-06-14 10:59:23,117 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  114/  625], loss: 1.901, per_step_time: 424ms, lr: 1.863083e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:23,117 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   18.2% |█████████                                         | 4.70687 samples/s/p  0:03:37 }
2024-06-14 10:59:23,972 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  116/  625], loss: 2.175, per_step_time: 424ms, lr: 1.8582396e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:23,972 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   18.6% |█████████                                         | 4.71243 samples/s/p  0:03:36 }
2024-06-14 10:59:24,836 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  118/  625], loss: 2.218, per_step_time: 429ms, lr: 1.8533194e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:24,837 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   18.9% |█████████                                         | 4.65823 samples/s/p  0:03:37 }
2024-06-14 10:59:25,699 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  120/  625], loss: 2.120, per_step_time: 428ms, lr: 1.8483226e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:25,699 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   19.2% |█████████                                         | 4.66677 samples/s/p  0:03:36 }
2024-06-14 10:59:26,561 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  122/  625], loss: 2.050, per_step_time: 428ms, lr: 1.8432502e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:26,563 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   19.5% |█████████                                         | 4.66957 samples/s/p  0:03:35 }
2024-06-14 10:59:27,435 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  124/  625], loss: 2.139, per_step_time: 433ms, lr: 1.838102e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:27,435 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   19.8% |█████████                                         | 4.61601 samples/s/p  0:03:37 }
2024-06-14 10:59:28,303 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  126/  625], loss: 2.116, per_step_time: 431ms, lr: 1.8328792e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:28,303 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   20.2% |██████████                                        | 4.63983 samples/s/p  0:03:35 }
2024-06-14 10:59:29,158 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  128/  625], loss: 2.019, per_step_time: 423ms, lr: 1.8275821e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:29,159 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   20.5% |██████████                                        | 4.71848 samples/s/p  0:03:30 }
2024-06-14 10:59:30,018 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  130/  625], loss: 2.243, per_step_time: 427ms, lr: 1.8222114e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:30,019 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   20.8% |██████████                                        | 4.68337 samples/s/p  0:03:31 }
2024-06-14 10:59:30,890 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  132/  625], loss: 2.310, per_step_time: 433ms, lr: 1.8167673e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:30,891 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   21.1% |██████████                                        | 4.61646 samples/s/p  0:03:33 }
2024-06-14 10:59:31,779 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  134/  625], loss: 2.064, per_step_time: 441ms, lr: 1.8112505e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:31,780 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   21.4% |██████████                                        | 4.53406 samples/s/p  0:03:36 }
2024-06-14 10:59:32,640 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  136/  625], loss: 2.130, per_step_time: 426ms, lr: 1.8056617e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:32,640 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   21.8% |██████████                                        | 4.68517 samples/s/p  0:03:28 }
2024-06-14 10:59:33,497 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  138/  625], loss: 2.107, per_step_time: 425ms, lr: 1.8000013e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:33,497 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   22.1% |███████████                                       | 4.70056 samples/s/p  0:03:27 }
2024-06-14 10:59:34,350 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  140/  625], loss: 2.162, per_step_time: 423ms, lr: 1.7942699e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:34,350 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   22.4% |███████████                                       | 4.72220 samples/s/p  0:03:25 }
2024-06-14 10:59:35,204 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  142/  625], loss: 1.891, per_step_time: 424ms, lr: 1.788468e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:35,205 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   22.7% |███████████                                       | 4.71328 samples/s/p  0:03:24 }
2024-06-14 10:59:36,059 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  144/  625], loss: 2.112, per_step_time: 424ms, lr: 1.7825967e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:36,059 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   23.0% |███████████                                       | 4.71507 samples/s/p  0:03:24 }
2024-06-14 10:59:36,918 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  146/  625], loss: 2.156, per_step_time: 426ms, lr: 1.7766559e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:36,919 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   23.4% |███████████                                       | 4.68793 samples/s/p  0:03:24 }
2024-06-14 10:59:37,790 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  148/  625], loss: 2.008, per_step_time: 432ms, lr: 1.7706465e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:37,791 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   23.7% |███████████                                       | 4.62170 samples/s/p  0:03:26 }
2024-06-14 10:59:38,647 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  150/  625], loss: 2.122, per_step_time: 425ms, lr: 1.7645689e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:38,647 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   24.0% |████████████                                      | 4.70122 samples/s/p  0:03:22 }
2024-06-14 10:59:39,511 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  152/  625], loss: 2.088, per_step_time: 429ms, lr: 1.7584243e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:39,512 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   24.3% |████████████                                      | 4.65859 samples/s/p  0:03:23 }
2024-06-14 10:59:40,373 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  154/  625], loss: 2.291, per_step_time: 428ms, lr: 1.7522128e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:40,374 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   24.6% |████████████                                      | 4.67279 samples/s/p  0:03:21 }
2024-06-14 10:59:41,236 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  156/  625], loss: 2.127, per_step_time: 428ms, lr: 1.7459352e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:41,237 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   25.0% |████████████                                      | 4.66569 samples/s/p  0:03:21 }
2024-06-14 10:59:42,107 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  158/  625], loss: 1.901, per_step_time: 432ms, lr: 1.739592e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:42,109 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   25.3% |████████████                                      | 4.62670 samples/s/p  0:03:21 }
2024-06-14 10:59:42,968 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  160/  625], loss: 2.024, per_step_time: 427ms, lr: 1.733184e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:42,969 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   25.6% |████████████                                      | 4.68285 samples/s/p  0:03:18 }
2024-06-14 10:59:43,830 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  162/  625], loss: 2.163, per_step_time: 428ms, lr: 1.726712e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:43,831 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   25.9% |████████████                                      | 4.67243 samples/s/p  0:03:18 }
2024-06-14 10:59:44,689 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  164/  625], loss: 1.919, per_step_time: 426ms, lr: 1.720176e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:44,689 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   26.2% |█████████████                                     | 4.68954 samples/s/p  0:03:16 }
2024-06-14 10:59:45,562 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  166/  625], loss: 2.167, per_step_time: 423ms, lr: 1.7135775e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:45,563 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   26.6% |█████████████                                     | 4.72374 samples/s/p  0:03:14 }
2024-06-14 10:59:46,417 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  168/  625], loss: 1.992, per_step_time: 424ms, lr: 1.706917e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:46,418 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   26.9% |█████████████                                     | 4.70966 samples/s/p  0:03:14 }
2024-06-14 10:59:47,277 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  170/  625], loss: 2.020, per_step_time: 426ms, lr: 1.7001945e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:47,278 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   27.2% |█████████████                                     | 4.68502 samples/s/p  0:03:14 }
2024-06-14 10:59:48,141 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  172/  625], loss: 2.204, per_step_time: 428ms, lr: 1.6934111e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:48,141 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   27.5% |█████████████                                     | 4.66577 samples/s/p  0:03:14 }
2024-06-14 10:59:49,017 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  174/  625], loss: 1.881, per_step_time: 435ms, lr: 1.6865677e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:49,017 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   27.8% |█████████████                                     | 4.59641 samples/s/p  0:03:16 }
2024-06-14 10:59:49,871 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  176/  625], loss: 1.997, per_step_time: 424ms, lr: 1.679665e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:49,872 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   28.2% |██████████████                                    | 4.71362 samples/s/p  0:03:10 }
2024-06-14 10:59:50,725 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  178/  625], loss: 1.904, per_step_time: 423ms, lr: 1.6727032e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:50,726 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   28.5% |██████████████                                    | 4.72165 samples/s/p  0:03:09 }
2024-06-14 10:59:51,579 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  180/  625], loss: 2.004, per_step_time: 424ms, lr: 1.6656835e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:51,580 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   28.8% |██████████████                                    | 4.71654 samples/s/p  0:03:08 }
2024-06-14 10:59:52,444 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  182/  625], loss: 2.180, per_step_time: 429ms, lr: 1.6586066e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:52,445 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   29.1% |██████████████                                    | 4.65817 samples/s/p  0:03:10 }
2024-06-14 10:59:53,314 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  184/  625], loss: 2.050, per_step_time: 431ms, lr: 1.651473e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:53,314 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   29.4% |██████████████                                    | 4.63728 samples/s/p  0:03:10 }
2024-06-14 10:59:54,177 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  186/  625], loss: 2.011, per_step_time: 428ms, lr: 1.6442835e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:54,178 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   29.8% |██████████████                                    | 4.66311 samples/s/p  0:03:08 }
2024-06-14 10:59:55,050 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  188/  625], loss: 2.350, per_step_time: 433ms, lr: 1.6370386e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:55,051 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   30.1% |███████████████                                   | 4.61617 samples/s/p  0:03:09 }
2024-06-14 10:59:55,916 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  190/  625], loss: 2.247, per_step_time: 429ms, lr: 1.6297396e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:55,916 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   30.4% |███████████████                                   | 4.65184 samples/s/p  0:03:07 }
2024-06-14 10:59:56,774 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  192/  625], loss: 1.844, per_step_time: 426ms, lr: 1.6223867e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:56,775 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   30.7% |███████████████                                   | 4.69312 samples/s/p  0:03:04 }
2024-06-14 10:59:57,635 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  194/  625], loss: 2.143, per_step_time: 427ms, lr: 1.6149806e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:57,637 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   31.0% |███████████████                                   | 4.67770 samples/s/p  0:03:04 }
2024-06-14 10:59:58,494 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  196/  625], loss: 1.971, per_step_time: 425ms, lr: 1.6075228e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:58,494 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   31.4% |███████████████                                   | 4.69744 samples/s/p  0:03:02 }
2024-06-14 10:59:59,347 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  198/  625], loss: 2.258, per_step_time: 423ms, lr: 1.6000133e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 10:59:59,348 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   31.7% |███████████████                                   | 4.71920 samples/s/p  0:03:00 }
2024-06-14 11:00:00,205 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  200/  625], loss: 1.980, per_step_time: 425ms, lr: 1.5924528e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:00,205 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   32.0% |████████████████                                  | 4.69705 samples/s/p  0:03:00 }
2024-06-14 11:00:01,066 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  202/  625], loss: 2.431, per_step_time: 427ms, lr: 1.5848425e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:01,067 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   32.3% |████████████████                                  | 4.67491 samples/s/p  0:03:00 }
2024-06-14 11:00:01,922 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  204/  625], loss: 1.937, per_step_time: 424ms, lr: 1.5771835e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:01,922 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   32.6% |████████████████                                  | 4.70743 samples/s/p  0:02:58 }
2024-06-14 11:00:02,783 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  206/  625], loss: 2.143, per_step_time: 427ms, lr: 1.5694757e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:02,784 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   33.0% |████████████████                                  | 4.67948 samples/s/p  0:02:59 }
2024-06-14 11:00:03,641 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  208/  625], loss: 1.881, per_step_time: 426ms, lr: 1.5617205e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:03,642 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   33.3% |████████████████                                  | 4.69348 samples/s/p  0:02:57 }
2024-06-14 11:00:04,509 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  210/  625], loss: 1.910, per_step_time: 430ms, lr: 1.5539183e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:04,509 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   33.6% |████████████████                                  | 4.64263 samples/s/p  0:02:58 }
2024-06-14 11:00:05,390 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  212/  625], loss: 2.020, per_step_time: 437ms, lr: 1.54607e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:05,390 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   33.9% |████████████████                                  | 4.57124 samples/s/p  0:03:00 }
2024-06-14 11:00:06,250 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  214/  625], loss: 2.081, per_step_time: 426ms, lr: 1.5381767e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:06,250 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   34.2% |█████████████████                                 | 4.68485 samples/s/p  0:02:55 }
2024-06-14 11:00:07,114 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  216/  625], loss: 1.988, per_step_time: 427ms, lr: 1.530239e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:07,114 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   34.6% |█████████████████                                 | 4.67859 samples/s/p  0:02:54 }
2024-06-14 11:00:07,971 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  218/  625], loss: 2.206, per_step_time: 425ms, lr: 1.5222575e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:07,972 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   34.9% |█████████████████                                 | 4.70165 samples/s/p  0:02:53 }
2024-06-14 11:00:08,830 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  220/  625], loss: 2.279, per_step_time: 426ms, lr: 1.5142333e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:08,830 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   35.2% |█████████████████                                 | 4.69275 samples/s/p  0:02:52 }
2024-06-14 11:00:09,696 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  222/  625], loss: 2.162, per_step_time: 430ms, lr: 1.5061672e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:09,696 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   35.5% |█████████████████                                 | 4.65104 samples/s/p  0:02:53 }
2024-06-14 11:00:10,564 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  224/  625], loss: 2.106, per_step_time: 431ms, lr: 1.4980596e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:10,564 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   35.8% |█████████████████                                 | 4.63910 samples/s/p  0:02:52 }
2024-06-14 11:00:11,433 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  226/  625], loss: 2.340, per_step_time: 431ms, lr: 1.48991185e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:11,434 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   36.2% |██████████████████                                | 4.63023 samples/s/p  0:02:52 }
2024-06-14 11:00:12,301 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  228/  625], loss: 2.255, per_step_time: 430ms, lr: 1.4817247e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:12,301 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   36.5% |██████████████████                                | 4.64286 samples/s/p  0:02:51 }
2024-06-14 11:00:13,156 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  230/  625], loss: 2.061, per_step_time: 424ms, lr: 1.47349865e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:13,160 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   36.8% |██████████████████                                | 4.70861 samples/s/p  0:02:47 }
2024-06-14 11:00:14,018 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  232/  625], loss: 2.376, per_step_time: 426ms, lr: 1.4652348e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:14,019 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   37.1% |██████████████████                                | 4.68959 samples/s/p  0:02:47 }
2024-06-14 11:00:14,878 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  234/  625], loss: 1.900, per_step_time: 426ms, lr: 1.45693375e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:14,878 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   37.4% |██████████████████                                | 4.68504 samples/s/p  0:02:46 }
2024-06-14 11:00:15,733 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  236/  625], loss: 2.124, per_step_time: 424ms, lr: 1.4485967e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:15,733 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   37.8% |██████████████████                                | 4.71170 samples/s/p  0:02:45 }
2024-06-14 11:00:16,591 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  238/  625], loss: 2.067, per_step_time: 426ms, lr: 1.4402243e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:16,592 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   38.1% |███████████████████                               | 4.68883 samples/s/p  0:02:45 }
2024-06-14 11:00:17,451 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  240/  625], loss: 2.113, per_step_time: 427ms, lr: 1.4318173e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:17,452 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   38.4% |███████████████████                               | 4.68304 samples/s/p  0:02:44 }
2024-06-14 11:00:18,313 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  242/  625], loss: 2.142, per_step_time: 428ms, lr: 1.42337685e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:18,315 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   38.7% |███████████████████                               | 4.67243 samples/s/p  0:02:43 }
2024-06-14 11:00:19,176 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  244/  625], loss: 2.139, per_step_time: 427ms, lr: 1.41490345e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:19,176 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   39.0% |███████████████████                               | 4.67651 samples/s/p  0:02:42 }
2024-06-14 11:00:20,048 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  246/  625], loss: 2.143, per_step_time: 433ms, lr: 1.4063983e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:20,048 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   39.4% |███████████████████                               | 4.61742 samples/s/p  0:02:44 }
2024-06-14 11:00:20,909 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  248/  625], loss: 2.140, per_step_time: 426ms, lr: 1.3978619e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:20,909 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   39.7% |███████████████████                               | 4.68502 samples/s/p  0:02:40 }
2024-06-14 11:00:21,767 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  250/  625], loss: 2.005, per_step_time: 426ms, lr: 1.3892954e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:21,768 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   40.0% |████████████████████                              | 4.69222 samples/s/p  0:02:39 }
2024-06-14 11:00:22,624 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  252/  625], loss: 2.219, per_step_time: 424ms, lr: 1.3806995e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:22,624 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   40.3% |████████████████████                              | 4.70595 samples/s/p  0:02:38 }
2024-06-14 11:00:23,483 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  254/  625], loss: 2.088, per_step_time: 426ms, lr: 1.3720752e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:23,485 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   40.6% |████████████████████                              | 4.68462 samples/s/p  0:02:38 }
2024-06-14 11:00:24,354 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  256/  625], loss: 2.465, per_step_time: 431ms, lr: 1.3634231e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:24,354 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   41.0% |████████████████████                              | 4.63348 samples/s/p  0:02:39 }
2024-06-14 11:00:25,217 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  258/  625], loss: 2.293, per_step_time: 428ms, lr: 1.35474465e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:25,217 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   41.3% |████████████████████                              | 4.66445 samples/s/p  0:02:37 }
2024-06-14 11:00:26,071 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  260/  625], loss: 1.840, per_step_time: 424ms, lr: 1.34604015e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:26,071 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   41.6% |████████████████████                              | 4.71630 samples/s/p  0:02:34 }
2024-06-14 11:00:26,934 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  262/  625], loss: 2.065, per_step_time: 428ms, lr: 1.3373108e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:26,934 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   41.9% |████████████████████                              | 4.66638 samples/s/p  0:02:35 }
2024-06-14 11:00:27,802 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  264/  625], loss: 2.216, per_step_time: 430ms, lr: 1.3285573e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:27,802 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   42.2% |█████████████████████                             | 4.64068 samples/s/p  0:02:35 }
2024-06-14 11:00:28,662 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  266/  625], loss: 2.323, per_step_time: 427ms, lr: 1.3197807e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:28,664 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   42.6% |█████████████████████                             | 4.68116 samples/s/p  0:02:33 }
2024-06-14 11:00:29,519 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  268/  625], loss: 1.972, per_step_time: 424ms, lr: 1.3109819e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:29,519 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   42.9% |█████████████████████                             | 4.70880 samples/s/p  0:02:31 }
2024-06-14 11:00:30,394 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  270/  625], loss: 2.274, per_step_time: 434ms, lr: 1.3021614e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:30,394 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   43.2% |█████████████████████                             | 4.60283 samples/s/p  0:02:34 }
2024-06-14 11:00:31,255 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  272/  625], loss: 2.062, per_step_time: 428ms, lr: 1.2933207e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:31,256 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   43.5% |█████████████████████                             | 4.67254 samples/s/p  0:02:31 }
2024-06-14 11:00:32,113 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  274/  625], loss: 1.851, per_step_time: 425ms, lr: 1.2844603e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:32,114 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   43.8% |█████████████████████                             | 4.69540 samples/s/p  0:02:29 }
2024-06-14 11:00:32,985 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  276/  625], loss: 2.156, per_step_time: 433ms, lr: 1.2755811e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:32,985 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   44.2% |██████████████████████                            | 4.61880 samples/s/p  0:02:31 }
2024-06-14 11:00:33,842 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  278/  625], loss: 1.962, per_step_time: 425ms, lr: 1.2666843e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:33,844 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   44.5% |██████████████████████                            | 4.69956 samples/s/p  0:02:27 }
2024-06-14 11:00:34,712 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  280/  625], loss: 2.117, per_step_time: 431ms, lr: 1.2577704e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:34,712 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   44.8% |██████████████████████                            | 4.63522 samples/s/p  0:02:28 }
2024-06-14 11:00:35,571 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  282/  625], loss: 2.008, per_step_time: 426ms, lr: 1.2488406e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:35,571 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   45.1% |██████████████████████                            | 4.69031 samples/s/p  0:02:26 }
2024-06-14 11:00:36,426 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  284/  625], loss: 1.972, per_step_time: 424ms, lr: 1.2398958e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:36,426 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   45.4% |██████████████████████                            | 4.71257 samples/s/p  0:02:24 }
2024-06-14 11:00:37,287 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  286/  625], loss: 2.218, per_step_time: 427ms, lr: 1.2309367e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:37,287 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   45.8% |██████████████████████                            | 4.67776 samples/s/p  0:02:24 }
2024-06-14 11:00:38,148 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  288/  625], loss: 2.230, per_step_time: 427ms, lr: 1.22196425e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:38,149 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   46.1% |███████████████████████                           | 4.67460 samples/s/p  0:02:24 }
2024-06-14 11:00:39,024 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  290/  625], loss: 2.092, per_step_time: 435ms, lr: 1.2129796e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:39,026 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   46.4% |███████████████████████                           | 4.59747 samples/s/p  0:02:25 }
2024-06-14 11:00:39,901 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  292/  625], loss: 2.215, per_step_time: 434ms, lr: 1.2039834e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:39,901 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   46.7% |███████████████████████                           | 4.60227 samples/s/p  0:02:24 }
2024-06-14 11:00:40,763 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  294/  625], loss: 1.821, per_step_time: 428ms, lr: 1.1949766e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:40,763 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   47.0% |███████████████████████                           | 4.67234 samples/s/p  0:02:21 }
2024-06-14 11:00:41,625 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  296/  625], loss: 2.185, per_step_time: 428ms, lr: 1.1859604e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:41,626 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   47.4% |███████████████████████                           | 4.66898 samples/s/p  0:02:20 }
2024-06-14 11:00:42,479 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  298/  625], loss: 2.025, per_step_time: 423ms, lr: 1.1769353e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:42,479 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   47.7% |███████████████████████                           | 4.71732 samples/s/p  0:02:18 }
2024-06-14 11:00:43,338 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  300/  625], loss: 1.830, per_step_time: 426ms, lr: 1.1679025e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:43,338 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   48.0% |████████████████████████                          | 4.69088 samples/s/p  0:02:18 }
2024-06-14 11:00:44,202 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  302/  625], loss: 2.278, per_step_time: 429ms, lr: 1.1588627e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:44,204 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   48.3% |████████████████████████                          | 4.66083 samples/s/p  0:02:18 }
2024-06-14 11:00:45,061 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  304/  625], loss: 2.352, per_step_time: 425ms, lr: 1.1498169e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:45,061 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   48.6% |████████████████████████                          | 4.69584 samples/s/p  0:02:16 }
2024-06-14 11:00:45,917 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  306/  625], loss: 2.249, per_step_time: 425ms, lr: 1.1407663e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:45,917 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   49.0% |████████████████████████                          | 4.70482 samples/s/p  0:02:15 }
2024-06-14 11:00:46,785 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  308/  625], loss: 2.048, per_step_time: 430ms, lr: 1.1317113e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:46,785 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   49.3% |████████████████████████                          | 4.64186 samples/s/p  0:02:16 }
2024-06-14 11:00:47,655 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  310/  625], loss: 2.109, per_step_time: 432ms, lr: 1.1226531e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:47,656 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   49.6% |████████████████████████                          | 4.62520 samples/s/p  0:02:16 }
2024-06-14 11:00:48,509 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  312/  625], loss: 2.002, per_step_time: 424ms, lr: 1.1135928e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:48,510 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   49.9% |████████████████████████                          | 4.71512 samples/s/p  0:02:12 }
2024-06-14 11:00:49,363 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  314/  625], loss: 1.936, per_step_time: 423ms, lr: 1.1045309e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:49,365 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   50.2% |█████████████████████████                         | 4.71812 samples/s/p  0:02:11 }
2024-06-14 11:00:50,219 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  316/  625], loss: 2.151, per_step_time: 424ms, lr: 1.0954687e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:50,220 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   50.6% |█████████████████████████                         | 4.71077 samples/s/p  0:02:11 }
2024-06-14 11:00:51,075 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  318/  625], loss: 2.164, per_step_time: 424ms, lr: 1.086407e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:51,075 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   50.9% |█████████████████████████                         | 4.70862 samples/s/p  0:02:10 }
2024-06-14 11:00:51,932 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  320/  625], loss: 2.272, per_step_time: 425ms, lr: 1.0773465e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:51,932 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   51.2% |█████████████████████████                         | 4.69898 samples/s/p  0:02:09 }
2024-06-14 11:00:52,797 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  322/  625], loss: 2.102, per_step_time: 429ms, lr: 1.0682885e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:52,798 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   51.5% |█████████████████████████                         | 4.65117 samples/s/p  0:02:10 }
2024-06-14 11:00:53,655 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  324/  625], loss: 2.083, per_step_time: 425ms, lr: 1.05923355e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:53,655 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   51.8% |█████████████████████████                         | 4.69972 samples/s/p  0:02:08 }
2024-06-14 11:00:54,532 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  326/  625], loss: 2.145, per_step_time: 435ms, lr: 1.0501828e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:54,533 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   52.2% |██████████████████████████                        | 4.59259 samples/s/p  0:02:10 }
2024-06-14 11:00:55,391 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  328/  625], loss: 1.978, per_step_time: 426ms, lr: 1.0411371e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:55,392 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   52.5% |██████████████████████████                        | 4.69283 samples/s/p  0:02:06 }
2024-06-14 11:00:56,247 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  330/  625], loss: 2.179, per_step_time: 424ms, lr: 1.0320971e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:56,247 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   52.8% |██████████████████████████                        | 4.70607 samples/s/p  0:02:05 }
2024-06-14 11:00:57,122 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  332/  625], loss: 1.797, per_step_time: 434ms, lr: 1.0230644e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:57,123 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   53.1% |██████████████████████████                        | 4.59969 samples/s/p  0:02:07 }
2024-06-14 11:00:57,984 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  334/  625], loss: 2.193, per_step_time: 427ms, lr: 1.0140394e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:57,985 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   53.4% |██████████████████████████                        | 4.67331 samples/s/p  0:02:04 }
2024-06-14 11:00:58,848 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  336/  625], loss: 2.020, per_step_time: 428ms, lr: 1.005023e-05, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:58,848 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   53.8% |██████████████████████████                        | 4.66571 samples/s/p  0:02:03 }
2024-06-14 11:00:59,708 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  338/  625], loss: 2.047, per_step_time: 427ms, lr: 9.960164e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:00:59,710 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   54.1% |███████████████████████████                       | 4.67970 samples/s/p  0:02:02 }
2024-06-14 11:01:00,571 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  340/  625], loss: 2.114, per_step_time: 427ms, lr: 9.870201e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:00,572 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   54.4% |███████████████████████████                       | 4.67344 samples/s/p  0:02:01 }
2024-06-14 11:01:01,426 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  342/  625], loss: 2.346, per_step_time: 424ms, lr: 9.780355e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:01,427 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   54.7% |███████████████████████████                       | 4.70955 samples/s/p  0:02:00 }
2024-06-14 11:01:02,282 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  344/  625], loss: 2.125, per_step_time: 424ms, lr: 9.6906315e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:02,282 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   55.0% |███████████████████████████                       | 4.70982 samples/s/p  0:01:59 }
2024-06-14 11:01:03,137 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  346/  625], loss: 2.325, per_step_time: 424ms, lr: 9.601039e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:03,137 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   55.4% |███████████████████████████                       | 4.71008 samples/s/p  0:01:58 }
2024-06-14 11:01:04,000 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  348/  625], loss: 2.174, per_step_time: 428ms, lr: 9.511591e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:04,001 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   55.7% |███████████████████████████                       | 4.66497 samples/s/p  0:01:58 }
2024-06-14 11:01:04,855 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  350/  625], loss: 1.954, per_step_time: 424ms, lr: 9.422292e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:04,858 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   56.0% |████████████████████████████                      | 4.70829 samples/s/p  0:01:56 }
2024-06-14 11:01:05,721 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  352/  625], loss: 2.144, per_step_time: 429ms, lr: 9.333155e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:05,722 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   56.3% |████████████████████████████                      | 4.66089 samples/s/p  0:01:57 }
2024-06-14 11:01:06,586 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  354/  625], loss: 2.124, per_step_time: 429ms, lr: 9.244187e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:06,586 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   56.6% |████████████████████████████                      | 4.65645 samples/s/p  0:01:56 }
2024-06-14 11:01:07,450 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  356/  625], loss: 2.263, per_step_time: 429ms, lr: 9.155395e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:07,450 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   57.0% |████████████████████████████                      | 4.66084 samples/s/p  0:01:55 }
2024-06-14 11:01:08,310 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  358/  625], loss: 2.167, per_step_time: 426ms, lr: 9.066792e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:08,310 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   57.3% |████████████████████████████                      | 4.68546 samples/s/p  0:01:53 }
2024-06-14 11:01:09,167 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  360/  625], loss: 1.948, per_step_time: 426ms, lr: 8.9783825e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:09,168 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   57.6% |████████████████████████████                      | 4.69401 samples/s/p  0:01:52 }
2024-06-14 11:01:10,024 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  362/  625], loss: 2.022, per_step_time: 425ms, lr: 8.890179e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:10,026 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   57.9% |████████████████████████████                      | 4.69970 samples/s/p  0:01:51 }
2024-06-14 11:01:10,879 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  364/  625], loss: 2.255, per_step_time: 424ms, lr: 8.802191e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:10,880 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   58.2% |█████████████████████████████                     | 4.71504 samples/s/p  0:01:50 }
2024-06-14 11:01:11,735 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  366/  625], loss: 2.117, per_step_time: 424ms, lr: 8.7144235e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:11,735 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   58.6% |█████████████████████████████                     | 4.70802 samples/s/p  0:01:50 }
2024-06-14 11:01:12,590 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  368/  625], loss: 2.035, per_step_time: 424ms, lr: 8.626889e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:12,590 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   58.9% |█████████████████████████████                     | 4.71116 samples/s/p  0:01:49 }
2024-06-14 11:01:13,448 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  370/  625], loss: 2.050, per_step_time: 426ms, lr: 8.539596e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:13,448 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   59.2% |█████████████████████████████                     | 4.69224 samples/s/p  0:01:48 }
2024-06-14 11:01:14,304 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  372/  625], loss: 2.337, per_step_time: 424ms, lr: 8.452551e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:14,304 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   59.5% |█████████████████████████████                     | 4.70588 samples/s/p  0:01:47 }
2024-06-14 11:01:15,158 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  374/  625], loss: 1.910, per_step_time: 423ms, lr: 8.365766e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:15,160 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   59.8% |█████████████████████████████                     | 4.72045 samples/s/p  0:01:46 }
2024-06-14 11:01:16,026 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  376/  625], loss: 1.971, per_step_time: 429ms, lr: 8.279245e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:16,026 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   60.2% |██████████████████████████████                    | 4.65225 samples/s/p  0:01:47 }
2024-06-14 11:01:16,890 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  378/  625], loss: 1.913, per_step_time: 429ms, lr: 8.193001e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:16,891 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   60.5% |██████████████████████████████                    | 4.65584 samples/s/p  0:01:46 }
2024-06-14 11:01:17,760 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  380/  625], loss: 2.143, per_step_time: 431ms, lr: 8.107044e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:17,760 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   60.8% |██████████████████████████████                    | 4.63372 samples/s/p  0:01:45 }
2024-06-14 11:01:18,613 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  382/  625], loss: 2.119, per_step_time: 423ms, lr: 8.021378e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:18,613 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   61.1% |██████████████████████████████                    | 4.72343 samples/s/p  0:01:42 }
2024-06-14 11:01:19,479 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  384/  625], loss: 2.139, per_step_time: 430ms, lr: 7.936014e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:19,479 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   61.4% |██████████████████████████████                    | 4.64959 samples/s/p  0:01:43 }
2024-06-14 11:01:20,340 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  386/  625], loss: 2.115, per_step_time: 427ms, lr: 7.850963e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:20,342 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   61.8% |██████████████████████████████                    | 4.67436 samples/s/p  0:01:42 }
2024-06-14 11:01:21,201 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  388/  625], loss: 2.097, per_step_time: 426ms, lr: 7.766229e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:21,201 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   62.1% |███████████████████████████████                   | 4.68597 samples/s/p  0:01:41 }
2024-06-14 11:01:22,064 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  390/  625], loss: 2.313, per_step_time: 428ms, lr: 7.681824e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:22,065 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   62.4% |███████████████████████████████                   | 4.66390 samples/s/p  0:01:40 }
2024-06-14 11:01:22,921 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  392/  625], loss: 2.132, per_step_time: 425ms, lr: 7.597754e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:22,921 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   62.7% |███████████████████████████████                   | 4.70200 samples/s/p  0:01:39 }
2024-06-14 11:01:23,777 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  394/  625], loss: 1.814, per_step_time: 425ms, lr: 7.5140288e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:23,777 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   63.0% |███████████████████████████████                   | 4.70570 samples/s/p  0:01:38 }
2024-06-14 11:01:24,642 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  396/  625], loss: 1.732, per_step_time: 429ms, lr: 7.430659e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:24,642 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   63.4% |███████████████████████████████                   | 4.65453 samples/s/p  0:01:38 }
2024-06-14 11:01:25,501 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  398/  625], loss: 1.979, per_step_time: 426ms, lr: 7.3476494e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:25,502 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   63.7% |███████████████████████████████                   | 4.69294 samples/s/p  0:01:36 }
2024-06-14 11:01:26,364 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  400/  625], loss: 1.940, per_step_time: 428ms, lr: 7.265011e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:26,364 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   64.0% |████████████████████████████████                  | 4.67104 samples/s/p  0:01:36 }
2024-06-14 11:01:27,222 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  402/  625], loss: 2.225, per_step_time: 425ms, lr: 7.182751e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:27,222 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   64.3% |████████████████████████████████                  | 4.69521 samples/s/p  0:01:34 }
2024-06-14 11:01:28,079 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  404/  625], loss: 2.085, per_step_time: 425ms, lr: 7.100879e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:28,080 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   64.6% |████████████████████████████████                  | 4.69575 samples/s/p  0:01:34 }
2024-06-14 11:01:28,933 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  406/  625], loss: 2.102, per_step_time: 423ms, lr: 7.019402e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:28,933 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   65.0% |████████████████████████████████                  | 4.71892 samples/s/p  0:01:32 }
2024-06-14 11:01:29,787 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  408/  625], loss: 2.258, per_step_time: 424ms, lr: 6.938326e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:29,788 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   65.3% |████████████████████████████████                  | 4.71360 samples/s/p  0:01:32 }
2024-06-14 11:01:30,643 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  410/  625], loss: 2.038, per_step_time: 425ms, lr: 6.8576646e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:30,645 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   65.6% |████████████████████████████████                  | 4.70558 samples/s/p  0:01:31 }
2024-06-14 11:01:31,509 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  412/  625], loss: 2.099, per_step_time: 429ms, lr: 6.7774217e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:31,510 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   65.9% |████████████████████████████████                  | 4.65770 samples/s/p  0:01:31 }
2024-06-14 11:01:32,373 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  414/  625], loss: 2.242, per_step_time: 428ms, lr: 6.697608e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:32,373 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   66.2% |█████████████████████████████████                 | 4.66246 samples/s/p  0:01:30 }
2024-06-14 11:01:33,227 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  416/  625], loss: 2.053, per_step_time: 424ms, lr: 6.618231e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:33,228 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   66.6% |█████████████████████████████████                 | 4.71274 samples/s/p  0:01:28 }
2024-06-14 11:01:34,091 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  418/  625], loss: 1.949, per_step_time: 429ms, lr: 6.5392987e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:34,092 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   66.9% |█████████████████████████████████                 | 4.66059 samples/s/p  0:01:28 }
2024-06-14 11:01:34,949 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  420/  625], loss: 2.064, per_step_time: 425ms, lr: 6.4608166e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:34,949 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   67.2% |█████████████████████████████████                 | 4.69806 samples/s/p  0:01:27 }
2024-06-14 11:01:35,803 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  422/  625], loss: 2.026, per_step_time: 424ms, lr: 6.3827947e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:35,804 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   67.5% |█████████████████████████████████                 | 4.71677 samples/s/p  0:01:26 }
2024-06-14 11:01:36,661 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  424/  625], loss: 2.279, per_step_time: 425ms, lr: 6.3052403e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:36,661 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   67.8% |█████████████████████████████████                 | 4.69585 samples/s/p  0:01:25 }
2024-06-14 11:01:37,517 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  426/  625], loss: 1.909, per_step_time: 424ms, lr: 6.2281633e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:37,517 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   68.2% |██████████████████████████████████                | 4.70704 samples/s/p  0:01:24 }
2024-06-14 11:01:38,371 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  428/  625], loss: 2.169, per_step_time: 424ms, lr: 6.151571e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:38,372 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   68.5% |██████████████████████████████████                | 4.71585 samples/s/p  0:01:23 }
2024-06-14 11:01:39,233 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  430/  625], loss: 2.250, per_step_time: 427ms, lr: 6.07547e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:39,234 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   68.8% |██████████████████████████████████                | 4.67810 samples/s/p  0:01:23 }
2024-06-14 11:01:40,091 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  432/  625], loss: 2.024, per_step_time: 425ms, lr: 5.9998665e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:40,091 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   69.1% |██████████████████████████████████                | 4.69627 samples/s/p  0:01:22 }
2024-06-14 11:01:40,962 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  434/  625], loss: 2.123, per_step_time: 432ms, lr: 5.9247714e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:40,963 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   69.4% |██████████████████████████████████                | 4.62118 samples/s/p  0:01:22 }
2024-06-14 11:01:41,826 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  436/  625], loss: 2.084, per_step_time: 428ms, lr: 5.850192e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:41,826 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   69.8% |██████████████████████████████████                | 4.66598 samples/s/p  0:01:21 }
2024-06-14 11:01:42,715 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  438/  625], loss: 2.195, per_step_time: 441ms, lr: 5.7761317e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:42,716 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   70.1% |███████████████████████████████████               | 4.53066 samples/s/p  0:01:22 }
2024-06-14 11:01:43,572 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  440/  625], loss: 2.158, per_step_time: 425ms, lr: 5.702603e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:43,573 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   70.4% |███████████████████████████████████               | 4.69958 samples/s/p  0:01:18 }
2024-06-14 11:01:44,438 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  442/  625], loss: 2.078, per_step_time: 430ms, lr: 5.629612e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:44,439 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   70.7% |███████████████████████████████████               | 4.65094 samples/s/p  0:01:18 }
2024-06-14 11:01:45,296 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  444/  625], loss: 2.064, per_step_time: 426ms, lr: 5.557163e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:45,297 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   71.0% |███████████████████████████████████               | 4.69370 samples/s/p  0:01:17 }
2024-06-14 11:01:46,166 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  446/  625], loss: 1.897, per_step_time: 432ms, lr: 5.485268e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:46,169 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   71.4% |███████████████████████████████████               | 4.62844 samples/s/p  0:01:17 }
2024-06-14 11:01:47,023 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  448/  625], loss: 1.968, per_step_time: 424ms, lr: 5.413932e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:47,024 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   71.7% |███████████████████████████████████               | 4.70986 samples/s/p  0:01:15 }
2024-06-14 11:01:47,880 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  450/  625], loss: 2.065, per_step_time: 425ms, lr: 5.3431622e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:47,881 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   72.0% |████████████████████████████████████              | 4.70122 samples/s/p  0:01:14 }
2024-06-14 11:01:48,735 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  452/  625], loss: 1.902, per_step_time: 424ms, lr: 5.2729647e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:48,735 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   72.3% |████████████████████████████████████              | 4.71113 samples/s/p  0:01:13 }
2024-06-14 11:01:49,606 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  454/  625], loss: 2.143, per_step_time: 432ms, lr: 5.2033474e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:49,607 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   72.6% |████████████████████████████████████              | 4.62334 samples/s/p  0:01:13 }
2024-06-14 11:01:50,469 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  456/  625], loss: 1.906, per_step_time: 428ms, lr: 5.1343195e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:50,469 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   73.0% |████████████████████████████████████              | 4.66833 samples/s/p  0:01:12 }
2024-06-14 11:01:51,328 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  458/  625], loss: 2.213, per_step_time: 426ms, lr: 5.0658864e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:51,329 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   73.3% |████████████████████████████████████              | 4.68956 samples/s/p  0:01:11 }
2024-06-14 11:01:52,191 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  460/  625], loss: 2.130, per_step_time: 428ms, lr: 4.9980554e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:52,192 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   73.6% |████████████████████████████████████              | 4.66936 samples/s/p  0:01:10 }
2024-06-14 11:01:53,059 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  462/  625], loss: 1.909, per_step_time: 430ms, lr: 4.9308323e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:53,059 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   73.9% |████████████████████████████████████              | 4.64235 samples/s/p  0:01:10 }
2024-06-14 11:01:53,929 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  464/  625], loss: 2.427, per_step_time: 432ms, lr: 4.8642232e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:53,930 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   74.2% |█████████████████████████████████████             | 4.62412 samples/s/p  0:01:09 }
2024-06-14 11:01:54,792 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  466/  625], loss: 2.287, per_step_time: 428ms, lr: 4.798238e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:54,793 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   74.6% |█████████████████████████████████████             | 4.66724 samples/s/p  0:01:08 }
2024-06-14 11:01:55,659 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  468/  625], loss: 2.021, per_step_time: 430ms, lr: 4.7328813e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:55,659 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   74.9% |█████████████████████████████████████             | 4.64779 samples/s/p  0:01:07 }
2024-06-14 11:01:56,523 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  470/  625], loss: 2.129, per_step_time: 429ms, lr: 4.668159e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:56,525 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   75.2% |█████████████████████████████████████             | 4.66099 samples/s/p  0:01:06 }
2024-06-14 11:01:57,383 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  472/  625], loss: 2.062, per_step_time: 426ms, lr: 4.6040795e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:57,384 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   75.5% |█████████████████████████████████████             | 4.68769 samples/s/p  0:01:05 }
2024-06-14 11:01:58,238 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  474/  625], loss: 2.122, per_step_time: 424ms, lr: 4.540648e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:58,239 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   75.8% |█████████████████████████████████████             | 4.70865 samples/s/p  0:01:04 }
2024-06-14 11:01:59,094 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  476/  625], loss: 2.058, per_step_time: 425ms, lr: 4.477871e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:59,095 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   76.2% |██████████████████████████████████████            | 4.70464 samples/s/p  0:01:03 }
2024-06-14 11:01:59,951 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  478/  625], loss: 1.863, per_step_time: 425ms, lr: 4.415757e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:01:59,952 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   76.5% |██████████████████████████████████████            | 4.69878 samples/s/p  0:01:02 }
2024-06-14 11:02:00,808 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  480/  625], loss: 2.097, per_step_time: 424ms, lr: 4.3543096e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:00,808 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   76.8% |██████████████████████████████████████            | 4.70654 samples/s/p  0:01:01 }
2024-06-14 11:02:01,667 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  482/  625], loss: 1.933, per_step_time: 426ms, lr: 4.2935358e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:01,668 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   77.1% |██████████████████████████████████████            | 4.68961 samples/s/p  0:01:00 }
2024-06-14 11:02:02,541 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  484/  625], loss: 2.424, per_step_time: 433ms, lr: 4.2334414e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:02,541 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   77.4% |██████████████████████████████████████            | 4.61259 samples/s/p  0:01:01 }
2024-06-14 11:02:03,394 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  486/  625], loss: 2.037, per_step_time: 423ms, lr: 4.1740327e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:03,394 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   77.8% |██████████████████████████████████████            | 4.72127 samples/s/p  0:00:58 }
2024-06-14 11:02:04,248 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  488/  625], loss: 1.980, per_step_time: 424ms, lr: 4.115317e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:04,248 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   78.1% |███████████████████████████████████████           | 4.71571 samples/s/p  0:00:58 }
2024-06-14 11:02:05,103 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  490/  625], loss: 2.036, per_step_time: 424ms, lr: 4.0573004e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:05,103 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   78.4% |███████████████████████████████████████           | 4.71047 samples/s/p  0:00:57 }
2024-06-14 11:02:05,968 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  492/  625], loss: 2.267, per_step_time: 429ms, lr: 3.999987e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:05,968 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   78.7% |███████████████████████████████████████           | 4.65281 samples/s/p  0:00:57 }
2024-06-14 11:02:06,831 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  494/  625], loss: 2.131, per_step_time: 428ms, lr: 3.943383e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:06,833 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   79.0% |███████████████████████████████████████           | 4.66269 samples/s/p  0:00:56 }
2024-06-14 11:02:07,710 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  496/  625], loss: 2.006, per_step_time: 435ms, lr: 3.887493e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:07,710 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   79.4% |███████████████████████████████████████           | 4.58800 samples/s/p  0:00:56 }
2024-06-14 11:02:08,573 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  498/  625], loss: 2.367, per_step_time: 428ms, lr: 3.8323265e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:08,573 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   79.7% |███████████████████████████████████████           | 4.66685 samples/s/p  0:00:54 }
2024-06-14 11:02:09,431 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  500/  625], loss: 2.191, per_step_time: 426ms, lr: 3.777885e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:09,431 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   80.0% |████████████████████████████████████████          | 4.69066 samples/s/p  0:00:53 }
2024-06-14 11:02:10,290 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  502/  625], loss: 2.396, per_step_time: 426ms, lr: 3.724177e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:10,291 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   80.3% |████████████████████████████████████████          | 4.68793 samples/s/p  0:00:52 }
2024-06-14 11:02:11,158 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  504/  625], loss: 2.191, per_step_time: 431ms, lr: 3.6712065e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:11,159 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   80.6% |████████████████████████████████████████          | 4.63764 samples/s/p  0:00:52 }
2024-06-14 11:02:12,019 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  506/  625], loss: 2.175, per_step_time: 427ms, lr: 3.6189776e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:12,020 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   81.0% |████████████████████████████████████████          | 4.67853 samples/s/p  0:00:50 }
2024-06-14 11:02:12,884 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  508/  625], loss: 2.094, per_step_time: 429ms, lr: 3.5674984e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:12,884 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   81.3% |████████████████████████████████████████          | 4.65600 samples/s/p  0:00:50 }
2024-06-14 11:02:13,739 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  510/  625], loss: 2.040, per_step_time: 424ms, lr: 3.516773e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:13,740 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   81.6% |████████████████████████████████████████          | 4.70894 samples/s/p  0:00:48 }
2024-06-14 11:02:14,597 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  512/  625], loss: 2.101, per_step_time: 425ms, lr: 3.4668067e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:14,597 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   81.9% |████████████████████████████████████████          | 4.69773 samples/s/p  0:00:48 }
2024-06-14 11:02:15,451 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  514/  625], loss: 1.859, per_step_time: 424ms, lr: 3.4176028e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:15,452 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   82.2% |█████████████████████████████████████████         | 4.71132 samples/s/p  0:00:47 }
2024-06-14 11:02:16,313 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  516/  625], loss: 2.025, per_step_time: 427ms, lr: 3.3691672e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:16,313 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   82.6% |█████████████████████████████████████████         | 4.67602 samples/s/p  0:00:46 }
2024-06-14 11:02:17,174 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  518/  625], loss: 2.176, per_step_time: 427ms, lr: 3.3215056e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:17,177 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   82.9% |█████████████████████████████████████████         | 4.67466 samples/s/p  0:00:45 }
2024-06-14 11:02:18,039 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  520/  625], loss: 2.298, per_step_time: 428ms, lr: 3.2746239e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:18,040 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   83.2% |█████████████████████████████████████████         | 4.66443 samples/s/p  0:00:45 }
2024-06-14 11:02:18,909 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  522/  625], loss: 2.025, per_step_time: 431ms, lr: 3.2285252e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:18,909 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   83.5% |█████████████████████████████████████████         | 4.63023 samples/s/p  0:00:44 }
2024-06-14 11:02:19,773 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  524/  625], loss: 1.996, per_step_time: 429ms, lr: 3.1832142e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:19,773 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   83.8% |█████████████████████████████████████████         | 4.66200 samples/s/p  0:00:43 }
2024-06-14 11:02:20,636 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  526/  625], loss: 2.027, per_step_time: 428ms, lr: 3.1386953e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:20,636 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   84.2% |██████████████████████████████████████████        | 4.66963 samples/s/p  0:00:42 }
2024-06-14 11:02:21,491 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  528/  625], loss: 2.019, per_step_time: 424ms, lr: 3.0949732e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:21,491 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   84.5% |██████████████████████████████████████████        | 4.70734 samples/s/p  0:00:41 }
2024-06-14 11:02:22,349 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  530/  625], loss: 2.020, per_step_time: 426ms, lr: 3.052053e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:22,350 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   84.8% |██████████████████████████████████████████        | 4.69225 samples/s/p  0:00:40 }
2024-06-14 11:02:23,208 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  532/  625], loss: 2.105, per_step_time: 425ms, lr: 3.0099382e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:23,208 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   85.1% |██████████████████████████████████████████        | 4.69583 samples/s/p  0:00:39 }
2024-06-14 11:02:24,069 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  534/  625], loss: 2.057, per_step_time: 427ms, lr: 2.9686348e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:24,070 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   85.4% |██████████████████████████████████████████        | 4.67383 samples/s/p  0:00:38 }
2024-06-14 11:02:24,932 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  536/  625], loss: 2.122, per_step_time: 428ms, lr: 2.9281446e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:24,933 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   85.8% |██████████████████████████████████████████        | 4.66762 samples/s/p  0:00:38 }
2024-06-14 11:02:25,787 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  538/  625], loss: 2.170, per_step_time: 424ms, lr: 2.8884726e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:25,788 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   86.1% |███████████████████████████████████████████       | 4.70885 samples/s/p  0:00:36 }
2024-06-14 11:02:26,642 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  540/  625], loss: 2.143, per_step_time: 424ms, lr: 2.849624e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:26,642 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   86.4% |███████████████████████████████████████████       | 4.71413 samples/s/p  0:00:36 }
2024-06-14 11:02:27,499 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  542/  625], loss: 1.965, per_step_time: 425ms, lr: 2.811601e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:27,500 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   86.7% |███████████████████████████████████████████       | 4.70001 samples/s/p  0:00:35 }
2024-06-14 11:02:28,361 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  544/  625], loss: 2.018, per_step_time: 427ms, lr: 2.774409e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:28,361 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   87.0% |███████████████████████████████████████████       | 4.67562 samples/s/p  0:00:34 }
2024-06-14 11:02:29,224 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  546/  625], loss: 1.866, per_step_time: 428ms, lr: 2.738051e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:29,225 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   87.4% |███████████████████████████████████████████       | 4.66451 samples/s/p  0:00:33 }
2024-06-14 11:02:30,084 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  548/  625], loss: 2.327, per_step_time: 426ms, lr: 2.702529e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:30,084 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   87.7% |███████████████████████████████████████████       | 4.68929 samples/s/p  0:00:32 }
2024-06-14 11:02:30,952 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  550/  625], loss: 1.927, per_step_time: 431ms, lr: 2.6678485e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:30,973 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   88.0% |████████████████████████████████████████████      | 4.63786 samples/s/p  0:00:32 }
2024-06-14 11:02:31,826 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  552/  625], loss: 2.202, per_step_time: 423ms, lr: 2.634014e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:31,826 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   88.3% |████████████████████████████████████████████      | 4.72211 samples/s/p  0:00:30 }
2024-06-14 11:02:32,685 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  554/  625], loss: 2.099, per_step_time: 426ms, lr: 2.6010273e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:32,686 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   88.6% |████████████████████████████████████████████      | 4.69019 samples/s/p  0:00:30 }
2024-06-14 11:02:33,550 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  556/  625], loss: 2.097, per_step_time: 429ms, lr: 2.5688921e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:33,551 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   89.0% |████████████████████████████████████████████      | 4.65964 samples/s/p  0:00:29 }
2024-06-14 11:02:34,414 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  558/  625], loss: 2.084, per_step_time: 429ms, lr: 2.5376128e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:34,415 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   89.3% |████████████████████████████████████████████      | 4.66009 samples/s/p  0:00:28 }
2024-06-14 11:02:35,273 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  560/  625], loss: 2.029, per_step_time: 426ms, lr: 2.507189e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:35,274 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   89.6% |████████████████████████████████████████████      | 4.68851 samples/s/p  0:00:27 }
2024-06-14 11:02:36,135 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  562/  625], loss: 1.920, per_step_time: 427ms, lr: 2.477628e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:36,135 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   89.9% |████████████████████████████████████████████      | 4.67759 samples/s/p  0:00:26 }
2024-06-14 11:02:36,993 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  564/  625], loss: 2.198, per_step_time: 426ms, lr: 2.448931e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:36,994 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   90.2% |█████████████████████████████████████████████     | 4.69072 samples/s/p  0:00:26 }
2024-06-14 11:02:37,854 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  566/  625], loss: 2.315, per_step_time: 427ms, lr: 2.4211004e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:37,855 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   90.6% |█████████████████████████████████████████████     | 4.67946 samples/s/p  0:00:25 }
2024-06-14 11:02:38,733 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  568/  625], loss: 2.131, per_step_time: 436ms, lr: 2.3941404e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:38,733 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   90.9% |█████████████████████████████████████████████     | 4.58599 samples/s/p  0:00:24 }
2024-06-14 11:02:39,590 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  570/  625], loss: 2.102, per_step_time: 426ms, lr: 2.3680527e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:39,591 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   91.2% |█████████████████████████████████████████████     | 4.69455 samples/s/p  0:00:23 }
2024-06-14 11:02:40,444 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  572/  625], loss: 2.156, per_step_time: 423ms, lr: 2.34284e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:40,445 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   91.5% |█████████████████████████████████████████████     | 4.71862 samples/s/p  0:00:22 }
2024-06-14 11:02:41,298 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  574/  625], loss: 2.269, per_step_time: 424ms, lr: 2.3185044e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:41,299 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   91.8% |█████████████████████████████████████████████     | 4.71415 samples/s/p  0:00:21 }
2024-06-14 11:02:42,160 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  576/  625], loss: 2.171, per_step_time: 427ms, lr: 2.2950505e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:42,160 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   92.2% |██████████████████████████████████████████████    | 4.67437 samples/s/p  0:00:20 }
2024-06-14 11:02:43,031 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  578/  625], loss: 2.046, per_step_time: 432ms, lr: 2.272477e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:43,032 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   92.5% |██████████████████████████████████████████████    | 4.62777 samples/s/p  0:00:20 }
2024-06-14 11:02:43,891 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  580/  625], loss: 1.847, per_step_time: 427ms, lr: 2.2507902e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:43,892 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   92.8% |██████████████████████████████████████████████    | 4.68361 samples/s/p  0:00:19 }
2024-06-14 11:02:44,746 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  582/  625], loss: 1.909, per_step_time: 424ms, lr: 2.2299891e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:44,747 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   93.1% |██████████████████████████████████████████████    | 4.71157 samples/s/p  0:00:18 }
2024-06-14 11:02:45,601 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  584/  625], loss: 2.144, per_step_time: 424ms, lr: 2.2100787e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:45,601 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   93.4% |██████████████████████████████████████████████    | 4.71187 samples/s/p  0:00:17 }
2024-06-14 11:02:46,467 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  586/  625], loss: 2.221, per_step_time: 430ms, lr: 2.1910591e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:46,468 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   93.8% |██████████████████████████████████████████████    | 4.64707 samples/s/p  0:00:16 }
2024-06-14 11:02:47,337 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  588/  625], loss: 1.999, per_step_time: 431ms, lr: 2.1729327e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:47,337 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   94.1% |███████████████████████████████████████████████   | 4.63196 samples/s/p  0:00:15 }
2024-06-14 11:02:48,202 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  590/  625], loss: 2.224, per_step_time: 429ms, lr: 2.1557007e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:48,205 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   94.4% |███████████████████████████████████████████████   | 4.65294 samples/s/p  0:00:15 }
2024-06-14 11:02:49,065 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  592/  625], loss: 2.299, per_step_time: 427ms, lr: 2.1393655e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:49,066 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   94.7% |███████████████████████████████████████████████   | 4.67678 samples/s/p  0:00:14 }
2024-06-14 11:02:49,921 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  594/  625], loss: 2.095, per_step_time: 425ms, lr: 2.1239277e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:49,921 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   95.0% |███████████████████████████████████████████████   | 4.70470 samples/s/p  0:00:13 }
2024-06-14 11:02:50,781 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  596/  625], loss: 2.202, per_step_time: 426ms, lr: 2.1093917e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:50,781 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   95.4% |███████████████████████████████████████████████   | 4.68390 samples/s/p  0:00:12 }
2024-06-14 11:02:51,648 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  598/  625], loss: 2.070, per_step_time: 430ms, lr: 2.0957564e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:51,649 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   95.7% |███████████████████████████████████████████████   | 4.64311 samples/s/p  0:00:11 }
2024-06-14 11:02:52,506 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  600/  625], loss: 2.215, per_step_time: 425ms, lr: 2.083023e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:52,506 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   96.0% |████████████████████████████████████████████████  | 4.69725 samples/s/p  0:00:10 }
2024-06-14 11:02:53,380 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  602/  625], loss: 2.175, per_step_time: 434ms, lr: 2.0711943e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:53,381 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   96.3% |████████████████████████████████████████████████  | 4.60781 samples/s/p  0:00:09 }
2024-06-14 11:02:54,244 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  604/  625], loss: 2.053, per_step_time: 428ms, lr: 2.060272e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:54,244 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   96.6% |████████████████████████████████████████████████  | 4.66614 samples/s/p  0:00:09 }
2024-06-14 11:02:55,099 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  606/  625], loss: 1.810, per_step_time: 424ms, lr: 2.0502544e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:55,100 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   97.0% |████████████████████████████████████████████████  | 4.71276 samples/s/p  0:00:08 }
2024-06-14 11:02:55,960 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  608/  625], loss: 2.056, per_step_time: 426ms, lr: 2.0411446e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:55,960 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   97.3% |████████████████████████████████████████████████  | 4.68403 samples/s/p  0:00:07 }
2024-06-14 11:02:56,823 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  610/  625], loss: 2.223, per_step_time: 428ms, lr: 2.032944e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:56,823 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   97.6% |████████████████████████████████████████████████  | 4.66792 samples/s/p  0:00:06 }
2024-06-14 11:02:57,692 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  612/  625], loss: 2.132, per_step_time: 432ms, lr: 2.025652e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:57,693 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   97.9% |████████████████████████████████████████████████  | 4.62916 samples/s/p  0:00:05 }
2024-06-14 11:02:58,562 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  614/  625], loss: 1.951, per_step_time: 431ms, lr: 2.0192701e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:58,564 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   98.2% |█████████████████████████████████████████████████ | 4.63146 samples/s/p  0:00:04 }
2024-06-14 11:02:59,424 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  616/  625], loss: 2.088, per_step_time: 426ms, lr: 2.0137977e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:02:59,424 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   98.6% |█████████████████████████████████████████████████ | 4.68398 samples/s/p  0:00:03 }
2024-06-14 11:03:00,288 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  618/  625], loss: 2.017, per_step_time: 429ms, lr: 2.0092375e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:03:00,288 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   98.9% |█████████████████████████████████████████████████ | 4.66143 samples/s/p  0:00:03 }
2024-06-14 11:03:01,171 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  620/  625], loss: 2.174, per_step_time: 438ms, lr: 2.0055882e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:03:01,171 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   99.2% |█████████████████████████████████████████████████ | 4.56142 samples/s/p  0:00:02 }
2024-06-14 11:03:02,029 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  622/  625], loss: 2.043, per_step_time: 426ms, lr: 2.0028513e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:03:02,029 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   99.5% |█████████████████████████████████████████████████ | 4.69224 samples/s/p  0:00:01 }
2024-06-14 11:03:02,888 - mindformers[mindformers/core/callback/callback.py:314] - INFO - { Epoch:[  1/  1], step:[  624/  625], loss: 2.025, per_step_time: 426ms, lr: 2.0010261e-06, overflow cond: False, loss_scale: 32768.0
2024-06-14 11:03:02,889 - mindformers[mindformers/core/callback/callback.py:324] - INFO -   99.8% |█████████████████████████████████████████████████ | 4.68855 samples/s/p  0:00:00 }
2024-06-14 11:03:02,893 - mindformers[mindformers/core/callback/callback.py:561] - INFO - ......Saving ckpt......
2024-06-14 11:03:18,319 - mindformers[mindformers/trainer/base_trainer.py:774] - INFO - .........Training Over!.............
