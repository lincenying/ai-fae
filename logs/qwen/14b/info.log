[INFO] 2024-06-28 09:47:00,234 [mindformers/tools/utils.py:155] set_output_path: set output path to '/home/ma-user/work/mindformers/research/qwen/output'
[INFO] 2024-06-28 09:47:00,235 [mindformers/trainer/base_trainer.py:90] __init__: Now Running Task is: text_generation, Model is: qwen_14b
[WARNING] 2024-06-28 09:47:00,235 [mindformers/trainer/base_trainer.py:131] __init__: Input model name is not in the supported list or unspecified.
[WARNING] 2024-06-28 09:47:00,235 [mindformers/trainer/base_trainer.py:132] __init__: See the list of supported task and model name: OrderedDict([('general', OrderedDict([('common', '/home/ma-user/work/mindformers/configs/general/run_general_task.yaml')])), ('masked_image_modeling', OrderedDict([('mae_vit_base_p16', '/home/ma-user/work/mindformers/configs/mae/run_mae_vit_base_p16_224_800ep.yaml'), ('common', '/home/ma-user/work/mindformers/configs/mae/run_mae_vit_base_p16_224_800ep.yaml')])), ('image_classification', OrderedDict([('vit_base_p16', '/home/ma-user/work/mindformers/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('swin_base_p4w7', '/home/ma-user/work/mindformers/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('mindspore/vit_base_p16', '/home/ma-user/work/mindformers/configs/vit/run_vit_base_p16_224_100ep.yaml'), ('mindspore/swin_base_p4w7', '/home/ma-user/work/mindformers/configs/swin/run_swin_base_p4w7_224_100ep.yaml'), ('common', '/home/ma-user/work/mindformers/configs/vit/run_vit_base_p16_224_100ep.yaml')])), ('fill_mask', OrderedDict([('bert_base_uncased', '/home/ma-user/work/mindformers/configs/bert/run_bert_base_uncased.yaml'), ('bert_tiny_uncased', '/home/ma-user/work/mindformers/configs/bert/run_bert_tiny_uncased.yaml'), ('common', '/home/ma-user/work/mindformers/configs/bert/run_bert_tiny_uncased.yaml')])), ('contrastive_language_image_pretrain', OrderedDict([('clip_vit_b_32', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('blip2_stage1_vit_g', '/home/ma-user/work/mindformers/configs/blip2/run_blip2_stage1_vit_g_qformer_pretrain.yaml'), ('blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/work/mindformers/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b.yaml'), ('blip2_stage2_vit_g_llama_7b', '/home/ma-user/work/mindformers/configs/blip2/run_blip2_stage2_vit_g_llama_7b.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml'), ('clip_vit_b_16', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_16_pretrain_flickr8k.yaml'), ('clip_vit_l_14', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_l_14_pretrain_flickr8k.yaml'), ('clip_vit_l_14@336', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_l_14@336_pretrain_flickr8k.yaml'), ('common', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_32_pretrain_flickr8k.yaml')])), ('image_to_text_retrieval', OrderedDict([('blip2_stage1_evaluator', '/home/ma-user/work/mindformers/configs/blip2/run_blip2_stage1_vit_g_retrieval_flickr30k.yaml')])), ('zero_shot_image_classification', OrderedDict([('clip_vit_b_32', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('mindspore/clip_vit_b_32', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_b_16', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_16_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_l_14_zero_shot_image_classification_cifar100.yaml'), ('clip_vit_l_14@336', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_l_14@336_zero_shot_image_classification_cifar100.yaml'), ('blip2_stage1_classification', '/home/ma-user/work/mindformers/configs/blip2/run_blip2_stage1_vit_g_zero_shot_image_classification_cifar100.yaml'), ('common', '/home/ma-user/work/mindformers/configs/clip/run_clip_vit_b_32_zero_shot_image_classification_cifar100.yaml')])), ('image_to_text_generation', OrderedDict([('itt_blip2_stage2_vit_g_baichuan_7b', '/home/ma-user/work/mindformers/configs/blip2/run_blip2_stage2_vit_g_baichuan_7b_image_to_text_generation.yaml'), ('itt_blip2_stage2_vit_g_llama_7b', '/home/ma-user/work/mindformers/configs/blip2/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml'), ('common', '/home/ma-user/work/mindformers/configs/clip/run_blip2_stage2_vit_g_llama_7b_image_to_text_generation.yaml')])), ('translation', OrderedDict([('t5_small', '/home/ma-user/work/mindformers/configs/t5/run_t5_small_on_wmt16.yaml'), ('t5_tiny', '/home/ma-user/work/mindformers/configs/t5/run_t5_tiny_on_wmt16.yaml'), ('common', '/home/ma-user/work/mindformers/configs/t5/run_t5_small_on_wmt16.yaml')])), ('text_classification', OrderedDict([('txtcls_bert_base_uncased', '/home/ma-user/work/mindformers/configs/txtcls/run_txtcls_bert_base_uncased.yaml'), ('txtcls_bert_base_uncased_mnli', '/home/ma-user/work/mindformers/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('mindspore/txtcls_bert_base_uncased_mnli', '/home/ma-user/work/mindformers/configs/txtcls/run_txtcls_bert_base_uncased_mnli.yaml'), ('gpt2_txtcls', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2_txtcls.yaml'), ('common', '/home/ma-user/work/mindformers/configs/txtcls/run_txtcls_bert_base_uncased.yaml')])), ('token_classification', OrderedDict([('tokcls_bert_base_chinese', '/home/ma-user/work/mindformers/configs/tokcls/run_tokcls_bert_base_chinese.yaml'), ('tokcls_bert_base_chinese_cluener', '/home/ma-user/work/mindformers/configs/tokcls/run_tokcls_bert_base_chinese_cluener.yaml'), ('common', '/home/ma-user/work/mindformers/configs/tokcls/run_tokcls_bert_base_chinese.yaml')])), ('question_answering', OrderedDict([('qa_bert_base_uncased', '/home/ma-user/work/mindformers/configs/qa/run_qa_bert_base_uncased.yaml'), ('qa_bert_base_uncased_squad', '/home/ma-user/work/mindformers/configs/qa/run_qa_bert_base_uncased.yaml'), ('mindspore/qa_bert_base_uncased', '/home/ma-user/work/mindformers/configs/qa/run_qa_bert_base_uncased.yaml'), ('common', '/home/ma-user/work/mindformers/configs/qa/run_qa_bert_base_uncased.yaml')])), ('text_generation', OrderedDict([('gpt2', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2.yaml'), ('gpt2_lora', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2_lora.yaml'), ('gpt2_13b', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2_13b.yaml'), ('gpt2_52b', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2_52b.yaml'), ('gpt2_xl', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2_xl.yaml'), ('gpt2_xl_lora', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2_xl_lora.yaml'), ('llama_7b', '/home/ma-user/work/mindformers/configs/llama/run_llama_7b.yaml'), ('llama_13b', '/home/ma-user/work/mindformers/configs/llama/run_llama_13b.yaml'), ('llama_65b', '/home/ma-user/work/mindformers/configs/llama/run_llama_65b.yaml'), ('llama2_7b', '/home/ma-user/work/mindformers/configs/llama2/run_llama2_7b.yaml'), ('llama2_13b', '/home/ma-user/work/mindformers/configs/llama2/run_llama2_13b.yaml'), ('llama2_70b', '/home/ma-user/work/mindformers/configs/llama2/run_llama2_70b.yaml'), ('codellama_34b', '/home/ma-user/work/mindformers/configs/codellama/run_codellama_34b_910b.yaml'), ('llama_7b_lora', '/home/ma-user/work/mindformers/configs/llama/run_llama_7b_lora.yaml'), ('pangualpha_2_6b', '/home/ma-user/work/mindformers/configs/pangualpha/run_pangualpha_2_6b.yaml'), ('pangualpha_13b', '/home/ma-user/work/mindformers/configs/pangualpha/run_pangualpha_13b.yaml'), ('glm_6b', '/home/ma-user/work/mindformers/configs/glm/run_glm_6b_finetune.yaml'), ('glm_6b_chat', '/home/ma-user/work/mindformers/configs/glm/run_glm_6b_infer.yaml'), ('glm_6b_lora', '/home/ma-user/work/mindformers/configs/glm/run_glm_6b_lora.yaml'), ('glm_6b_lora_chat', '/home/ma-user/work/mindformers/configs/glm/run_glm_6b_lora_infer.yaml'), ('glm2_6b', '/home/ma-user/work/mindformers/configs/glm2/run_glm2_6b.yaml'), ('glm2_6b_lora', '/home/ma-user/work/mindformers/configs/glm2/run_glm2_6b_lora.yaml'), ('glm2_6b_ptuning2', '/home/ma-user/work/mindformers/configs/glm2/run_glm2_6b_ptuning2.yaml'), ('glm3_6b', '/home/ma-user/work/mindformers/configs/glm3/run_glm3_6b.yaml'), ('codegeex2_6b', '/home/ma-user/work/mindformers/configs/codegeex2/run_codegeex2_6b.yaml'), ('bloom_560m', '/home/ma-user/work/mindformers/configs/bloom/run_bloom_560m.yaml'), ('bloom_7.1b', '/home/ma-user/work/mindformers/configs/bloom/run_bloom_7.1b.yaml'), ('bloom_65b', '/home/ma-user/work/mindformers/configs/bloom/run_bloom_65b.yaml'), ('bloom_176b', '/home/ma-user/work/mindformers/configs/bloom/run_bloom_176b.yaml'), ('baichuan_7b', '/home/ma-user/work/mindformers/research/baichuan/run_baichuan_7b.yaml'), ('baichuan2_7b', '/home/ma-user/work/mindformers/research/baichuan2/run_baichuan2_7b.yaml'), ('baichuan2_13b', '/home/ma-user/work/mindformers/research/baichuan2/run_baichuan2_13b.yaml'), ('ziya_13b', '/home/ma-user/work/mindformers/research/ziya/run_ziya_13b.yaml'), ('skywork_13b', '/home/ma-user/work/mindformers/research/skywork/run_skywork_13b.yaml'), ('internlm_7b', '/home/ma-user/work/mindformers/research/internlm/run_internlm_7b.yaml'), ('internlm_7b_lora', '/home/ma-user/work/mindformers/research/internlm/run_internlm_7b_lora.yaml'), ('qwen_7b', '/home/ma-user/work/mindformers/research/qwen/run_qwen_7b.yaml'), ('qwen_7b_lora', '/home/ma-user/work/mindformers/research/qwen/run_qwen_7b_lora.yaml'), ('common', '/home/ma-user/work/mindformers/configs/gpt2/run_gpt2.yaml')])), ('segment_anything', OrderedDict([('sam_vit_b', '/home/ma-user/work/mindformers/configs/sam/run_sam_vit-b.yaml'), ('sam_vit_l', '/home/ma-user/work/mindformers/configs/sam/run_sam_vit-l.yaml'), ('sam_vit_h', '/home/ma-user/work/mindformers/configs/sam/run_sam_vit-h.yaml'), ('common', '/home/ma-user/work/mindformers/configs/sam/run_sam_vit-h.yaml')]))])
[WARNING] 2024-06-28 09:47:00,236 [mindformers/trainer/base_trainer.py:133] __init__: The default model config: /home/ma-user/work/mindformers/configs/gpt2/run_gpt2.yaml will now be used for the text_generation task 
[INFO] 2024-06-28 09:47:00,237 [mindformers/core/parallel_config.py:45] build_parallel_config: initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
[INFO] 2024-06-28 09:47:00,237 [mindformers/core/parallel_config.py:51] build_parallel_config: initial parallel_config from dict: {'data_parallel': 8, 'model_parallel': 1, 'pipeline_stage': 1, 'micro_batch_num': 1, 'vocab_emb_dp': False, 'gradient_aggregation_group': 4}
[INFO] 2024-06-28 09:47:00,237 [mindformers/trainer/base_trainer.py:233] _check_global_batch_size_for_auto_parallel: The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 1
[INFO] 2024-06-28 09:47:00,238 [mindformers/trainer/base_trainer.py:237] _check_global_batch_size_for_auto_parallel: global_batch_size = batch_size_per_card * device_num * gradient_accumulation_steps = 1 = 1 * 1 * 1
[INFO] 2024-06-28 09:47:00,238 [mindformers/trainer/base_trainer.py:246] _check_global_batch_size_for_auto_parallel: parallel_config will be change to default config: [ParallelConfig]
_recompute:[ParallelConfig]
_recompute:True
_select_recompute:False
_parallel_optimizer_comm_recompute:False
_mp_comm_recompute:True
_recompute_slice_activation:True

select_recompute:False
use_seq_parallel:False
_gradient_aggregation_group:4
_embed_dp_mp_config:[ParallelConfig]
_dp_mp_config:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_vocab_emb_dp:False
use_seq_parallel:False
select_recompute:False

_pp_config:[ParallelConfig]
_pipeline_stage:1
_micro_batch_num:1

_moe_config:[ParallelConfig]
_dpmp:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_expert_parallel:1
use_seq_parallel:False
select_recompute:False

.
[INFO] 2024-06-28 09:47:00,239 [mindformers/trainer/base_trainer.py:388] create_network: .........Build Network From Config..........
[INFO] 2024-06-28 09:47:00,239 [mindformers/version_control.py:60] decorator: The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
[INFO] 2024-06-28 09:47:00,240 [mindformers/version_control.py:64] decorator: 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
[INFO] 2024-06-28 09:47:00,240 [mindformers/version_control.py:70] decorator: The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.
[INFO] 2024-06-28 09:47:00,240 [mindformers/version_control.py:73] decorator: The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
[WARNING] 2024-06-28 09:47:32,499 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:47:39,259 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:47:46,053 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:47:52,858 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:47:59,619 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:48:06,461 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:48:13,310 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:48:20,142 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:48:26,918 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[WARNING] 2024-06-28 09:48:33,743 [mindformers/modules/layers.py:554] shard: The user passed the custom defined activation function True. If the user want to enable shard for the activation cell, the user should set the shard for each primitives in the cell.
[INFO] 2024-06-28 09:52:12,122 [mindformers/models/base_model.py:117] load_checkpoint: model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[INFO] 2024-06-28 09:52:12,142 [mindformers/trainer/base_trainer.py:539] count_parameters: Network Parameters: 14167 M.
[INFO] 2024-06-28 09:52:13,307 [mindformers/trainer/utils.py:733] load_ckpt: .............Start load checkpoint from checkpoint..................
[INFO] 2024-06-28 09:55:54,136 [mindformers/trainer/utils.py:767] load_ckpt: Network parameters are not loaded: (['transformer.layers.0.attention.kvcache_mgr.key_past', 'transformer.layers.0.attention.kvcache_mgr.value_past', 'transformer.layers.1.attention.kvcache_mgr.key_past', 'transformer.layers.1.attention.kvcache_mgr.value_past', 'transformer.layers.2.attention.kvcache_mgr.key_past', 'transformer.layers.2.attention.kvcache_mgr.value_past', 'transformer.layers.3.attention.kvcache_mgr.key_past', 'transformer.layers.3.attention.kvcache_mgr.value_past', 'transformer.layers.4.attention.kvcache_mgr.key_past', 'transformer.layers.4.attention.kvcache_mgr.value_past', 'transformer.layers.5.attention.kvcache_mgr.key_past', 'transformer.layers.5.attention.kvcache_mgr.value_past', 'transformer.layers.6.attention.kvcache_mgr.key_past', 'transformer.layers.6.attention.kvcache_mgr.value_past', 'transformer.layers.7.attention.kvcache_mgr.key_past', 'transformer.layers.7.attention.kvcache_mgr.value_past', 'transformer.layers.8.attention.kvcache_mgr.key_past', 'transformer.layers.8.attention.kvcache_mgr.value_past', 'transformer.layers.9.attention.kvcache_mgr.key_past', 'transformer.layers.9.attention.kvcache_mgr.value_past', 'transformer.layers.10.attention.kvcache_mgr.key_past', 'transformer.layers.10.attention.kvcache_mgr.value_past', 'transformer.layers.11.attention.kvcache_mgr.key_past', 'transformer.layers.11.attention.kvcache_mgr.value_past', 'transformer.layers.12.attention.kvcache_mgr.key_past', 'transformer.layers.12.attention.kvcache_mgr.value_past', 'transformer.layers.13.attention.kvcache_mgr.key_past', 'transformer.layers.13.attention.kvcache_mgr.value_past', 'transformer.layers.14.attention.kvcache_mgr.key_past', 'transformer.layers.14.attention.kvcache_mgr.value_past', 'transformer.layers.15.attention.kvcache_mgr.key_past', 'transformer.layers.15.attention.kvcache_mgr.value_past', 'transformer.layers.16.attention.kvcache_mgr.key_past', 'transformer.layers.16.attention.kvcache_mgr.value_past', 'transformer.layers.17.attention.kvcache_mgr.key_past', 'transformer.layers.17.attention.kvcache_mgr.value_past', 'transformer.layers.18.attention.kvcache_mgr.key_past', 'transformer.layers.18.attention.kvcache_mgr.value_past', 'transformer.layers.19.attention.kvcache_mgr.key_past', 'transformer.layers.19.attention.kvcache_mgr.value_past', 'transformer.layers.20.attention.kvcache_mgr.key_past', 'transformer.layers.20.attention.kvcache_mgr.value_past', 'transformer.layers.21.attention.kvcache_mgr.key_past', 'transformer.layers.21.attention.kvcache_mgr.value_past', 'transformer.layers.22.attention.kvcache_mgr.key_past', 'transformer.layers.22.attention.kvcache_mgr.value_past', 'transformer.layers.23.attention.kvcache_mgr.key_past', 'transformer.layers.23.attention.kvcache_mgr.value_past', 'transformer.layers.24.attention.kvcache_mgr.key_past', 'transformer.layers.24.attention.kvcache_mgr.value_past', 'transformer.layers.25.attention.kvcache_mgr.key_past', 'transformer.layers.25.attention.kvcache_mgr.value_past', 'transformer.layers.26.attention.kvcache_mgr.key_past', 'transformer.layers.26.attention.kvcache_mgr.value_past', 'transformer.layers.27.attention.kvcache_mgr.key_past', 'transformer.layers.27.attention.kvcache_mgr.value_past', 'transformer.layers.28.attention.kvcache_mgr.key_past', 'transformer.layers.28.attention.kvcache_mgr.value_past', 'transformer.layers.29.attention.kvcache_mgr.key_past', 'transformer.layers.29.attention.kvcache_mgr.value_past', 'transformer.layers.30.attention.kvcache_mgr.key_past', 'transformer.layers.30.attention.kvcache_mgr.value_past', 'transformer.layers.31.attention.kvcache_mgr.key_past', 'transformer.layers.31.attention.kvcache_mgr.value_past', 'transformer.layers.32.attention.kvcache_mgr.key_past', 'transformer.layers.32.attention.kvcache_mgr.value_past', 'transformer.layers.33.attention.kvcache_mgr.key_past', 'transformer.layers.33.attention.kvcache_mgr.value_past', 'transformer.layers.34.attention.kvcache_mgr.key_past', 'transformer.layers.34.attention.kvcache_mgr.value_past', 'transformer.layers.35.attention.kvcache_mgr.key_past', 'transformer.layers.35.attention.kvcache_mgr.value_past', 'transformer.layers.36.attention.kvcache_mgr.key_past', 'transformer.layers.36.attention.kvcache_mgr.value_past', 'transformer.layers.37.attention.kvcache_mgr.key_past', 'transformer.layers.37.attention.kvcache_mgr.value_past', 'transformer.layers.38.attention.kvcache_mgr.key_past', 'transformer.layers.38.attention.kvcache_mgr.value_past', 'transformer.layers.39.attention.kvcache_mgr.key_past', 'transformer.layers.39.attention.kvcache_mgr.value_past'], [])
[WARNING] 2024-06-28 09:55:54,182 [mindformers/generation/text_generator.py:1099] generate: When do_sample is set to False, top_k will be set to 1 and top_p will be set to 0, making them inactive.
[INFO] 2024-06-28 09:55:54,182 [mindformers/generation/text_generator.py:1103] generate: Generation Config is: {'max_length': 512, 'max_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1.0, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 151643, 'bos_token_id': 1, 'eos_token_id': 151643, '_from_model_config': True}
[INFO] 2024-06-28 09:55:54,183 [mindformers/generation/text_generator.py:176] _get_generation_mode: The generation mode will be **GREEDY_SEARCH**.
[INFO] 2024-06-28 09:59:01,074 [mindformers/generation/text_generator.py:478] _greedy_search: total time: 186.8907973766327 s; generated tokens: 361 tokens; generate speed: 1.9316092877087618 tokens/s
[INFO] 2024-06-28 09:59:01,083 [mindformers/trainer/base_trainer.py:946] predict_process: output result is: [{'text_generation_text': ['生成一份杭州的旅游攻略，包括景点介绍、美食推荐和住宿建议。\n\n杭州是中国著名的旅游城市，拥有许多美丽的景点、美食和住宿选择。以下是一份杭州旅游攻略，希望对您有所帮助。\n\n景点介绍：\n1. 西湖：杭州最著名的景点之一，被誉为“人间天堂”。西湖周围有许多美丽的园林和古建筑，如雷峰塔、断桥、三潭印月等。\n2. 千岛湖：位于杭州市西南部，是一个由1078个岛屿组成的大型淡水湖。千岛湖周围有许多美丽的景点，如龙川、九溪、三潭等。\n3. 灵隐寺：位于西湖风景区内，是中国佛教禅宗的发源地之一。灵隐寺内有许多古老的建筑和文物，如大雄宝殿、法堂、钟楼等。\n\n美食推荐：\n1. 西湖醋鱼：是杭州的特色菜之一，以西湖醋为主要调料，鱼肉鲜嫩，味道酸甜可口。\n2. 虾爆鳝丝：是杭州的传统名菜之一，以鳝鱼和虾仁为主要原料，口感鲜美。\n3. 葱油拌面：是杭州的特色小吃之一，以面条和葱油为主要原料，味道香浓。\n\n住宿建议：\n1. 西湖国宾馆：位于西湖风景区内，环境优美，设施齐全，是杭州最著名的五星级酒店之一。\n2. 杭州凯悦酒店：位于市中心，交通便利，设施豪华，是杭州的五星级酒店之一。\n3. 杭州香格里拉大酒店：位于市中心，交通便利，设施豪华，是杭州的五星级酒店之一。\n\n希望这份杭州旅游攻略对您有所帮助，祝您旅途愉快！']}]
[INFO] 2024-06-28 09:59:01,083 [mindformers/trainer/base_trainer.py:947] predict_process: output result is saved at: text_generation_result.txt
[INFO] 2024-06-28 09:59:01,084 [mindformers/trainer/base_trainer.py:948] predict_process: .........Predict Over!.............
